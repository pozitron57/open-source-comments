{
  "sha": "564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
  "node_id": "C_kwDOBtgov9oAKDU2NGU4ZmYzMTY2Njg5YmQ4ZGZkMWMzNzJlOTE0NGVjYzYxZmRiYTI",
  "commit": {
    "author": {
      "name": "Dmitry Verkhoturov",
      "email": "paskal.07@gmail.com",
      "date": "2025-12-04T01:47:01Z"
    },
    "committer": {
      "name": "GitHub",
      "email": "noreply@github.com",
      "date": "2025-12-04T01:47:01Z"
    },
    "message": "Update go dependencies (#1972)",
    "tree": {
      "sha": "f959628f3cc85df92284038dfc94b0bdc48c0882",
      "url": "https://api.github.com/repos/umputun/remark42/git/trees/f959628f3cc85df92284038dfc94b0bdc48c0882"
    },
    "url": "https://api.github.com/repos/umputun/remark42/git/commits/564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
    "comment_count": 0,
    "verification": {
      "verified": true,
      "reason": "valid",
      "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsFcBAABCAAQBQJpMOgVCRC1aQ7uu5UhlAAAxY4QAAPAzSkYyrp7FoPROQYbmiGa\naSlAGXwfkYE8A9RN20dV90wuVliXUtubwFixlAuUM+RQNN2eApGL5xBEmfoiA704\nNfRfdSIR8GX2pG/FycihQqY0vQM71wLQ6Or32jFNVMNCF5Asp11lfF3i4jmIpcHz\nP/QEu0lMccgCCyCaJnmDUyU0VkMcbIZScjVXnVGqDhF6bcO7PtRBt9WX8Q/sbNw1\n8R9KDKRpL9+Mo2eytTTFdDDSfJ6d91Xp1OBpedTzEs8DBBpHNqGlCzhkesIqasuT\n8DhkBopNn6P7c7X5WckVaqIed7GT9n+3XuGRqhWWCZ4+3yX9ZYa51iCyF8HueCxB\nP/RHdHl78rlMyQTd9GZQAKU63GQAwld1fthKlwpsRecRnA/sJDQMJC/1V12LE/HB\n7n40olcWiBb5cFOKLrovmzVeQiGu/7M3cSgIGHJK2oRMBkX8vkTCZhJxl/f5LAae\nOfxfmDD2jCKwRgn7mLWKgV/KYgT5vR3CSGEtnf44LNeeIzu01EHkG4IH8tZyDvfL\nBkJKJK6B4y1SbIxtttXvom2ubBGSueHcQwfyg9H1BqVWrTi1aLsgcghr5cKMPVWp\nMnFcruAUuQ6/0MQYbFd/VHSCd9MPwS1DgUmZaUIoX8ZWt6hAT6lDLU5UummG/7gV\nowaJpPtzda3TnD60JNvz\n=mQeh\n-----END PGP SIGNATURE-----\n",
      "payload": "tree f959628f3cc85df92284038dfc94b0bdc48c0882\nparent b4511427905c02e55730d7cb27036f5796decaa9\nauthor Dmitry Verkhoturov <paskal.07@gmail.com> 1764812821 +0100\ncommitter GitHub <noreply@github.com> 1764812821 -0600\n\nUpdate go dependencies (#1972)\n\n",
      "verified_at": "2025-12-04T01:47:02Z"
    }
  },
  "url": "https://api.github.com/repos/umputun/remark42/commits/564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
  "html_url": "https://github.com/umputun/remark42/commit/564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
  "comments_url": "https://api.github.com/repos/umputun/remark42/commits/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/comments",
  "author": {
    "login": "paskal",
    "id": 712534,
    "node_id": "MDQ6VXNlcjcxMjUzNA==",
    "avatar_url": "https://avatars.githubusercontent.com/u/712534?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/paskal",
    "html_url": "https://github.com/paskal",
    "followers_url": "https://api.github.com/users/paskal/followers",
    "following_url": "https://api.github.com/users/paskal/following{/other_user}",
    "gists_url": "https://api.github.com/users/paskal/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/paskal/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/paskal/subscriptions",
    "organizations_url": "https://api.github.com/users/paskal/orgs",
    "repos_url": "https://api.github.com/users/paskal/repos",
    "events_url": "https://api.github.com/users/paskal/events{/privacy}",
    "received_events_url": "https://api.github.com/users/paskal/received_events",
    "type": "User",
    "user_view_type": "public",
    "site_admin": false
  },
  "committer": {
    "login": "web-flow",
    "id": 19864447,
    "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
    "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/web-flow",
    "html_url": "https://github.com/web-flow",
    "followers_url": "https://api.github.com/users/web-flow/followers",
    "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
    "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
    "organizations_url": "https://api.github.com/users/web-flow/orgs",
    "repos_url": "https://api.github.com/users/web-flow/repos",
    "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
    "received_events_url": "https://api.github.com/users/web-flow/received_events",
    "type": "User",
    "user_view_type": "public",
    "site_admin": false
  },
  "parents": [
    {
      "sha": "b4511427905c02e55730d7cb27036f5796decaa9",
      "url": "https://api.github.com/repos/umputun/remark42/commits/b4511427905c02e55730d7cb27036f5796decaa9",
      "html_url": "https://github.com/umputun/remark42/commit/b4511427905c02e55730d7cb27036f5796decaa9"
    }
  ],
  "stats": {
    "total": 21614,
    "additions": 18663,
    "deletions": 2951
  },
  "files": [
    {
      "sha": "094fb9bfdbfa818f6ad0e68d455654a97536f678",
      "filename": ".github/workflows/ci-build.yml",
      "status": "modified",
      "additions": 7,
      "deletions": 0,
      "changes": 7,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/.github%2Fworkflows%2Fci-build.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/.github%2Fworkflows%2Fci-build.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/.github%2Fworkflows%2Fci-build.yml?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -41,6 +41,13 @@ jobs:\n       - name: available platforms\n         run: echo ${{ steps.buildx.outputs.platforms }}\n \n+      - name: free disk space\n+        run: |\n+          sudo rm -rf /usr/share/dotnet\n+          sudo rm -rf /opt/ghc\n+          sudo rm -rf /usr/local/share/boost\n+          docker system prune -af\n+\n       - name: build docker image without pushing (only outside master)\n         if: ${{ github.ref != 'refs/heads/master' }}\n         run: |"
    },
    {
      "sha": "f72c6b56d64f8161b25954f0f0e7b0156105fd97",
      "filename": "backend/app/notify/email.go",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fapp%2Fnotify%2Femail.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fapp%2Fnotify%2Femail.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fapp%2Fnotify%2Femail.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -10,7 +10,7 @@ import (\n \n \tlog \"github.com/go-pkgz/lgr\"\n \tntf \"github.com/go-pkgz/notify\"\n-\t\"github.com/go-pkgz/repeater\"\n+\t\"github.com/go-pkgz/repeater/v2\"\n \t\"github.com/hashicorp/go-multierror\"\n \n \t\"github.com/umputun/remark42/backend/app/templates\"\n@@ -161,7 +161,7 @@ func (e *Email) buildAndSendMessage(ctx context.Context, req Request, email stri\n \t\treturn err\n \t}\n \n-\treturn repeater.NewDefault(5, time.Millisecond*250).Do(\n+\treturn repeater.NewFixed(5, time.Millisecond*250).Do(\n \t\tctx,\n \t\tfunc() error {\n \t\t\treturn e.Email.Send(\n@@ -196,7 +196,7 @@ func (e *Email) SendVerification(ctx context.Context, req VerificationRequest) e\n \t\treturn err\n \t}\n \n-\treturn repeater.NewDefault(5, time.Millisecond*250).Do(\n+\treturn repeater.NewFixed(5, time.Millisecond*250).Do(\n \t\tctx,\n \t\tfunc() error {\n \t\t\treturn e.Email.Send("
    },
    {
      "sha": "ef3ee4acbb01c36fc361490c3dfc0f6fdaa55766",
      "filename": "backend/app/rest/api/rest.go",
      "status": "modified",
      "additions": 26,
      "deletions": 13,
      "changes": 39,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fapp%2Frest%2Fapi%2Frest.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fapp%2Frest%2Fapi%2Frest.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fapp%2Frest%2Fapi%2Frest.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -15,8 +15,8 @@ import (\n \t\"sync\"\n \t\"time\"\n \n-\t\"github.com/didip/tollbooth/v7\"\n-\t\"github.com/didip/tollbooth_chi\"\n+\t\"github.com/didip/tollbooth/v8\"\n+\t\"github.com/didip/tollbooth/v8/limiter\"\n \t\"github.com/go-chi/chi/v5\"\n \t\"github.com/go-chi/chi/v5/middleware\"\n \t\"github.com/go-chi/cors\"\n@@ -234,14 +234,14 @@ func (s *Rest) routes() chi.Router {\n \n \trouter.Group(func(r chi.Router) {\n \t\tr.Use(middleware.Timeout(5 * time.Second))\n-\t\tr.Use(logInfoWithBody, tollbooth_chi.LimitHandler(tollbooth.NewLimiter(2, nil)), middleware.NoCache)\n+\t\tr.Use(logInfoWithBody, rateLimiter(2), middleware.NoCache)\n \t\tr.Use(validEmailAuth()) // reject suspicious email logins\n \t\tr.Mount(\"/auth\", authHandler)\n \t})\n \n \trouter.Group(func(r chi.Router) {\n \t\tr.Use(middleware.Timeout(5 * time.Second))\n-\t\tr.Use(tollbooth_chi.LimitHandler(tollbooth.NewLimiter(100, nil)))\n+\t\tr.Use(rateLimiter(100))\n \t\tr.Mount(\"/avatar\", avatarHandler)\n \t})\n \n@@ -251,14 +251,14 @@ func (s *Rest) routes() chi.Router {\n \trouter.Route(\"/api/v1\", func(rapi chi.Router) {\n \t\trapi.Group(func(rava chi.Router) {\n \t\t\trava.Use(middleware.Timeout(5 * time.Second))\n-\t\t\trava.Use(tollbooth_chi.LimitHandler(tollbooth.NewLimiter(100, nil)))\n+\t\t\trava.Use(rateLimiter(100))\n \t\t\trava.Mount(\"/avatar\", avatarHandler)\n \t\t})\n \n \t\t// open routes\n \t\trapi.Group(func(ropen chi.Router) {\n \t\t\tropen.Use(middleware.Timeout(30 * time.Second))\n-\t\t\tropen.Use(tollbooth_chi.LimitHandler(tollbooth.NewLimiter(s.openRouteLimiter, nil)))\n+\t\t\tropen.Use(rateLimiter(s.openRouteLimiter))\n \t\t\tropen.Use(authMiddleware.Trace, middleware.NoCache, logInfoWithBody)\n \t\t\tropen.Get(\"/config\", s.configCtrl)\n \t\t\tropen.Get(\"/find\", s.pubRest.findCommentsCtrl)\n@@ -281,7 +281,7 @@ func (s *Rest) routes() chi.Router {\n \t\t// open routes, cached\n \t\trapi.Group(func(ropen chi.Router) {\n \t\t\tropen.Use(middleware.Timeout(30 * time.Second))\n-\t\t\tropen.Use(tollbooth_chi.LimitHandler(tollbooth.NewLimiter(10, nil)))\n+\t\t\tropen.Use(rateLimiter(10))\n \t\t\tropen.Use(authMiddleware.Trace, logInfoWithBody)\n \t\t\tropen.Get(\"/picture/{user}/{id}\", s.pubRest.loadPictureCtrl)\n \t\t\tropen.Get(\"/qr/telegram\", s.pubRest.telegramQrCtrl)\n@@ -290,7 +290,7 @@ func (s *Rest) routes() chi.Router {\n \t\t// protected routes, require auth\n \t\trapi.Group(func(rauth chi.Router) {\n \t\t\trauth.Use(middleware.Timeout(30 * time.Second))\n-\t\t\trauth.Use(tollbooth_chi.LimitHandler(tollbooth.NewLimiter(10, nil)))\n+\t\t\trauth.Use(rateLimiter(10))\n \t\t\trauth.Use(authMiddleware.Auth, matchSiteID, middleware.NoCache, logInfoWithBody)\n \t\t\trauth.Get(\"/user\", s.privRest.userInfoCtrl)\n \t\t\trauth.Get(\"/userdata\", s.privRest.userAllDataCtrl)\n@@ -299,7 +299,7 @@ func (s *Rest) routes() chi.Router {\n \t\t// admin routes, require auth and admin users only\n \t\trapi.Route(\"/admin\", func(radmin chi.Router) {\n \t\t\tradmin.Use(middleware.Timeout(30 * time.Second))\n-\t\t\tradmin.Use(tollbooth_chi.LimitHandler(tollbooth.NewLimiter(10, nil)))\n+\t\t\tradmin.Use(rateLimiter(10))\n \t\t\tradmin.Use(authMiddleware.Auth, authMiddleware.AdminOnly, matchSiteID)\n \t\t\tradmin.Use(middleware.NoCache, logInfoWithBody)\n \n@@ -325,7 +325,7 @@ func (s *Rest) routes() chi.Router {\n \t\t// protected routes, throttled to 10/s by default, controlled by external UpdateLimiter param\n \t\trapi.Group(func(rauth chi.Router) {\n \t\t\trauth.Use(middleware.Timeout(10 * time.Second))\n-\t\t\trauth.Use(tollbooth_chi.LimitHandler(tollbooth.NewLimiter(s.updateLimiter(), nil)))\n+\t\t\trauth.Use(rateLimiter(s.updateLimiter()))\n \t\t\trauth.Use(authMiddleware.Auth, matchSiteID, subscribersOnly(s.SubscribersOnly))\n \t\t\trauth.Use(middleware.NoCache, logInfoWithBody)\n \n@@ -345,7 +345,7 @@ func (s *Rest) routes() chi.Router {\n \t\t// protected routes, anonymous rejected\n \t\trapi.Group(func(rauth chi.Router) {\n \t\t\trauth.Use(middleware.Timeout(10 * time.Second))\n-\t\t\trauth.Use(tollbooth_chi.LimitHandler(tollbooth.NewLimiter(s.updateLimiter(), nil)))\n+\t\t\trauth.Use(rateLimiter(s.updateLimiter()))\n \t\t\trauth.Use(authMiddleware.Auth, rejectAnonUser, matchSiteID)\n \t\t\trauth.Use(logger.New(logger.Log(log.Default()), logger.Prefix(\"[DEBUG]\"), logger.IPfn(ipFn)).Handler)\n \t\t\trauth.Post(\"/picture\", s.privRest.savePictureCtrl)\n@@ -355,7 +355,7 @@ func (s *Rest) routes() chi.Router {\n \t// open routes on root level\n \trouter.Group(func(rroot chi.Router) {\n \t\trroot.Use(middleware.Timeout(10 * time.Second))\n-\t\trroot.Use(tollbooth_chi.LimitHandler(tollbooth.NewLimiter(50, nil)))\n+\t\trroot.Use(rateLimiter(50))\n \t\trroot.Get(\"/robots.txt\", s.pubRest.robotsCtrl)\n \t\trroot.Get(\"/email/unsubscribe.html\", s.privRest.emailUnsubscribeCtrl)\n \t\trroot.Post(\"/email/unsubscribe.html\", s.privRest.emailUnsubscribeCtrl)\n@@ -491,7 +491,7 @@ func addFileServer(r chi.Router, embedFS embed.FS, webRoot, version string) {\n \twebFS = http.StripPrefix(\"/web\", webFS)\n \tr.Get(\"/web\", http.RedirectHandler(\"/web/\", http.StatusMovedPermanently).ServeHTTP)\n \n-\tr.With(tollbooth_chi.LimitHandler(tollbooth.NewLimiter(20, nil)),\n+\tr.With(rateLimiter(20),\n \t\tmiddleware.Timeout(10*time.Second),\n \t\tcacheControl(time.Hour, version),\n \t).Get(\"/web/*\", func(w http.ResponseWriter, r *http.Request) {\n@@ -728,3 +728,16 @@ func parseError(err error, defaultCode int) (code int) {\n \n \treturn code\n }\n+\n+// rateLimiter creates a rate limiting middleware with proper IP lookup configuration.\n+// tollbooth v8 requires explicit IP lookup method to be set.\n+// uses RemoteAddr which is set by chi's middleware.RealIP to the real client IP\n+// from X-Forwarded-For, X-Real-IP, or True-Client-IP headers.\n+func rateLimiter(maxReq float64) func(http.Handler) http.Handler {\n+\tlmt := tollbooth.NewLimiter(maxReq, nil)\n+\tlmt.SetIPLookup(limiter.IPLookup{\n+\t\tName:           \"RemoteAddr\",\n+\t\tIndexFromRight: 0,\n+\t})\n+\treturn tollbooth.HTTPMiddleware(lmt)\n+}"
    },
    {
      "sha": "03f8a5261a26a70a95cc84c67960fed828880a91",
      "filename": "backend/app/rest/proxy/image.go",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fapp%2Frest%2Fproxy%2Fimage.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fapp%2Frest%2Fproxy%2Fimage.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fapp%2Frest%2Fproxy%2Fimage.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -12,7 +12,7 @@ import (\n \n \t\"github.com/PuerkitoBio/goquery\"\n \tlog \"github.com/go-pkgz/lgr\"\n-\t\"github.com/go-pkgz/repeater\"\n+\t\"github.com/go-pkgz/repeater/v2\"\n \n \t\"github.com/umputun/remark42/backend/app/rest\"\n \t\"github.com/umputun/remark42/backend/app/store/image\"\n@@ -151,7 +151,7 @@ func (p Image) downloadImage(ctx context.Context, imgURL string) ([]byte, error)\n \tclient := http.Client{Timeout: 30 * time.Second}\n \tdefer client.CloseIdleConnections()\n \tvar resp *http.Response\n-\terr := repeater.NewDefault(5, time.Second).Do(ctx, func() error {\n+\terr := repeater.NewFixed(5, time.Second).Do(ctx, func() error {\n \t\tvar e error\n \t\treq, e := http.NewRequest(\"GET\", imgURL, http.NoBody)\n \t\tif e != nil {"
    },
    {
      "sha": "87cb1f80ac5a908e0b805db3933be41dd3f0905a",
      "filename": "backend/go.mod",
      "status": "modified",
      "additions": 15,
      "deletions": 15,
      "changes": 30,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fgo.mod",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fgo.mod",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fgo.mod?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -6,16 +6,15 @@ require (\n \tgithub.com/Depado/bfchroma/v2 v2.0.0\n \tgithub.com/PuerkitoBio/goquery v1.11.0\n \tgithub.com/alecthomas/chroma/v2 v2.20.0\n-\tgithub.com/didip/tollbooth/v7 v7.0.2\n-\tgithub.com/didip/tollbooth_chi v0.0.0-20220719025231-d662a7f6928f\n+\tgithub.com/didip/tollbooth/v8 v8.0.1\n \tgithub.com/go-chi/chi/v5 v5.2.3\n \tgithub.com/go-chi/cors v1.2.2\n-\tgithub.com/go-pkgz/auth/v2 v2.0.1-0.20250415030422-4f9f2c5e3b0d\n+\tgithub.com/go-pkgz/auth/v2 v2.1.0\n \tgithub.com/go-pkgz/jrpc v0.4.0\n \tgithub.com/go-pkgz/lcw/v2 v2.0.0\n \tgithub.com/go-pkgz/lgr v0.12.1\n-\tgithub.com/go-pkgz/notify v1.2.0\n-\tgithub.com/go-pkgz/repeater v1.2.0\n+\tgithub.com/go-pkgz/notify v1.3.0\n+\tgithub.com/go-pkgz/repeater/v2 v2.2.0\n \tgithub.com/go-pkgz/rest v1.20.4\n \tgithub.com/go-pkgz/syncs v1.3.2\n \tgithub.com/golang-jwt/jwt/v5 v5.3.0\n@@ -37,35 +36,36 @@ require (\n )\n \n require (\n-\tcloud.google.com/go/compute/metadata v0.6.0 // indirect\n+\tcloud.google.com/go/compute/metadata v0.9.0 // indirect\n \tgithub.com/andybalholm/cascadia v1.3.3 // indirect\n \tgithub.com/aymerick/douceur v0.2.0 // indirect\n \tgithub.com/cespare/xxhash/v2 v2.3.0 // indirect\n \tgithub.com/davecgh/go-spew v1.1.1 // indirect\n \tgithub.com/dghubble/oauth1 v0.7.3 // indirect\n \tgithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect\n \tgithub.com/dlclark/regexp2 v1.11.5 // indirect\n-\tgithub.com/go-oauth2/oauth2/v4 v4.5.3 // indirect\n-\tgithub.com/go-pkgz/email v0.5.0 // indirect\n-\tgithub.com/go-pkgz/expirable-cache/v3 v3.0.0 // indirect\n+\tgithub.com/go-oauth2/oauth2/v4 v4.5.4 // indirect\n+\tgithub.com/go-pkgz/email v0.6.0 // indirect\n+\tgithub.com/go-pkgz/expirable-cache/v3 v3.1.0 // indirect\n+\tgithub.com/go-pkgz/repeater v1.2.0 // indirect\n \tgithub.com/go-pkgz/routegroup v1.6.0 // indirect\n \tgithub.com/golang/snappy v1.0.0 // indirect\n \tgithub.com/gorilla/css v1.0.1 // indirect\n \tgithub.com/gorilla/websocket v1.5.3 // indirect\n \tgithub.com/hashicorp/errwrap v1.1.0 // indirect\n \tgithub.com/hashicorp/golang-lru/v2 v2.0.7 // indirect\n-\tgithub.com/klauspost/compress v1.18.0 // indirect\n+\tgithub.com/klauspost/compress v1.18.2 // indirect\n \tgithub.com/montanaflynn/stats v0.7.1 // indirect\n \tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n-\tgithub.com/redis/go-redis/v9 v9.7.3 // indirect\n+\tgithub.com/redis/go-redis/v9 v9.17.2 // indirect\n \tgithub.com/rrivera/identicon v0.0.0-20240116195454-d5ba35832c0d // indirect\n-\tgithub.com/slack-go/slack v0.15.0 // indirect\n+\tgithub.com/slack-go/slack v0.17.3 // indirect\n \tgithub.com/xdg-go/pbkdf2 v1.0.0 // indirect\n-\tgithub.com/xdg-go/scram v1.1.2 // indirect\n+\tgithub.com/xdg-go/scram v1.2.0 // indirect\n \tgithub.com/xdg-go/stringprep v1.0.4 // indirect\n \tgithub.com/youmark/pkcs8 v0.0.0-20240726163527-a2c0da244d78 // indirect\n-\tgo.mongodb.org/mongo-driver v1.17.3 // indirect\n-\tgolang.org/x/oauth2 v0.29.0 // indirect\n+\tgo.mongodb.org/mongo-driver v1.17.6 // indirect\n+\tgolang.org/x/oauth2 v0.33.0 // indirect\n \tgolang.org/x/sync v0.18.0 // indirect\n \tgolang.org/x/sys v0.38.0 // indirect\n \tgolang.org/x/text v0.31.0 // indirect"
    },
    {
      "sha": "7cb27b45d96f24b7b7c102d6e851153c5e4bc333",
      "filename": "backend/go.sum",
      "status": "modified",
      "additions": 36,
      "deletions": 139,
      "changes": 175,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fgo.sum",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fgo.sum",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fgo.sum?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -1,6 +1,5 @@\n-cloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\n-cloud.google.com/go/compute/metadata v0.6.0 h1:A6hENjEsCDtC1k8byVsgwvVcioamEHvZ4j01OwKxG9I=\n-cloud.google.com/go/compute/metadata v0.6.0/go.mod h1:FjyFAW1MW0C203CEOMDTu3Dk1FlqW3Rga40jzHL4hfg=\n+cloud.google.com/go/compute/metadata v0.9.0 h1:pDUj4QMoPejqq20dK0Pg2N4yG9zIkYGdBtwLoEkH9Zs=\n+cloud.google.com/go/compute/metadata v0.9.0/go.mod h1:E0bWwX5wTnLPedCKqk3pJmVgCBSM6qQI1yTBdEb3C10=\n github.com/Depado/bfchroma/v2 v2.0.0 h1:IRpN9BPkNwEpR6w1ectIcNWOuhDSLx+8f1pn83fzxx8=\n github.com/Depado/bfchroma/v2 v2.0.0/go.mod h1:wFwW/Pw8Tnd0irzgO9Zxtxgzp3aPS8qBWlyadxujxmw=\n github.com/PuerkitoBio/goquery v1.11.0 h1:jZ7pwMQXIITcUXNH83LLk+txlaEy6NVOfTuP43xxfqw=\n@@ -27,94 +26,71 @@ github.com/bsm/ginkgo/v2 v2.12.0 h1:Ny8MWAHyOepLGlLKYmXG4IEkioBysk6GpaRTLC8zwWs=\n github.com/bsm/ginkgo/v2 v2.12.0/go.mod h1:SwYbGRRDovPVboqFv0tPTcG1sN61LM1Z4ARdbAV9g4c=\n github.com/bsm/gomega v1.27.10 h1:yeMWxP2pV2fG3FgAODIY8EiRE3dy0aeFYt4l7wh6yKA=\n github.com/bsm/gomega v1.27.10/go.mod h1:JyEr/xRbxbtgWNi8tIEVPUYZ5Dzef52k01W3YH0H+O0=\n-github.com/bytedance/gopkg v0.0.0-20221122125632-68358b8ecec6/go.mod h1:5FoAH5xUHHCMDvQPy1rnj8moqLkLHFaDVBjHhcFwEi0=\n github.com/cespare/xxhash/v2 v2.3.0 h1:UL815xU9SqsFlibzuggzjXhog7bL6oX9BbNZnL2UFvs=\n github.com/cespare/xxhash/v2 v2.3.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\n-github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\n github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\n github.com/dghubble/oauth1 v0.7.3 h1:EkEM/zMDMp3zOsX2DC/ZQ2vnEX3ELK0/l9kb+vs4ptE=\n github.com/dghubble/oauth1 v0.7.3/go.mod h1:oxTe+az9NSMIucDPDCCtzJGsPhciJV33xocHfcR2sVY=\n github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f h1:lO4WD4F/rVNCu3HqELle0jiPLLBs70cWOduZpkS1E78=\n github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f/go.mod h1:cuUVRXasLTGF7a8hSLbxyZXjz+1KgoB3wDUb6vlszIc=\n-github.com/didip/tollbooth/v7 v7.0.0/go.mod h1:VZhDSGl5bDSPj4wPsih3PFa4Uh9Ghv8hgacaTm5PRT4=\n-github.com/didip/tollbooth/v7 v7.0.2 h1:WYEfusYI6g64cN0qbZgekDrYfuYBZjUZd5+RlWi69p4=\n-github.com/didip/tollbooth/v7 v7.0.2/go.mod h1:RtRYfEmFGX70+ike5kSndSvLtQ3+F2EAmTI4Un/VXNc=\n-github.com/didip/tollbooth_chi v0.0.0-20220719025231-d662a7f6928f h1:jtKwihcLmUC9BAhoJ9adCUqdSSZcOdH2KL7mPTUm2aw=\n-github.com/didip/tollbooth_chi v0.0.0-20220719025231-d662a7f6928f/go.mod h1:q9C80dnsuVRP2dAskjnXRNWdUJqtGgwG9wNrzt0019s=\n+github.com/didip/tollbooth/v8 v8.0.1 h1:VAAapTo1t4Bn6bbpcHjuovwoa9u3JH++wgjbpWv+rB8=\n+github.com/didip/tollbooth/v8 v8.0.1/go.mod h1:oEd9l+ep373d7DmvKLc0a5gasPOev2mTewi6KPQBGJ4=\n github.com/dlclark/regexp2 v1.11.5 h1:Q/sSnsKerHeCkc/jSTNq1oCm7KiVgUMZRDUoRu0JQZQ=\n github.com/dlclark/regexp2 v1.11.5/go.mod h1:DHkYz0B9wPfa6wondMfaivmHpzrQ3v9q8cnmRbL6yW8=\n-github.com/fasthttp-contrib/websocket v0.0.0-20160511215533-1f3b11f56072/go.mod h1:duJ4Jxv5lDcvg4QuQr0oowTf7dz4/CR8NtyCooz9HL8=\n github.com/fatih/structs v1.1.0 h1:Q7juDM0QtcnhCpeyLGQKyg4TOIghuNXrkL32pHAUMxo=\n github.com/fatih/structs v1.1.0/go.mod h1:9NiDSp5zOcgEDl+j00MP/WkGVPOlPRLejGD8Ga6PJ7M=\n-github.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=\n-github.com/fsnotify/fsnotify v1.4.9/go.mod h1:znqG4EE+3YCdAaPaxE2ZRY/06pZUdp0tY4IgpuI1SZQ=\n github.com/gavv/httpexpect v2.0.0+incompatible h1:1X9kcRshkSKEjNJJxX9Y9mQ5BRfbxU5kORdjhlA1yX8=\n github.com/gavv/httpexpect v2.0.0+incompatible/go.mod h1:x+9tiU1YnrOvnB725RkpoLv1M62hOWzwo5OXotisrKc=\n github.com/go-chi/chi/v5 v5.2.3 h1:WQIt9uxdsAbgIYgid+BpYc+liqQZGMHRaUwp0JUcvdE=\n github.com/go-chi/chi/v5 v5.2.3/go.mod h1:L2yAIGWB3H+phAw1NxKwWM+7eUH/lU8pOMm5hHcoops=\n github.com/go-chi/cors v1.2.2 h1:Jmey33TE+b+rB7fT8MUy1u0I4L+NARQlK6LhzKPSyQE=\n github.com/go-chi/cors v1.2.2/go.mod h1:sSbTewc+6wYHBBCW7ytsFSn836hqM7JxpglAy2Vzc58=\n-github.com/go-oauth2/oauth2/v4 v4.5.3 h1:lQt7O9KOnu/v4awe166FH7+p8tFUXQyR+no6nctAKU0=\n-github.com/go-oauth2/oauth2/v4 v4.5.3/go.mod h1:ryzb7zr8fdQBlciD0+tcnEWeOok5B0J8V/DniwYqQ2k=\n-github.com/go-pkgz/auth/v2 v2.0.1-0.20250415030422-4f9f2c5e3b0d h1:xyTp7JRaHy3/ttpcNiJO/YzxENZcvxcZL8iE3QfssnY=\n-github.com/go-pkgz/auth/v2 v2.0.1-0.20250415030422-4f9f2c5e3b0d/go.mod h1:YTCEC2cBE2ugyRf0ePew4NZIJE5ozglI06/IokmtiwU=\n-github.com/go-pkgz/email v0.5.0 h1:fdtMDGJ8NwyBACLR0LYHaCIK/OeUwZHMhH7Q0+oty9U=\n-github.com/go-pkgz/email v0.5.0/go.mod h1:BdxglsQnymzhfdbnncEE72a6DrucZHy6I+42LK2jLEc=\n-github.com/go-pkgz/expirable-cache v0.1.0/go.mod h1:GTrEl0X+q0mPNqN6dtcQXksACnzCBQ5k/k1SwXJsZKs=\n-github.com/go-pkgz/expirable-cache/v3 v3.0.0 h1:u3/gcu3sabLYiTCevoRKv+WzjIn5oo7P8XtiXBeRDLw=\n-github.com/go-pkgz/expirable-cache/v3 v3.0.0/go.mod h1:2OQiDyEGQalYecLWmXprm3maPXeVb5/6/X7yRPYTzec=\n+github.com/go-oauth2/oauth2/v4 v4.5.4 h1:YjI0tmGW8oxVhn9QSBIxlr641QugWrJY5UWa6XmLcW0=\n+github.com/go-oauth2/oauth2/v4 v4.5.4/go.mod h1:BXiOY+QZtZy2ewbsGk2B5P8TWmtz/Rf7ES5ZttQFxfQ=\n+github.com/go-pkgz/auth/v2 v2.1.0 h1:ECUVtCWzJG9yDNMVWh1tX/qoDXaUpwSROyZ4VzmoAhI=\n+github.com/go-pkgz/auth/v2 v2.1.0/go.mod h1:adiVuMWvZ07wgLLWg1xjG+GQIn8wLty8UEXkjEOsZmw=\n+github.com/go-pkgz/email v0.6.0 h1:snZnXldjeF4PgKSjnx9Fa25mtOgFpAOEeWvnQvrxjLE=\n+github.com/go-pkgz/email v0.6.0/go.mod h1:+wgi4x7S33IuCzfcCM5euN0GwQG6XvO/PBLxrNffYLI=\n+github.com/go-pkgz/expirable-cache/v3 v3.1.0 h1:s05P851/O6QJ6Mc+7o2bh9aGtD3romB1SxDTXifdoqc=\n+github.com/go-pkgz/expirable-cache/v3 v3.1.0/go.mod h1:6pVgNleydKPj0J2/mzrI02/RDo4ivKx5v2XlNmIjhjo=\n github.com/go-pkgz/jrpc v0.4.0 h1:oD7xiGrzDkndkuCjeHGugQXxbggLSV7O1QmHhoc5pYY=\n github.com/go-pkgz/jrpc v0.4.0/go.mod h1:JFoY3bRjRyx4M3CbEVDFQStMB1m2gmQ7OjqFK7q3kOo=\n github.com/go-pkgz/lcw/v2 v2.0.0 h1:gTwXpiJBhQeA1rXuqkRuLcV79uATFna8CckH8ZBBrH0=\n github.com/go-pkgz/lcw/v2 v2.0.0/go.mod h1:yxJHOn+IbQBQHxUqkCtMrbGjIfdYcsBAZcVCBaL1Va8=\n github.com/go-pkgz/lgr v0.12.1 h1:8GVfG2rSARq3Eaj5PP158rtBR2LHVGkwioIkQBGbvKg=\n github.com/go-pkgz/lgr v0.12.1/go.mod h1:A4AxjOthFVFK6jRnVYMeusno5SeDAxcLVHd0kI/lN/Y=\n-github.com/go-pkgz/notify v1.2.0 h1:jqbsbWkodCDB9ffyI9fCG1bTSgidWJRS5UWD/twjU44=\n-github.com/go-pkgz/notify v1.2.0/go.mod h1:BTHj9ly7xZpaxg91lBdxCxSmkpVQ9R6q5QviX/upeYo=\n+github.com/go-pkgz/notify v1.3.0 h1:YxF/ThEoCetdcoghWdyeqaBpCkZ8mvyve7HXbCAOzYU=\n+github.com/go-pkgz/notify v1.3.0/go.mod h1:qdfi5OsViKlIFPryIOaINHTOtS9GFhOYXPqJmAMlaGU=\n github.com/go-pkgz/repeater v1.2.0 h1:oJFvjyKdTDd5RCzpzxlzYIZFFj6Zfl17rE1aUfu6UjQ=\n github.com/go-pkgz/repeater v1.2.0/go.mod h1:vypP6xamA53MFmafnGUucqOmALKk36xgKu2hSG73LHM=\n+github.com/go-pkgz/repeater/v2 v2.2.0 h1:8nZR/NaknmLfx2YMHbr78u9OL4Xj+8+romm9dz4FpMg=\n+github.com/go-pkgz/repeater/v2 v2.2.0/go.mod h1:RgX5vUbLKq7PV82QUDP5pFbQS1os4Z+U9XzKymK23A8=\n github.com/go-pkgz/rest v1.20.4 h1:8ufcP1IqoDhCvIFdXPtvyX4HSS16SM6THBe2a6L0/kg=\n github.com/go-pkgz/rest v1.20.4/go.mod h1:2/LEZGndSxpVvExsMn48AjUgiTn6kILqjpoaRnl62JU=\n github.com/go-pkgz/routegroup v1.6.0 h1:44XHZgF6JIIldRlv+zjg6SygULASmjifnfIQjwCT0e4=\n github.com/go-pkgz/routegroup v1.6.0/go.mod h1:Pmu04fhgWhRtBMIJ8HXppnnzOPjnL/IEPBIdO2zmeqg=\n github.com/go-pkgz/syncs v1.3.2 h1:gmioASlJNy3gNosPlgvWOM2QP0Hdjzn2u+/sUShgd8E=\n github.com/go-pkgz/syncs v1.3.2/go.mod h1:qjgzpp7OpuhDf7BWsW/FHCu9DLjE32NPy6/vXAXT/Cw=\n-github.com/go-session/session/v3 v3.2.1/go.mod h1:RftEBbyuzqkNCAxIrCLJe+rfBqB/4G11qxq9KYKrx4M=\n-github.com/go-test/deep v1.0.4 h1:u2CU3YKy9I2pmu9pX0eq50wCgjfGIt539SqR7FbHiho=\n-github.com/go-test/deep v1.0.4/go.mod h1:wGDj63lr65AM2AQyKZd/NYHGb0R+1RLqB8NKt3aSFNA=\n-github.com/golang-jwt/jwt v3.2.2+incompatible/go.mod h1:8pz2t5EyA70fFQQSrl6XZXzqecmYZeUEB8OUGHkxJ+I=\n+github.com/go-test/deep v1.1.1 h1:0r/53hagsehfO4bzD2Pgr/+RgHqhmf+k1Bpse2cTu1U=\n+github.com/go-test/deep v1.1.1/go.mod h1:5C2ZWiW0ErCdrYzpqxLbTX7MG14M9iiw8DgHncVwcsE=\n github.com/golang-jwt/jwt/v5 v5.3.0 h1:pv4AsKCKKZuqlgs5sUmn4x8UlGa0kEVt/puTpKx9vvo=\n github.com/golang-jwt/jwt/v5 v5.3.0/go.mod h1:fxCRLWMO43lRc8nhHWY6LGqRcf+1gQWArsqaEUEa5bE=\n-github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n-github.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\n-github.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\n-github.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\n-github.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\n-github.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\n-github.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\n-github.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\n github.com/golang/snappy v1.0.0 h1:Oy607GVXHs7RtbggtPBnr2RmDArIsAefDwvrdWvRhGs=\n github.com/golang/snappy v1.0.0/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\n-github.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\n-github.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\n-github.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\n-github.com/google/go-cmp v0.5.7/go.mod h1:n+brtR0CgQNWTVd5ZUFpTBC8YFBDLK/h/bpaJ8/DtOE=\n-github.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=\n github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\n+github.com/google/go-cmp v0.7.0 h1:wk8382ETsv4JYUZwIsn6YpYiWiBsYLSJiTsyBybVuN8=\n+github.com/google/go-cmp v0.7.0/go.mod h1:pXiqmnSA92OHEEa9HXL2W4E7lf9JzCmGVUdgjX3N/iU=\n github.com/google/go-querystring v1.0.0 h1:Xkwi/a1rcvNg1PPYe5vI8GbeBY/jrVuDX5ASuANWTrk=\n github.com/google/go-querystring v1.0.0/go.mod h1:odCYkC5MyYFN7vkCjXpyrEuKhc/BUO6wN/zVPAxq5ck=\n-github.com/google/uuid v1.1.1/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\n github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=\n github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\n-github.com/gopherjs/gopherjs v0.0.0-20181017120253-0766667cb4d1/go.mod h1:wJfORRmW1u3UXTncJ5qlYoELFm8eSnnEO6hX4iZ3EWY=\n github.com/gopherjs/gopherjs v0.0.0-20200217142428-fce0ec30dd00 h1:l5lAOZEym3oK3SQ2HBHWsJUfbNBiTXJDeW2QDxw9AQ0=\n github.com/gopherjs/gopherjs v0.0.0-20200217142428-fce0ec30dd00/go.mod h1:wJfORRmW1u3UXTncJ5qlYoELFm8eSnnEO6hX4iZ3EWY=\n github.com/gorilla/css v1.0.1 h1:ntNaBIghp6JmvWnxbZKANoLyuXTPZ4cAMlo6RyhlbO8=\n github.com/gorilla/css v1.0.1/go.mod h1:BvnYkspnSzMmwRK+b8/xgNPLiIuNZr6vbZBTPQ2A3b0=\n github.com/gorilla/feeds v1.2.0 h1:O6pBiXJ5JHhPvqy53NsjKOThq+dNFm8+DFrxBEdzSCc=\n github.com/gorilla/feeds v1.2.0/go.mod h1:WMib8uJP3BbY+X8Szd1rA5Pzhdfh+HCCAYT2z7Fza6Y=\n-github.com/gorilla/websocket v1.4.2/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=\n github.com/gorilla/websocket v1.5.3 h1:saDtZ6Pbx/0u+bgYQ3q96pZgCzfhKXGPqt7kZ72aNNg=\n github.com/gorilla/websocket v1.5.3/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=\n github.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\n@@ -126,44 +102,30 @@ github.com/hashicorp/golang-lru/v2 v2.0.7 h1:a+bsQ5rvGLjzHuww6tVxozPZFVghXaHOwFs\n github.com/hashicorp/golang-lru/v2 v2.0.7/go.mod h1:QeFd9opnmA6QUJc5vARoKUSoFhyfM2/ZepoAG6RGpeM=\n github.com/hexops/gotextdiff v1.0.3 h1:gitA9+qJrrTCsiCl7+kh75nPqQt1cx4ZkudSTLoUqJM=\n github.com/hexops/gotextdiff v1.0.3/go.mod h1:pSWU5MAI3yDq+fZBTazCSJysOMbxWL1BSow5/V2vxeg=\n-github.com/hpcloud/tail v1.0.0/go.mod h1:ab1qPbhIpdTxEkNHXyeSf5vhxWSCs/tWer42PpOxQnU=\n github.com/imkira/go-interpol v1.1.0 h1:KIiKr0VSG2CUW1hl1jpiyuzuJeKUUpC8iM1AIE7N1Vk=\n github.com/imkira/go-interpol v1.1.0/go.mod h1:z0h2/2T3XF8kyEPpRgJ3kmNv+C43p+I/CoI+jC3w2iA=\n github.com/jessevdk/go-flags v1.6.1 h1:Cvu5U8UGrLay1rZfv/zP7iLpSHGUZ/Ou68T0iX1bBK4=\n github.com/jessevdk/go-flags v1.6.1/go.mod h1:Mk8T1hIAWpOiJiHa9rJASDK2UGWji0EuPGBnNLMooyc=\n github.com/jtolds/gls v4.20.0+incompatible h1:xdiiI2gbIgH/gLH7ADydsJ1uDOEzR8yvV7C0MuV77Wo=\n github.com/jtolds/gls v4.20.0+incompatible/go.mod h1:QJZ7F/aHp+rZTRtaJ1ow/lLfFfVYBRgL+9YlvaHOwJU=\n-github.com/k0kubun/colorstring v0.0.0-20150214042306-9440f1994b88/go.mod h1:3w7q1U84EfirKl04SVQ/s7nPm1ZPhiXd34z40TNz36k=\n-github.com/klauspost/compress v1.15.0/go.mod h1:/3/Vjq9QcHkK5uEr5lBEmyoZ1iFhe47etQ6QUkpK6sk=\n-github.com/klauspost/compress v1.18.0 h1:c/Cqfb0r+Yi+JtIEq73FWXVkRonBlf0CRNYc8Zttxdo=\n-github.com/klauspost/compress v1.18.0/go.mod h1:2Pp+KzxcywXVXMr50+X0Q/Lsb43OQHYWRCY2AiWywWQ=\n-github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\n+github.com/klauspost/compress v1.18.2 h1:iiPHWW0YrcFgpBYhsA6D1+fqHssJscY/Tm/y2Uqnapk=\n+github.com/klauspost/compress v1.18.2/go.mod h1:R0h/fSBs8DE4ENlcrlib3PsXS61voFxhIs2DeRhCvJ4=\n github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=\n github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=\n-github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\n-github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\n github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=\n github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=\n github.com/kyokomi/emoji/v2 v2.2.13 h1:GhTfQa67venUUvmleTNFnb+bi7S3aocF7ZCXU9fSO7U=\n github.com/kyokomi/emoji/v2 v2.2.13/go.mod h1:JUcn42DTdsXJo1SWanHh4HKDEyPaR5CqkmoirZZP9qE=\n-github.com/mattn/go-colorable v0.1.7/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=\n-github.com/mattn/go-isatty v0.0.12/go.mod h1:cbi8OIDigv2wuxKPP5vlRcQ1OAZbq2CE4Kysco4FUpU=\n github.com/microcosm-cc/bluemonday v1.0.27 h1:MpEUotklkwCSLeH+Qdx1VJgNqLlpY2KXwXFM08ygZfk=\n github.com/microcosm-cc/bluemonday v1.0.27/go.mod h1:jFi9vgW+H7c3V0lb6nR74Ib/DIB5OBs92Dimizgw2cA=\n github.com/montanaflynn/stats v0.7.1 h1:etflOAAHORrCC44V+aR6Ftzort912ZU+YLiSTuV8eaE=\n github.com/montanaflynn/stats v0.7.1/go.mod h1:etXPPgVO6n31NxCd9KQUMvCM+ve0ruNzt6R8Bnaayow=\n github.com/moul/http2curl v1.0.0 h1:dRMWoAtb+ePxMlLkrCbAqh4TlPHXvoGUSQ323/9Zahs=\n github.com/moul/http2curl v1.0.0/go.mod h1:8UbvGypXm98wA/IqH45anm5Y2Z6ep6O31QGOAZ3H0fQ=\n-github.com/nxadm/tail v1.4.4/go.mod h1:kenIhsEOeOJmVchQTgglprH7qJGnHDVpk1VPCcaMI8A=\n-github.com/onsi/ginkgo v1.6.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\n-github.com/onsi/ginkgo v1.12.1/go.mod h1:zj2OWP4+oCPe1qIXoGWkgMRwljMUYCdkwsT2108oapk=\n-github.com/onsi/ginkgo v1.13.0/go.mod h1:+REjRxOmWfHCjfv9TTWB1jD1Frx4XydAD3zm1lskyM0=\n-github.com/onsi/gomega v1.7.1/go.mod h1:XdKZgCCFLUoM/7CFJVPcG8C1xQ1AJ0vpAezJrB7JYyY=\n-github.com/onsi/gomega v1.10.1/go.mod h1:iN09h71vgCQne3DLsj+A5owkum+a2tYe+TOCB1ybHNo=\n github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\n github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\n-github.com/redis/go-redis/v9 v9.7.3 h1:YpPyAayJV+XErNsatSElgRZZVCwXX9QzkKYNvO7x0wM=\n-github.com/redis/go-redis/v9 v9.7.3/go.mod h1:bGUrSggJ9X9GUmZpZNEOQKaANxSGgOEBRltRTZHSvrA=\n+github.com/redis/go-redis/v9 v9.17.2 h1:P2EGsA4qVIM3Pp+aPocCJ7DguDHhqrXNhVcEp4ViluI=\n+github.com/redis/go-redis/v9 v9.17.2/go.mod h1:u410H11HMLoB+TP67dz8rL9s6QW2j76l0//kSOd3370=\n github.com/rogpeppe/go-internal v1.9.0 h1:73kH8U+JUqXU8lRuOHeVHaa/SZPifC7BkcraZVejAe8=\n github.com/rogpeppe/go-internal v1.9.0/go.mod h1:WtVeX8xhTBvf0smdhujwtBcq4Qrzq/fJaraNFVN+nFs=\n github.com/rrivera/identicon v0.0.0-20240116195454-d5ba35832c0d h1:l3+2LWCbVxn5itfvXAfH9n4YL9jh8l1g5zcncbIc1cs=\n@@ -172,61 +134,42 @@ github.com/rs/xid v1.6.0 h1:fV591PaemRlL6JfRxGDEPl69wICngIQ3shQtzfy2gxU=\n github.com/rs/xid v1.6.0/go.mod h1:7XoLgs4eV+QndskICGsho+ADou8ySMSjJKDIan90Nz0=\n github.com/russross/blackfriday/v2 v2.1.0 h1:JIOH55/0cWyOuilr9/qlrm0BSXldqnqwMsf35Ld67mk=\n github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=\n-github.com/sclevine/agouti v3.0.0+incompatible/go.mod h1:b4WX9W9L1sfQKXeJf1mUTLZKJ48R1S7H23Ji7oFO5Bw=\n github.com/sergi/go-diff v1.1.0 h1:we8PVUC3FE2uYfodKH/nBHMSetSfHDR6scGdBi+erh0=\n github.com/sergi/go-diff v1.1.0/go.mod h1:STckp+ISIX8hZLjrqAeVduY0gWCT9IjLuqbuNXdaHfM=\n github.com/skip2/go-qrcode v0.0.0-20200617195104-da1b6568686e h1:MRM5ITcdelLK2j1vwZ3Je0FKVCfqOLp5zO6trqMLYs0=\n github.com/skip2/go-qrcode v0.0.0-20200617195104-da1b6568686e/go.mod h1:XV66xRDqSt+GTGFMVlhk3ULuV0y9ZmzeVGR4mloJI3M=\n-github.com/slack-go/slack v0.15.0 h1:LE2lj2y9vqqiOf+qIIy0GvEoxgF1N5yLGZffmEZykt0=\n-github.com/slack-go/slack v0.15.0/go.mod h1:hlGi5oXA+Gt+yWTPP0plCdRKmjsDxecdHxYQdlMQKOw=\n-github.com/smartystreets/assertions v0.0.0-20180927180507-b2de0cb4f26d/go.mod h1:OnSkiWE9lh6wB0YB77sQom3nweQdgAjqCqsofrRNTgc=\n+github.com/slack-go/slack v0.17.3 h1:zV5qO3Q+WJAQ/XwbGfNFrRMaJ5T/naqaonyPV/1TP4g=\n+github.com/slack-go/slack v0.17.3/go.mod h1:X+UqOufi3LYQHDnMG1vxf0J8asC6+WllXrVrhl8/Prk=\n github.com/smartystreets/assertions v1.1.0 h1:MkTeG1DMwsrdH7QtLXy5W+fUxWq+vmb6cLmyJ7aRtF0=\n github.com/smartystreets/assertions v1.1.0/go.mod h1:tcbTF8ujkAEcZ8TElKY+i30BzYlVhC/LOxJk7iOWnoo=\n github.com/smartystreets/goconvey v1.6.4 h1:fv0U8FUIMPNf1L9lnHLvLhgicrIVChEkdzIKYqbNC9s=\n github.com/smartystreets/goconvey v1.6.4/go.mod h1:syvi0/a8iFYH4r/RixwvyeAJjdLS9QV7WQ/tjFTllLA=\n-github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\n-github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\n-github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\n-github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\n-github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\n-github.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\n github.com/stretchr/testify v1.11.1 h1:7s2iGBzp5EwR7/aIZr8ao5+dra3wiQyKjjFuvgVKu7U=\n github.com/stretchr/testify v1.11.1/go.mod h1:wZwfW3scLgRK+23gO65QZefKpKQRnfz6sD981Nm4B6U=\n-github.com/tidwall/btree v0.0.0-20191029221954-400434d76274/go.mod h1:huei1BkDWJ3/sLXmO+bsCNELL+Bp2Kks9OLyQFkzvA8=\n-github.com/tidwall/btree v1.7.0 h1:L1fkJH/AuEh5zBnnBbmTwQ5Lt+bRJ5A8EWecslvo9iI=\n-github.com/tidwall/btree v1.7.0/go.mod h1:twD9XRA5jj9VUQGELzDO4HPQTNJsoWWfYEL+EUQ2cKY=\n-github.com/tidwall/buntdb v1.1.2/go.mod h1:xAzi36Hir4FarpSHyfuZ6JzPJdjRZ8QlLZSntE2mqlI=\n+github.com/tidwall/btree v1.8.1 h1:27ehoXvm5AG/g+1VxLS1SD3vRhp/H7LuEfwNvddEdmA=\n+github.com/tidwall/btree v1.8.1/go.mod h1:jBbTdUWhSZClZWoDg54VnvV7/54modSOzDN7VXftj1A=\n github.com/tidwall/buntdb v1.3.2 h1:qd+IpdEGs0pZci37G4jF51+fSKlkuUTMXuHhXL1AkKg=\n github.com/tidwall/buntdb v1.3.2/go.mod h1:lZZrZUWzlyDJKlLQ6DKAy53LnG7m5kHyrEHvvcDmBpU=\n-github.com/tidwall/gjson v1.3.4/go.mod h1:P256ACg0Mn+j1RXIDXoss50DeIABTYK1PULOJHhxOls=\n-github.com/tidwall/gjson v1.12.1/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=\n github.com/tidwall/gjson v1.18.0 h1:FIDeeyB800efLX89e5a8Y0BNH+LOngJyGrIWxG2FKQY=\n github.com/tidwall/gjson v1.18.0/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=\n-github.com/tidwall/grect v0.0.0-20161006141115-ba9a043346eb/go.mod h1:lKYYLFIr9OIgdgrtgkZ9zgRxRdvPYsExnYBsEAd8W5M=\n github.com/tidwall/grect v0.1.4 h1:dA3oIgNgWdSspFzn1kS4S/RDpZFLrIxAZOdJKjYapOg=\n github.com/tidwall/grect v0.1.4/go.mod h1:9FBsaYRaR0Tcy4UwefBX/UDcDcDy9V5jUcxHzv2jd5Q=\n-github.com/tidwall/match v1.0.1/go.mod h1:LujAq0jyVjBy028G1WhWfIzbpQfMO8bBZ6Tyb0+pL9E=\n-github.com/tidwall/match v1.1.1 h1:+Ho715JplO36QYgwN9PGYNhgZvoUSc9X2c80KVTi+GA=\n-github.com/tidwall/match v1.1.1/go.mod h1:eRSPERbgtNPcGhD8UCthc6PmLEQXEWd3PRB5JTxsfmM=\n-github.com/tidwall/pretty v1.0.0/go.mod h1:XNkn88O1ChpSDQmQeStsy+sBenx6DDtFZJxhVysOjyk=\n-github.com/tidwall/pretty v1.2.0/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=\n+github.com/tidwall/match v1.2.0 h1:0pt8FlkOwjN2fPt4bIl4BoNxb98gGHN2ObFEDkrfZnM=\n+github.com/tidwall/match v1.2.0/go.mod h1:eRSPERbgtNPcGhD8UCthc6PmLEQXEWd3PRB5JTxsfmM=\n github.com/tidwall/pretty v1.2.1 h1:qjsOFOWWQl+N3RsoF5/ssm1pHmJJwhjlSbZ51I6wMl4=\n github.com/tidwall/pretty v1.2.1/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=\n github.com/tidwall/rtred v0.1.2 h1:exmoQtOLvDoO8ud++6LwVsAMTu0KPzLTUrMln8u1yu8=\n github.com/tidwall/rtred v0.1.2/go.mod h1:hd69WNXQ5RP9vHd7dqekAz+RIdtfBogmglkZSRxCHFQ=\n-github.com/tidwall/rtree v0.0.0-20180113144539-6cd427091e0e/go.mod h1:/h+UnNGt0IhNNJLkGikcdcJqm66zGD/uJGMRxK/9+Ao=\n-github.com/tidwall/tinyqueue v0.0.0-20180302190814-1e39f5511563/go.mod h1:mLqSmt7Dv/CNneF2wfcChfN1rvapyQr01LGKnKex0DQ=\n github.com/tidwall/tinyqueue v0.1.1 h1:SpNEvEggbpyN5DIReaJ2/1ndroY8iyEGxPYxoSaymYE=\n github.com/tidwall/tinyqueue v0.1.1/go.mod h1:O/QNHwrnjqr6IHItYrzoHAKYhBkLI67Q096fQP5zMYw=\n github.com/valyala/bytebufferpool v1.0.0 h1:GqA5TC/0021Y/b9FG4Oi9Mr3q7XYx6KllzawFIhcdPw=\n github.com/valyala/bytebufferpool v1.0.0/go.mod h1:6bBcMArwyJ5K/AmCkWv1jt77kVWyCJ6HpOuEn7z0Csc=\n github.com/valyala/fasthttp v1.34.0 h1:d3AAQJ2DRcxJYHm7OXNXtXt2as1vMDfxeIcFvhmGGm4=\n github.com/valyala/fasthttp v1.34.0/go.mod h1:epZA5N+7pY6ZaEKRmstzOuYJx9HI8DI1oaCGZpdH4h0=\n-github.com/valyala/tcplisten v1.0.0/go.mod h1:T0xQ8SeCZGxckz9qRXTfG43PvQ/mcWh7FwZEA7Ioqkc=\n github.com/xdg-go/pbkdf2 v1.0.0 h1:Su7DPu48wXMwC3bs7MCNG+z4FhcyEuz5dlvchbq0B0c=\n github.com/xdg-go/pbkdf2 v1.0.0/go.mod h1:jrpuAogTd400dnrH08LKmI/xc1MbPOebTwRqcT5RDeI=\n-github.com/xdg-go/scram v1.1.2 h1:FHX5I5B4i4hKRVRBCFRxq1iQRej7WO3hhBuJf+UUySY=\n-github.com/xdg-go/scram v1.1.2/go.mod h1:RT/sEzTbU5y00aCK8UOx6R7YryM0iF1N2MOmC3kKLN4=\n+github.com/xdg-go/scram v1.2.0 h1:bYKF2AEwG5rqd1BumT4gAnvwU/M9nBp2pTSxeZw7Wvs=\n+github.com/xdg-go/scram v1.2.0/go.mod h1:3dlrS0iBaWKYVt2ZfA4cj48umJZ+cAEbR6/SjLA88I8=\n github.com/xdg-go/stringprep v1.0.4 h1:XLI/Ng3O1Atzq0oBs3TWm+5ZVgkq2aqdlvP9JtoZ6c8=\n github.com/xdg-go/stringprep v1.0.4/go.mod h1:mPGuuIYwz7CmR2bT9j4GbQqutWS1zV24gijq1dTyGkM=\n github.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f h1:J9EGpcZtP0E/raorCMxlFGSTBrsSlaDGf3jU/qvAE2c=\n@@ -243,19 +186,17 @@ github.com/yudai/gojsondiff v1.0.0 h1:27cbfqXLVEJ1o8I6v3y9lg8Ydm53EKqHXAOMxEGlCO\n github.com/yudai/gojsondiff v1.0.0/go.mod h1:AY32+k2cwILAkW1fbgxQ5mUmMiZFgLIV+FBNExI05xg=\n github.com/yudai/golcs v0.0.0-20170316035057-ecda9a501e82 h1:BHyfKlQyqbsFN5p3IfnEUduWvb9is428/nNb5L3U01M=\n github.com/yudai/golcs v0.0.0-20170316035057-ecda9a501e82/go.mod h1:lgjkn3NuSvDfVJdfcVVdX+jpBxNmX4rDAzaS45IcYoM=\n-github.com/yudai/pp v2.0.1+incompatible/go.mod h1:PuxR/8QJ7cyCkFp/aUDS+JY727OFEZkTdatxwunjIkc=\n github.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=\n github.com/yuin/gopher-lua v1.1.1 h1:kYKnWBjvbNP4XLT3+bPEwAXJx262OhaHDWDVOPjL46M=\n github.com/yuin/gopher-lua v1.1.1/go.mod h1:GBR0iDaNXjAgGg9zfCvksxSRnQx76gclCIb7kdAd1Pw=\n go.etcd.io/bbolt v1.4.3 h1:dEadXpI6G79deX5prL3QRNP6JB8UxVkqo4UPnHaNXJo=\n go.etcd.io/bbolt v1.4.3/go.mod h1:tKQlpPaYCVFctUIgFKFnAlvbmB3tpy1vkTnDWohtc0E=\n-go.mongodb.org/mongo-driver v1.17.3 h1:TQyXhnsWfWtgAhMtOgtYHMTkZIfBTpMTsMnd9ZBeHxQ=\n-go.mongodb.org/mongo-driver v1.17.3/go.mod h1:Hy04i7O2kC4RS06ZrhPRqj/u4DTYkFDAAccj+rVKqgQ=\n+go.mongodb.org/mongo-driver v1.17.6 h1:87JUG1wZfWsr6rIz3ZmpH90rL5tea7O3IHuSwHUpsss=\n+go.mongodb.org/mongo-driver v1.17.6/go.mod h1:Hy04i7O2kC4RS06ZrhPRqj/u4DTYkFDAAccj+rVKqgQ=\n go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=\n go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=\n golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\n golang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\n-golang.org/x/crypto v0.0.0-20220214200702-86341886e292/go.mod h1:IxCIyHEi3zRg3s0A5j5BB6A9Jmi73HwBIUl50j+osU4=\n golang.org/x/crypto v0.13.0/go.mod h1:y6Z2r+Rw4iayiXXAIxJIDAJ1zMW4yaTpebo8fPOliYc=\n golang.org/x/crypto v0.19.0/go.mod h1:Iy9bg/ha4yyC70EfRS8jz+B6ybOBKMaSxLj6P6oBDfU=\n golang.org/x/crypto v0.23.0/go.mod h1:CKFgDieR+mRhux2Lsu27y0fO304Db0wZe70UKqHu0v8=\n@@ -269,16 +210,8 @@ golang.org/x/mod v0.8.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=\n golang.org/x/mod v0.12.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=\n golang.org/x/mod v0.15.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=\n golang.org/x/mod v0.17.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=\n-golang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\n-golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\n-golang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\n golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n-golang.org/x/net v0.0.0-20200520004742-59133d7f0dd7/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\n golang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\n-golang.org/x/net v0.0.0-20211112202133-69e39bad7dc2/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\n-golang.org/x/net v0.0.0-20220225172249-27dd8689420f/go.mod h1:CfG3xpIq0wQ8r1q4Su4UZFWDARRcnwPjda9FqA0JpMk=\n golang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=\n golang.org/x/net v0.6.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=\n golang.org/x/net v0.10.0/go.mod h1:0qNGK6F8kojg2nk9dLZ2mShWaEBan6FAoqfSigmmuDg=\n@@ -288,13 +221,9 @@ golang.org/x/net v0.25.0/go.mod h1:JkAGAh7GEvH74S6FOH42FLoXpXbE/aqXSrIQjXgsiwM=\n golang.org/x/net v0.33.0/go.mod h1:HXLR5J+9DxmrqMwG9qjGCxZ+zKXxBru04zlTvWlWuN4=\n golang.org/x/net v0.47.0 h1:Mx+4dIFzqraBXUugkia1OOvlD6LemFo1ALMHjrXDOhY=\n golang.org/x/net v0.47.0/go.mod h1:/jNxtkgq5yWUGYkaZGqo27cfGZ1c5Nen03aYrrKpVRU=\n-golang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n-golang.org/x/oauth2 v0.29.0 h1:WdYw2tdTK1S8olAzWHdgeqfy+Mtm9XNhv/xJsY65d98=\n-golang.org/x/oauth2 v0.29.0/go.mod h1:onh5ek6nERTohokkhCD/y2cV4Do3fxFHFuAejCkRWT8=\n-golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n+golang.org/x/oauth2 v0.33.0 h1:4Q+qn+E5z8gPRJfmRy7C2gGG3T4jIprK6aSYgTXGRpo=\n+golang.org/x/oauth2 v0.33.0/go.mod h1:lzm5WQJQwKZ3nwavOZ3IS5Aulzxi68dUSgRHujetwEA=\n golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.0.0-20210220032951-036812b2e83c/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.1.0/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.3.0/go.mod h1:FU7BRWz2tNW+3quACPkgCx/L+uEAv1htQ0V83Z9Rj+Y=\n@@ -303,23 +232,11 @@ golang.org/x/sync v0.7.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\n golang.org/x/sync v0.10.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\n golang.org/x/sync v0.18.0 h1:kr88TuHDroi+UVf+0hZnirlk8o8T+4MrK6mr60WkH/I=\n golang.org/x/sync v0.18.0/go.mod h1:9KTHXmSnoGruLpwFjVSX0lNNA75CykiMECbovNTZqGI=\n-golang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n-golang.org/x/sys v0.0.0-20190904154756-749cb33beabd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20191005200804-aed5e4c7ecf9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20191120155948-bd437916bb0e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200116001909-b77594299b42/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200223170610-d5e6a3e2c0ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20200519105757-fe76b779f299/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n-golang.org/x/sys v0.0.0-20210423082822-04245dca01da/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n golang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20211216021012-1d35b9e2eb4e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20220227234510-4e6760a101f9/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n-golang.org/x/sys v0.0.0-20221010170243-090e33056c14/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.8.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.12.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n@@ -338,9 +255,7 @@ golang.org/x/term v0.17.0/go.mod h1:lLRBjIVuehSbZlaOtGMbcMncT+aqLLLmKrsjNrUguwk=\n golang.org/x/term v0.20.0/go.mod h1:8UkIAJTvZgivsXaD6/pH6U9ecQzZ45awqEOzuCvwpFY=\n golang.org/x/term v0.27.0/go.mod h1:iMsnZpn0cago0GOrHO2+Y7u7JPn5AylBrcoWkElMTSM=\n golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n-golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\n golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\n-golang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\n golang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\n golang.org/x/text v0.3.8/go.mod h1:E6s5w1FMmriuDzIBO73fBruAKo1PCIq6d2Q6DHfQ8WQ=\n golang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=\n@@ -352,32 +267,14 @@ golang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=\n golang.org/x/text v0.31.0 h1:aC8ghyu4JhP8VojJ2lEHBnochRno1sgL6nEi9WGFGMM=\n golang.org/x/text v0.31.0/go.mod h1:tKRAlv61yKIjGGHX/4tP1LTbc13YSec1pxVEWXzfoeM=\n golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n-golang.org/x/tools v0.0.0-20190328211700-ab21143f2384/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\n golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n golang.org/x/tools v0.1.12/go.mod h1:hNGJHUnrk76NpqgfD5Aqm5Crs+Hm0VOH/i9J2+nxYbc=\n golang.org/x/tools v0.6.0/go.mod h1:Xwgl3UAJ/d3gWutnCtw505GrjyAbvKui8lOU390QaIU=\n golang.org/x/tools v0.13.0/go.mod h1:HvlwmtVNQAhOuCjW7xxvovg8wbNq7LwfXh/k7wXUl58=\n golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d/go.mod h1:aiJjzUbINMkxbQROHiO6hDPo2LHcIPhhQsa9DLh0yGk=\n golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\n-golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\n-google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n-google.golang.org/appengine v1.6.6/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\n-google.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\n-google.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\n-google.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=\n-google.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=\n-google.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=\n-google.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\n gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n-gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\n gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\n gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=\n-gopkg.in/fsnotify.v1 v1.4.7/go.mod h1:Tz8NjZHkW78fSQdbUxIjBTcgA1z1m8ZHf0WmKUhAMys=\n-gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=\n-gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-gopkg.in/yaml.v2 v2.2.4/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-gopkg.in/yaml.v2 v2.3.0/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\n-gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n-gopkg.in/yaml.v3 v3.0.0-20200615113413-eeeca48fe776/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\n gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM="
    },
    {
      "sha": "e384683c501e68bfdd1178f618c3e9bc88347244",
      "filename": "backend/vendor/cloud.google.com/go/compute/metadata/CHANGES.md",
      "status": "modified",
      "additions": 49,
      "deletions": 0,
      "changes": 49,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2FCHANGES.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2FCHANGES.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2FCHANGES.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -1,5 +1,54 @@\n # Changes\n \n+## [0.9.0](https://github.com/googleapis/google-cloud-go/compare/compute/metadata/v0.8.4...compute/metadata/v0.9.0) (2025-09-24)\n+\n+\n+### Features\n+\n+* **compute/metadata:** Retry on HTTP 429 ([#12932](https://github.com/googleapis/google-cloud-go/issues/12932)) ([1e91f5c](https://github.com/googleapis/google-cloud-go/commit/1e91f5c07acacd38ecdd4ff3e83e092b745e0bc2))\n+\n+## [0.8.4](https://github.com/googleapis/google-cloud-go/compare/compute/metadata/v0.8.3...compute/metadata/v0.8.4) (2025-09-18)\n+\n+\n+### Bug Fixes\n+\n+* **compute/metadata:** Set subClient for UseDefaultClient case ([#12911](https://github.com/googleapis/google-cloud-go/issues/12911)) ([9e2646b](https://github.com/googleapis/google-cloud-go/commit/9e2646b1821231183fd775bb107c062865eeaccd))\n+\n+## [0.8.3](https://github.com/googleapis/google-cloud-go/compare/compute/metadata/v0.8.2...compute/metadata/v0.8.3) (2025-09-17)\n+\n+\n+### Bug Fixes\n+\n+* **compute/metadata:** Disable Client timeouts for subscription client ([#12910](https://github.com/googleapis/google-cloud-go/issues/12910)) ([187a58a](https://github.com/googleapis/google-cloud-go/commit/187a58a540494e1e8562b046325b8cad8cf7af4a))\n+\n+## [0.8.2](https://github.com/googleapis/google-cloud-go/compare/compute/metadata/v0.8.1...compute/metadata/v0.8.2) (2025-09-17)\n+\n+\n+### Bug Fixes\n+\n+* **compute/metadata:** Racy test and uninitialized subClient ([#12892](https://github.com/googleapis/google-cloud-go/issues/12892)) ([4943ca2](https://github.com/googleapis/google-cloud-go/commit/4943ca2bf83908a23806247bc4252dfb440d09cc)), refs [#12888](https://github.com/googleapis/google-cloud-go/issues/12888)\n+\n+## [0.8.1](https://github.com/googleapis/google-cloud-go/compare/compute/metadata/v0.8.0...compute/metadata/v0.8.1) (2025-09-16)\n+\n+\n+### Bug Fixes\n+\n+* **compute/metadata:** Use separate client for subscribe methods  ([#12885](https://github.com/googleapis/google-cloud-go/issues/12885)) ([76b80f8](https://github.com/googleapis/google-cloud-go/commit/76b80f8df9bf9339d175407e8c15936fe1ac1c9c))\n+\n+## [0.8.0](https://github.com/googleapis/google-cloud-go/compare/compute/metadata/v0.7.0...compute/metadata/v0.8.0) (2025-08-06)\n+\n+\n+### Features\n+\n+* **compute/metadata:** Add Options.UseDefaultClient ([#12657](https://github.com/googleapis/google-cloud-go/issues/12657)) ([1a88209](https://github.com/googleapis/google-cloud-go/commit/1a8820900f20e038291c4bb2c5284a449196e81f)), refs [#11078](https://github.com/googleapis/google-cloud-go/issues/11078)\n+\n+## [0.7.0](https://github.com/googleapis/google-cloud-go/compare/compute/metadata/v0.6.0...compute/metadata/v0.7.0) (2025-05-13)\n+\n+\n+### Features\n+\n+* **compute/metadata:** Allow canceling GCE detection ([#11786](https://github.com/googleapis/google-cloud-go/issues/11786)) ([78100fe](https://github.com/googleapis/google-cloud-go/commit/78100fe7e28cd30f1e10b47191ac3c9839663b64))\n+\n ## [0.6.0](https://github.com/googleapis/google-cloud-go/compare/compute/metadata/v0.5.2...compute/metadata/v0.6.0) (2024-12-13)\n \n "
    },
    {
      "sha": "6bd1891660509a21a87a7865add39e58a480ff09",
      "filename": "backend/vendor/cloud.google.com/go/compute/metadata/metadata.go",
      "status": "modified",
      "additions": 160,
      "deletions": 95,
      "changes": 255,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fmetadata.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fmetadata.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fmetadata.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -22,6 +22,7 @@ package metadata // import \"cloud.google.com/go/compute/metadata\"\n import (\n \t\"context\"\n \t\"encoding/json\"\n+\t\"errors\"\n \t\"fmt\"\n \t\"io\"\n \t\"log/slog\"\n@@ -62,21 +63,26 @@ var (\n )\n \n var defaultClient = &Client{\n-\thc:     newDefaultHTTPClient(),\n-\tlogger: slog.New(noOpHandler{}),\n+\thc:        newDefaultHTTPClient(true),\n+\tsubClient: newDefaultHTTPClient(false),\n+\tlogger:    slog.New(noOpHandler{}),\n }\n \n-func newDefaultHTTPClient() *http.Client {\n-\treturn &http.Client{\n-\t\tTransport: &http.Transport{\n-\t\t\tDial: (&net.Dialer{\n-\t\t\t\tTimeout:   2 * time.Second,\n-\t\t\t\tKeepAlive: 30 * time.Second,\n-\t\t\t}).Dial,\n-\t\t\tIdleConnTimeout: 60 * time.Second,\n-\t\t},\n-\t\tTimeout: 5 * time.Second,\n+func newDefaultHTTPClient(enableTimeouts bool) *http.Client {\n+\ttransport := &http.Transport{\n+\t\tDial: (&net.Dialer{\n+\t\t\tTimeout:   2 * time.Second,\n+\t\t\tKeepAlive: 30 * time.Second,\n+\t\t}).Dial,\n \t}\n+\tc := &http.Client{\n+\t\tTransport: transport,\n+\t}\n+\tif enableTimeouts {\n+\t\ttransport.IdleConnTimeout = 60 * time.Second\n+\t\tc.Timeout = 5 * time.Second\n+\t}\n+\treturn c\n }\n \n // NotDefinedError is returned when requested metadata is not defined.\n@@ -117,80 +123,18 @@ var (\n // NOTE: True returned from `OnGCE` does not guarantee that the metadata server\n // is accessible from this process and have all the metadata defined.\n func OnGCE() bool {\n-\tonGCEOnce.Do(initOnGCE)\n-\treturn onGCE\n-}\n-\n-func initOnGCE() {\n-\tonGCE = testOnGCE()\n+\treturn OnGCEWithContext(context.Background())\n }\n \n-func testOnGCE() bool {\n-\t// The user explicitly said they're on GCE, so trust them.\n-\tif os.Getenv(metadataHostEnv) != \"\" {\n-\t\treturn true\n-\t}\n-\n-\tctx, cancel := context.WithCancel(context.Background())\n-\tdefer cancel()\n-\n-\tresc := make(chan bool, 2)\n-\n-\t// Try two strategies in parallel.\n-\t// See https://github.com/googleapis/google-cloud-go/issues/194\n-\tgo func() {\n-\t\treq, _ := http.NewRequest(\"GET\", \"http://\"+metadataIP, nil)\n-\t\treq.Header.Set(\"User-Agent\", userAgent)\n-\t\tres, err := newDefaultHTTPClient().Do(req.WithContext(ctx))\n-\t\tif err != nil {\n-\t\t\tresc <- false\n-\t\t\treturn\n-\t\t}\n-\t\tdefer res.Body.Close()\n-\t\tresc <- res.Header.Get(\"Metadata-Flavor\") == \"Google\"\n-\t}()\n-\n-\tgo func() {\n-\t\tresolver := &net.Resolver{}\n-\t\taddrs, err := resolver.LookupHost(ctx, \"metadata.google.internal.\")\n-\t\tif err != nil || len(addrs) == 0 {\n-\t\t\tresc <- false\n-\t\t\treturn\n-\t\t}\n-\t\tresc <- strsContains(addrs, metadataIP)\n-\t}()\n-\n-\ttryHarder := systemInfoSuggestsGCE()\n-\tif tryHarder {\n-\t\tres := <-resc\n-\t\tif res {\n-\t\t\t// The first strategy succeeded, so let's use it.\n-\t\t\treturn true\n-\t\t}\n-\t\t// Wait for either the DNS or metadata server probe to\n-\t\t// contradict the other one and say we are running on\n-\t\t// GCE. Give it a lot of time to do so, since the system\n-\t\t// info already suggests we're running on a GCE BIOS.\n-\t\ttimer := time.NewTimer(5 * time.Second)\n-\t\tdefer timer.Stop()\n-\t\tselect {\n-\t\tcase res = <-resc:\n-\t\t\treturn res\n-\t\tcase <-timer.C:\n-\t\t\t// Too slow. Who knows what this system is.\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\n-\t// There's no hint from the system info that we're running on\n-\t// GCE, so use the first probe's result as truth, whether it's\n-\t// true or false. The goal here is to optimize for speed for\n-\t// users who are NOT running on GCE. We can't assume that\n-\t// either a DNS lookup or an HTTP request to a blackholed IP\n-\t// address is fast. Worst case this should return when the\n-\t// metaClient's Transport.ResponseHeaderTimeout or\n-\t// Transport.Dial.Timeout fires (in two seconds).\n-\treturn <-resc\n+// OnGCEWithContext reports whether this process is running on Google Compute Platforms.\n+// This function's return value is memoized for better performance.\n+// NOTE: True returned from `OnGCEWithContext` does not guarantee that the metadata server\n+// is accessible from this process and have all the metadata defined.\n+func OnGCEWithContext(ctx context.Context) bool {\n+\tonGCEOnce.Do(func() {\n+\t\tonGCE = defaultClient.OnGCEWithContext(ctx)\n+\t})\n+\treturn onGCE\n }\n \n // Subscribe calls Client.SubscribeWithContext on the default client.\n@@ -412,47 +356,161 @@ func strsContains(ss []string, s string) bool {\n \n // A Client provides metadata.\n type Client struct {\n-\thc     *http.Client\n-\tlogger *slog.Logger\n+\thc *http.Client\n+\t// subClient by default is a HTTP Client that is only used for subscribe\n+\t// methods that should not specify a timeout. If the user specifies a client\n+\t// this with be the same as 'hc'.\n+\tsubClient *http.Client\n+\tlogger    *slog.Logger\n }\n \n // Options for configuring a [Client].\n type Options struct {\n \t// Client is the HTTP client used to make requests. Optional.\n+\t// If UseDefaultClient is true, this field is ignored.\n+\t// If this field is nil, a new default http.Client will be created.\n \tClient *http.Client\n \t// Logger is used to log information about HTTP request and responses.\n \t// If not provided, nothing will be logged. Optional.\n \tLogger *slog.Logger\n+\t// UseDefaultClient specifies that the client should use the same default\n+\t// internal http.Client that is used in functions such as GetWithContext.\n+\t// This is useful for sharing a single TCP connection pool across requests.\n+\t// The difference vs GetWithContext is the ability to use this struct\n+\t// to provide a custom logger. If this field is true, the Client\n+\t// field is ignored.\n+\tUseDefaultClient bool\n }\n \n // NewClient returns a Client that can be used to fetch metadata.\n // Returns the client that uses the specified http.Client for HTTP requests.\n-// If nil is specified, returns the default client.\n+// If nil is specified, returns the default internal Client that is\n+// also used in functions such as GetWithContext. This is useful for sharing\n+// a single TCP connection pool across requests.\n func NewClient(c *http.Client) *Client {\n-\treturn NewWithOptions(&Options{\n-\t\tClient: c,\n-\t})\n+\tif c == nil {\n+\t\t// Preserve original behavior for nil argument.\n+\t\treturn defaultClient\n+\t}\n+\t// Return a new client with a no-op logger for backward compatibility.\n+\treturn &Client{hc: c, subClient: c, logger: slog.New(noOpHandler{})}\n }\n \n // NewWithOptions returns a Client that is configured with the provided Options.\n func NewWithOptions(opts *Options) *Client {\n+\t// Preserve original behavior for nil opts.\n \tif opts == nil {\n \t\treturn defaultClient\n \t}\n+\n+\t// Handle explicit request for the internal default http.Client.\n+\tif opts.UseDefaultClient {\n+\t\tlogger := opts.Logger\n+\t\tif logger == nil {\n+\t\t\tlogger = slog.New(noOpHandler{})\n+\t\t}\n+\t\treturn &Client{hc: defaultClient.hc, subClient: defaultClient.subClient, logger: logger}\n+\t}\n+\n+\t// Handle isolated client creation.\n \tclient := opts.Client\n+\tsubClient := opts.Client\n \tif client == nil {\n-\t\tclient = newDefaultHTTPClient()\n+\t\tclient = newDefaultHTTPClient(true)\n+\t\tsubClient = newDefaultHTTPClient(false)\n \t}\n \tlogger := opts.Logger\n \tif logger == nil {\n \t\tlogger = slog.New(noOpHandler{})\n \t}\n-\treturn &Client{hc: client, logger: logger}\n+\treturn &Client{hc: client, subClient: subClient, logger: logger}\n+}\n+\n+// NOTE: metadataRequestStrategy is assigned to a variable for test stubbing purposes.\n+var metadataRequestStrategy = func(ctx context.Context, httpClient *http.Client, resc chan bool) {\n+\treq, _ := http.NewRequest(\"GET\", \"http://\"+metadataIP, nil)\n+\treq.Header.Set(\"User-Agent\", userAgent)\n+\tres, err := httpClient.Do(req.WithContext(ctx))\n+\tif err != nil {\n+\t\tresc <- false\n+\t\treturn\n+\t}\n+\tdefer res.Body.Close()\n+\tresc <- res.Header.Get(\"Metadata-Flavor\") == \"Google\"\n+}\n+\n+// NOTE: dnsRequestStrategy is assigned to a variable for test stubbing purposes.\n+var dnsRequestStrategy = func(ctx context.Context, resc chan bool) {\n+\tresolver := &net.Resolver{}\n+\taddrs, err := resolver.LookupHost(ctx, \"metadata.google.internal.\")\n+\tif err != nil || len(addrs) == 0 {\n+\t\tresc <- false\n+\t\treturn\n+\t}\n+\tresc <- strsContains(addrs, metadataIP)\n+}\n+\n+// OnGCEWithContext reports whether this process is running on Google Compute Platforms.\n+// NOTE: True returned from `OnGCEWithContext` does not guarantee that the metadata server\n+// is accessible from this process and have all the metadata defined.\n+func (c *Client) OnGCEWithContext(ctx context.Context) bool {\n+\t// The user explicitly said they're on GCE, so trust them.\n+\tif os.Getenv(metadataHostEnv) != \"\" {\n+\t\treturn true\n+\t}\n+\n+\tctx, cancel := context.WithCancel(ctx)\n+\tdefer cancel()\n+\n+\tresc := make(chan bool, 2)\n+\n+\t// Try two strategies in parallel.\n+\t// See https://github.com/googleapis/google-cloud-go/issues/194\n+\tgo metadataRequestStrategy(ctx, c.hc, resc)\n+\tgo dnsRequestStrategy(ctx, resc)\n+\n+\ttryHarder := systemInfoSuggestsGCE()\n+\tif tryHarder {\n+\t\tres := <-resc\n+\t\tif res {\n+\t\t\t// The first strategy succeeded, so let's use it.\n+\t\t\treturn true\n+\t\t}\n+\n+\t\t// Wait for either the DNS or metadata server probe to\n+\t\t// contradict the other one and say we are running on\n+\t\t// GCE. Give it a lot of time to do so, since the system\n+\t\t// info already suggests we're running on a GCE BIOS.\n+\t\t// Ensure cancellations from the calling context are respected.\n+\t\twaitContext, cancelWait := context.WithTimeout(ctx, 5*time.Second)\n+\t\tdefer cancelWait()\n+\t\tselect {\n+\t\tcase res = <-resc:\n+\t\t\treturn res\n+\t\tcase <-waitContext.Done():\n+\t\t\t// Too slow. Who knows what this system is.\n+\t\t\treturn false\n+\t\t}\n+\t}\n+\n+\t// There's no hint from the system info that we're running on\n+\t// GCE, so use the first probe's result as truth, whether it's\n+\t// true or false. The goal here is to optimize for speed for\n+\t// users who are NOT running on GCE. We can't assume that\n+\t// either a DNS lookup or an HTTP request to a blackholed IP\n+\t// address is fast. Worst case this should return when the\n+\t// metaClient's Transport.ResponseHeaderTimeout or\n+\t// Transport.Dial.Timeout fires (in two seconds).\n+\treturn <-resc\n }\n \n // getETag returns a value from the metadata service as well as the associated ETag.\n // This func is otherwise equivalent to Get.\n func (c *Client) getETag(ctx context.Context, suffix string) (value, etag string, err error) {\n+\treturn c.getETagWithSubClient(ctx, suffix, false)\n+}\n+\n+func (c *Client) getETagWithSubClient(ctx context.Context, suffix string, enableSubClient bool) (value, etag string, err error) {\n \t// Using a fixed IP makes it very difficult to spoof the metadata service in\n \t// a container, which is an important use-case for local testing of cloud\n \t// deployments. To enable spoofing of the metadata service, the environment\n@@ -479,9 +537,13 @@ func (c *Client) getETag(ctx context.Context, suffix string) (value, etag string\n \tvar reqErr error\n \tvar body []byte\n \tretryer := newRetryer()\n+\thc := c.hc\n+\tif enableSubClient {\n+\t\thc = c.subClient\n+\t}\n \tfor {\n \t\tc.logger.DebugContext(ctx, \"metadata request\", \"request\", httpRequest(req, nil))\n-\t\tres, reqErr = c.hc.Do(req)\n+\t\tres, reqErr = hc.Do(req)\n \t\tvar code int\n \t\tif res != nil {\n \t\t\tcode = res.StatusCode\n@@ -827,7 +889,7 @@ func (c *Client) SubscribeWithContext(ctx context.Context, suffix string, fn fun\n \tconst failedSubscribeSleep = time.Second * 5\n \n \t// First check to see if the metadata value exists at all.\n-\tval, lastETag, err := c.getETag(ctx, suffix)\n+\tval, lastETag, err := c.getETagWithSubClient(ctx, suffix, true)\n \tif err != nil {\n \t\treturn err\n \t}\n@@ -843,8 +905,11 @@ func (c *Client) SubscribeWithContext(ctx context.Context, suffix string, fn fun\n \t\tsuffix += \"?wait_for_change=true&last_etag=\"\n \t}\n \tfor {\n-\t\tval, etag, err := c.getETag(ctx, suffix+url.QueryEscape(lastETag))\n+\t\tval, etag, err := c.getETagWithSubClient(ctx, suffix+url.QueryEscape(lastETag), true)\n \t\tif err != nil {\n+\t\t\tif errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {\n+\t\t\t\treturn err\n+\t\t\t}\n \t\t\tif _, deleted := err.(NotDefinedError); !deleted {\n \t\t\t\ttime.Sleep(failedSubscribeSleep)\n \t\t\t\tcontinue // Retry on other errors."
    },
    {
      "sha": "d516f30f80506301bdc361bb68063ffecdd65d01",
      "filename": "backend/vendor/cloud.google.com/go/compute/metadata/retry.go",
      "status": "modified",
      "additions": 3,
      "deletions": 0,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fretry.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fretry.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fretry.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -95,6 +95,9 @@ func shouldRetry(status int, err error) bool {\n \tif 500 <= status && status <= 599 {\n \t\treturn true\n \t}\n+\tif status == http.StatusTooManyRequests {\n+\t\treturn true\n+\t}\n \tif err == io.ErrUnexpectedEOF {\n \t\treturn true\n \t}"
    },
    {
      "sha": "d57ae1b27c2e6d76fa6d31af12e6f06213d55545",
      "filename": "backend/vendor/cloud.google.com/go/compute/metadata/syscheck.go",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fsyscheck.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fsyscheck.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fsyscheck.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -20,7 +20,9 @@ package metadata\n // doing network requests) suggests that we're running on GCE. If this\n // returns true, testOnGCE tries a bit harder to reach its metadata\n // server.\n-func systemInfoSuggestsGCE() bool {\n+//\n+// NOTE: systemInfoSuggestsGCE is assigned to a varible for test stubbing purposes.\n+var systemInfoSuggestsGCE = func() bool {\n \t// We don't currently have checks for other GOOS\n \treturn false\n }"
    },
    {
      "sha": "17ba5a3a23f1decfc5daa7c02869e1bf34c94cce",
      "filename": "backend/vendor/cloud.google.com/go/compute/metadata/syscheck_linux.go",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fsyscheck_linux.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fsyscheck_linux.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fsyscheck_linux.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -21,8 +21,10 @@ import (\n \t\"strings\"\n )\n \n-func systemInfoSuggestsGCE() bool {\n+// NOTE: systemInfoSuggestsGCE is assigned to a varible for test stubbing purposes.\n+var systemInfoSuggestsGCE = func() bool {\n \tb, _ := os.ReadFile(\"/sys/class/dmi/id/product_name\")\n+\n \tname := strings.TrimSpace(string(b))\n \treturn name == \"Google\" || name == \"Google Compute Engine\"\n }"
    },
    {
      "sha": "f57a5b14e9126283a6ef1fc3c2173acd92acac3c",
      "filename": "backend/vendor/cloud.google.com/go/compute/metadata/syscheck_windows.go",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fsyscheck_windows.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fsyscheck_windows.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fcloud.google.com%2Fgo%2Fcompute%2Fmetadata%2Fsyscheck_windows.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -22,7 +22,8 @@ import (\n \t\"golang.org/x/sys/windows/registry\"\n )\n \n-func systemInfoSuggestsGCE() bool {\n+// NOTE: systemInfoSuggestsGCE is assigned to a varible for test stubbing purposes.\n+var systemInfoSuggestsGCE = func() bool {\n \tk, err := registry.OpenKey(registry.LOCAL_MACHINE, `SYSTEM\\HardwareConfig\\Current`, registry.QUERY_VALUE)\n \tif err != nil {\n \t\treturn false"
    },
    {
      "sha": "91ea4d7145c3b3e2359b4d952731b9ca07902c0b",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/.gitignore",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2F.gitignore",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2F.gitignore",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2F.gitignore?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/.gitignore"
    },
    {
      "sha": "5d0a4b64fe9662265c3fa5009552accacd74321e",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/.golangci.yml",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2F.golangci.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2F.golangci.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2F.golangci.yml?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/.golangci.yml"
    },
    {
      "sha": "349ee1c29e8906a6d8ebb65a671636abdbb99564",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/LICENSE",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2FLICENSE",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2FLICENSE",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2FLICENSE?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/LICENSE"
    },
    {
      "sha": "e08fe4a6e4230546258b521949ecade00d30737d",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/README.md",
      "status": "renamed",
      "additions": 41,
      "deletions": 10,
      "changes": 51,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2FREADME.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2FREADME.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2FREADME.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -25,6 +25,9 @@ This is a generic middleware to rate-limit HTTP requests.\n \n **v7.x.x:** Replaced `time/rate` with `embedded time/rate` so that we can support more rate limit headers.\n \n+**v8.x.x:** Address `RemoteIP` vulnerability concern by replacing `SetIPLookups` with `SetIPLookup`, an explicit way to pick the IP address.\n+\n+\n ## Five Minute Tutorial\n \n ```go\n@@ -33,7 +36,8 @@ package main\n import (\n     \"net/http\"\n \n-    \"github.com/didip/tollbooth/v7\"\n+    \"github.com/didip/tollbooth/v8\"\n+    \"github.com/didip/tollbooth/v8/limiter\"\n )\n \n func HelloHandler(w http.ResponseWriter, req *http.Request) {\n@@ -42,7 +46,15 @@ func HelloHandler(w http.ResponseWriter, req *http.Request) {\n \n func main() {\n     // Create a request limiter per handler.\n-    http.Handle(\"/\", tollbooth.LimitFuncHandler(tollbooth.NewLimiter(1, nil), HelloHandler))\n+    lmt := tollbooth.NewLimiter(1, nil)\n+\n+    // New in version >= 8, you must explicitly define how to pick the IP address.\n+    lmt.SetIPLookup(limiter.IPLookup{\n+        Name:           \"X-Real-IP\",\n+        IndexFromRight: 0,\n+    })\n+\n+    http.Handle(\"/\", tollbooth.LimitFuncHandler(lmt, HelloHandler))\n     http.ListenAndServe(\":12345\", nil)\n }\n ```\n@@ -54,8 +66,8 @@ func main() {\n     import (\n         \"time\"\n     \n-        \"github.com/didip/tollbooth/v7\"\n-        \"github.com/didip/tollbooth/v7/limiter\"\n+        \"github.com/didip/tollbooth/v8\"\n+        \"github.com/didip/tollbooth/v8/limiter\"\n     )\n \n     lmt := tollbooth.NewLimiter(1, nil)\n@@ -66,10 +78,24 @@ func main() {\n     // every token bucket in it will expire 1 hour after it was initially set.\n     lmt = tollbooth.NewLimiter(1, &limiter.ExpirableOptions{DefaultExpirationTTL: time.Hour})\n \n-    // Configure list of places to look for IP address.\n-    // By default it's: \"RemoteAddr\", \"X-Forwarded-For\", \"X-Real-IP\"\n-    // If your application is behind a proxy, set \"X-Forwarded-For\" first.\n-    lmt.SetIPLookups([]string{\"RemoteAddr\", \"X-Forwarded-For\", \"X-Real-IP\"})\n+    // New in version >= 8, you must explicitly define how to pick the IP address.\n+    // If IP address cannot be found, rate limiter will not be activated.\n+    lmt.SetIPLookup(limiter.IPLookup{\n+        // The name of lookup method.\n+        // Possible options are: RemoteAddr, X-Forwarded-For, X-Real-IP, CF-Connecting-IP\n+        // All other headers are considered unknown and will be ignored.\n+        Name:            \"X-Real-IP\",\n+\n+        // The index position to pick the ip address from a comma separated list.\n+        // The index goes from right to left.\n+        //\n+        // When there are multiple of the same headers,\n+        // we will concat them together in the order of first to last seen.\n+        // And then we pick the IP using this index position.\n+        IndexFromRight: 0,\n+    })\n+\n+    // In version >= 8, lmt.SetIPLookups and lmt.GetIPLookups are removed.\n \n     // Limit only GET and POST requests.\n     lmt.SetMethods([]string{\"GET\", \"POST\"})\n@@ -89,8 +115,7 @@ func main() {\n     lmt.RemoveHeaderEntries(\"X-Access-Token\", []string{\"limitless-token\"})\n \n     // By the way, the setters are chainable. Example:\n-    lmt.SetIPLookups([]string{\"RemoteAddr\", \"X-Forwarded-For\", \"X-Real-IP\"}).\n-        SetMethods([]string{\"GET\", \"POST\"}).\n+    lmt.SetMethods([]string{\"GET\", \"POST\"}).\n         SetBasicAuthUsers([]string{\"sansa\"}).\n         SetBasicAuthUsers([]string{\"tyrion\"})\n     ```\n@@ -137,6 +162,12 @@ func main() {\n     ```go\n     lmt := tollbooth.NewLimiter(1, nil)\n \n+    // New in version >= 8, you must explicitly define how to pick the IP address.\n+    lmt.SetIPLookup(limiter.IPLookup{\n+        Name:           \"X-Forwarded-For\",\n+        IndexFromRight: 0,\n+    })\n+\n     // Set a custom message.\n     lmt.SetMessage(\"You have reached maximum request limit.\")\n ",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/README.md"
    },
    {
      "sha": "149bc5a161e26ef25a28bdcfe4c56a1a6ec1b4b3",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/errors/errors.go",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Ferrors%2Ferrors.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Ferrors%2Ferrors.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Ferrors%2Ferrors.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/errors/errors.go"
    },
    {
      "sha": "15167cd746c560e5b3d3b233a169aa64d3e9101e",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/internal/time/AUTHORS",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FAUTHORS",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FAUTHORS",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FAUTHORS?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/internal/time/AUTHORS"
    },
    {
      "sha": "1c4577e9680611383f46044d17fa343a96997c3c",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/internal/time/CONTRIBUTORS",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FCONTRIBUTORS",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FCONTRIBUTORS",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FCONTRIBUTORS?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/internal/time/CONTRIBUTORS"
    },
    {
      "sha": "6a66aea5eafe0ca6a688840c47219556c552488e",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/internal/time/LICENSE",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FLICENSE",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FLICENSE",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FLICENSE?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/internal/time/LICENSE"
    },
    {
      "sha": "733099041f84fa1e58611ab2e11af51c1f26d1d2",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/internal/time/PATENTS",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FPATENTS",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FPATENTS",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2FPATENTS?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/internal/time/PATENTS"
    },
    {
      "sha": "6c3b442d7b17c4531cf787fc90fcf8e3a3d4a595",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/internal/time/rate/rate.go",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2Frate%2Frate.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2Frate%2Frate.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Finternal%2Ftime%2Frate%2Frate.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/internal/time/rate/rate.go"
    },
    {
      "sha": "5ca875f15b60ecc79a2b094a97f837e824273077",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/libstring/libstring.go",
      "status": "renamed",
      "additions": 26,
      "deletions": 27,
      "changes": 53,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Flibstring%2Flibstring.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Flibstring%2Flibstring.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Flibstring%2Flibstring.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -5,6 +5,8 @@ import (\n \t\"net\"\n \t\"net/http\"\n \t\"strings\"\n+\n+\t\"github.com/didip/tollbooth/v8/limiter\"\n )\n \n // StringInSlice finds needle in a slice of strings.\n@@ -17,38 +19,35 @@ func StringInSlice(sliceString []string, needle string) bool {\n \treturn false\n }\n \n-// RemoteIP finds IP Address given http.Request struct.\n-func RemoteIP(ipLookups []string, forwardedForIndexFromBehind int, r *http.Request) string {\n-\trealIP := r.Header.Get(\"X-Real-IP\")\n-\tforwardedFor := r.Header.Get(\"X-Forwarded-For\")\n-\n-\tfor _, lookup := range ipLookups {\n-\t\tif lookup == \"RemoteAddr\" {\n-\t\t\t// 1. Cover the basic use cases for both ipv4 and ipv6\n-\t\t\tip, _, err := net.SplitHostPort(r.RemoteAddr)\n-\t\t\tif err != nil {\n-\t\t\t\t// 2. Upon error, just return the remote addr.\n-\t\t\t\treturn r.RemoteAddr\n-\t\t\t}\n-\t\t\treturn ip\n+// RemoteIPFromIPLookup picks an ip address explicitly from limiter.IPLookup criteria.\n+// This function is intended to replace RemoteIP function.\n+func RemoteIPFromIPLookup(ipLookup limiter.IPLookup, r *http.Request) string {\n+\tswitch ipLookup.Name {\n+\tcase \"RemoteAddr\":\n+\t\t// 1. Cover the basic use cases for both ipv4 and ipv6\n+\t\tip, _, err := net.SplitHostPort(r.RemoteAddr)\n+\t\tif err != nil {\n+\t\t\t// 2. Upon error, just return the remote addr.\n+\t\t\treturn r.RemoteAddr\n \t\t}\n-\t\tif lookup == \"X-Forwarded-For\" && forwardedFor != \"\" {\n-\t\t\t// X-Forwarded-For is potentially a list of addresses separated with \",\"\n-\t\t\tparts := strings.Split(forwardedFor, \",\")\n-\t\t\tfor i, p := range parts {\n-\t\t\t\tparts[i] = strings.TrimSpace(p)\n-\t\t\t}\n+\t\treturn ip\n \n-\t\t\tpartIndex := len(parts) - 1 - forwardedForIndexFromBehind\n-\t\t\tif partIndex < 0 {\n-\t\t\t\tpartIndex = 0\n-\t\t\t}\n+\tcase \"X-Forwarded-For\", \"X-Real-IP\", \"CF-Connecting-IP\":\n+\t\tipAddrListCommaSeparated := r.Header.Values(ipLookup.Name)\n \n-\t\t\treturn parts[partIndex]\n+\t\tipAddrCommaSeparated := strings.Join(ipAddrListCommaSeparated, \",\")\n+\n+\t\tips := strings.Split(ipAddrCommaSeparated, \",\")\n+\t\tfor i, p := range ips {\n+\t\t\tips[i] = strings.TrimSpace(p)\n \t\t}\n-\t\tif lookup == \"X-Real-IP\" && realIP != \"\" {\n-\t\t\treturn realIP\n+\n+\t\tipIndex := len(ips) - 1 - ipLookup.IndexFromRight\n+\t\tif ipIndex < 0 {\n+\t\t\tipIndex = 0\n \t\t}\n+\n+\t\treturn ips[ipIndex]\n \t}\n \n \treturn \"\"",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/libstring/libstring.go"
    },
    {
      "sha": "b5b56d6fe1809163fbc3d89e2c8cda2cdb4b42a9",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/limiter/limiter.go",
      "status": "renamed",
      "additions": 27,
      "deletions": 13,
      "changes": 40,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Flimiter%2Flimiter.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Flimiter%2Flimiter.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Flimiter%2Flimiter.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -8,7 +8,7 @@ import (\n \n \tcache \"github.com/go-pkgz/expirable-cache/v3\"\n \n-\t\"github.com/didip/tollbooth/v7/internal/time/rate\"\n+\t\"github.com/didip/tollbooth/v8/internal/time/rate\"\n )\n \n // New is a constructor for Limiter.\n@@ -19,7 +19,6 @@ func New(generalExpirableOptions *ExpirableOptions) *Limiter {\n \t\tSetMessage(\"You have reached maximum request limit.\").\n \t\tSetStatusCode(429).\n \t\tSetOnLimitReached(nil).\n-\t\tSetIPLookups([]string{\"RemoteAddr\", \"X-Forwarded-For\", \"X-Real-IP\"}).\n \t\tSetForwardedForIndexFromBehind(0).\n \t\tSetHeaders(make(map[string][]string)).\n \t\tSetContextValues(make(map[string][]string)).\n@@ -43,6 +42,18 @@ func New(generalExpirableOptions *ExpirableOptions) *Limiter {\n \treturn lmt\n }\n \n+// IPLookup is a config struct to define how users want to pick the remote IP address.\n+type IPLookup struct {\n+\t// The name of lookup method.\n+\t// Possible options are: RemoteAddr, X-Forwarded-For, X-Real-IP, CF-Connecting-IP\n+\t// All other headers are considered unknown and will be ignored.\n+\tName string\n+\n+\t// The index position to pick the ip address from a comma separated list.\n+\t// The index goes from right to left.\n+\tIndexFromRight int\n+}\n+\n // Limiter is a config struct to limit a particular request handler.\n type Limiter struct {\n \t// Maximum number of requests to limit per second.\n@@ -66,10 +77,9 @@ type Limiter struct {\n \t// An option to write back what you want upon reaching a limit.\n \toverrideDefaultResponseWriter bool\n \n-\t// List of places to look up IP address.\n-\t// Default is \"RemoteAddr\", \"X-Forwarded-For\", \"X-Real-IP\".\n-\t// You can rearrange the order as you like.\n-\tipLookups []string\n+\t// Explicitly define how to look up IP address.\n+\t// This is intended to  replace ipLookups\n+\texplicitIPLookup IPLookup\n \n \tforwardedForIndex int\n \n@@ -270,10 +280,12 @@ func (l *Limiter) ExecOnLimitReached(w http.ResponseWriter, r *http.Request) {\n }\n \n // SetOverrideDefaultResponseWriter is a thread-safe way of setting the response writer override variable.\n-func (l *Limiter) SetOverrideDefaultResponseWriter(override bool) {\n+func (l *Limiter) SetOverrideDefaultResponseWriter(override bool) *Limiter {\n \tl.Lock()\n \tl.overrideDefaultResponseWriter = override\n \tl.Unlock()\n+\n+\treturn l\n }\n \n // GetOverrideDefaultResponseWriter is a thread-safe way of getting the response writer override variable.\n@@ -283,20 +295,22 @@ func (l *Limiter) GetOverrideDefaultResponseWriter() bool {\n \treturn l.overrideDefaultResponseWriter\n }\n \n-// SetIPLookups is thread-safe way of setting list of places to look up IP address.\n-func (l *Limiter) SetIPLookups(ipLookups []string) *Limiter {\n+// SetIPLookup is thread-safe way of setting an explicit way to look up IP address.\n+// This method is intended to replace SetIPLookups (version 6 or older).\n+func (l *Limiter) SetIPLookup(lookup IPLookup) *Limiter {\n \tl.Lock()\n-\tl.ipLookups = ipLookups\n+\tl.explicitIPLookup = lookup\n \tl.Unlock()\n \n \treturn l\n }\n \n-// GetIPLookups is thread-safe way of getting list of places to look up IP address.\n-func (l *Limiter) GetIPLookups() []string {\n+// GetIPLookup is thread-safe way of getting an explicit way to look up IP address.\n+// This method is intended to replace the old GetIPLookups (version 6 or older).\n+func (l *Limiter) GetIPLookup() IPLookup {\n \tl.RLock()\n \tdefer l.RUnlock()\n-\treturn l.ipLookups\n+\treturn l.explicitIPLookup\n }\n \n // SetIgnoreURL is thread-safe way of setting whenever ignore the URL on rate limit keys",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/limiter/limiter.go"
    },
    {
      "sha": "e5f537b5317628c0e7cee08064d46498285c357c",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/limiter/limiter_options.go",
      "status": "renamed",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Flimiter%2Flimiter_options.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Flimiter%2Flimiter_options.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Flimiter%2Flimiter_options.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/limiter/limiter_options.go"
    },
    {
      "sha": "27e008c48b8c766950e71f29d9133bde860714eb",
      "filename": "backend/vendor/github.com/didip/tollbooth/v8/tollbooth.go",
      "status": "renamed",
      "additions": 33,
      "deletions": 7,
      "changes": 40,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Ftollbooth.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Ftollbooth.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth%2Fv8%2Ftollbooth.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -7,9 +7,9 @@ import (\n \t\"net/http\"\n \t\"strings\"\n \n-\t\"github.com/didip/tollbooth/v7/errors\"\n-\t\"github.com/didip/tollbooth/v7/libstring\"\n-\t\"github.com/didip/tollbooth/v7/limiter\"\n+\t\"github.com/didip/tollbooth/v8/errors\"\n+\t\"github.com/didip/tollbooth/v8/libstring\"\n+\t\"github.com/didip/tollbooth/v8/limiter\"\n )\n \n // setResponseHeaders configures X-Rate-Limit-Limit and X-Rate-Limit-Duration\n@@ -37,8 +37,7 @@ func setRateLimitResponseHeaders(lmt *limiter.Limiter, w http.ResponseWriter, to\n func NewLimiter(max float64, tbOptions *limiter.ExpirableOptions) *limiter.Limiter {\n \treturn limiter.New(tbOptions).\n \t\tSetMax(max).\n-\t\tSetBurst(int(math.Max(1, max))).\n-\t\tSetIPLookups([]string{\"X-Forwarded-For\", \"X-Real-IP\", \"RemoteAddr\"})\n+\t\tSetBurst(int(math.Max(1, max)))\n }\n \n // LimitByKeys keeps track number of request made by keys separated by pipe.\n@@ -63,7 +62,7 @@ func ShouldSkipLimiter(lmt *limiter.Limiter, r *http.Request) bool {\n \t// ---------------------------------\n \t// Filter by remote ip\n \t// If we are unable to find remoteIP, skip limiter\n-\tremoteIP := libstring.RemoteIP(lmt.GetIPLookups(), lmt.GetForwardedForIndexFromBehind(), r)\n+\tremoteIP := libstring.RemoteIPFromIPLookup(lmt.GetIPLookup(), r)\n \tremoteIP = libstring.CanonicalizeIP(remoteIP)\n \tif remoteIP == \"\" {\n \t\treturn true\n@@ -195,7 +194,7 @@ func ShouldSkipLimiter(lmt *limiter.Limiter, r *http.Request) bool {\n \n // BuildKeys generates a slice of keys to rate-limit by given limiter and request structs.\n func BuildKeys(lmt *limiter.Limiter, r *http.Request) [][]string {\n-\tremoteIP := libstring.RemoteIP(lmt.GetIPLookups(), lmt.GetForwardedForIndexFromBehind(), r)\n+\tremoteIP := libstring.RemoteIPFromIPLookup(lmt.GetIPLookup(), r)\n \tremoteIP = libstring.CanonicalizeIP(remoteIP)\n \tpath := r.URL.Path\n \tsliceKeys := make([][]string, 0)\n@@ -347,3 +346,30 @@ func LimitHandler(lmt *limiter.Limiter, next http.Handler) http.Handler {\n func LimitFuncHandler(lmt *limiter.Limiter, nextFunc func(http.ResponseWriter, *http.Request)) http.Handler {\n \treturn LimitHandler(lmt, http.HandlerFunc(nextFunc))\n }\n+\n+// HTTPMiddleware wraps http.Handler with tollbooth limiter\n+func HTTPMiddleware(lmt *limiter.Limiter) func(http.Handler) http.Handler {\n+\t// // set IP lookup only if not set\n+\tif lmt.GetIPLookup().Name == \"\" {\n+\t\tlmt.SetIPLookup(limiter.IPLookup{Name: \"RemoteAddr\"})\n+\t}\n+\n+\treturn func(next http.Handler) http.Handler {\n+\t\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n+\t\t\tselect {\n+\t\t\tcase <-r.Context().Done():\n+\t\t\t\thttp.Error(w, \"Context was canceled\", http.StatusServiceUnavailable)\n+\t\t\t\treturn\n+\t\t\tdefault:\n+\t\t\t\tif httpError := LimitByRequest(lmt, w, r); httpError != nil {\n+\t\t\t\t\tlmt.ExecOnLimitReached(w, r)\n+\t\t\t\t\tw.Header().Add(\"Content-Type\", lmt.GetMessageContentType())\n+\t\t\t\t\tw.WriteHeader(httpError.StatusCode)\n+\t\t\t\t\tw.Write([]byte(httpError.Message)) //nolint:gosec // not much we can do here with failed write\n+\t\t\t\t\treturn\n+\t\t\t\t}\n+\t\t\t\tnext.ServeHTTP(w, r)\n+\t\t\t}\n+\t\t})\n+\t}\n+}",
      "previous_filename": "backend/vendor/github.com/didip/tollbooth/v7/tollbooth.go"
    },
    {
      "sha": "d1ba02b10138d6ad6082c95d2375333929cbb7bf",
      "filename": "backend/vendor/github.com/didip/tollbooth_chi/README.md",
      "status": "removed",
      "additions": 0,
      "deletions": 33,
      "changes": 33,
      "blob_url": "https://github.com/umputun/remark42/blob/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth_chi%2FREADME.md",
      "raw_url": "https://github.com/umputun/remark42/raw/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth_chi%2FREADME.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth_chi%2FREADME.md?ref=b4511427905c02e55730d7cb27036f5796decaa9",
      "patch": "@@ -1,33 +0,0 @@\n-## tollbooth_chi\n-\n-[Chi](https://github.com/pressly/chi) middleware for rate limiting HTTP requests.\n-\n-\n-## Five Minutes Tutorial\n-\n-```\n-package main\n-\n-import (\n-    \"github.com/didip/tollbooth\"\n-    \"github.com/didip/tollbooth_chi\"\n-    \"github.com/pressly/chi\"\n-    \"net/http\"\n-    \"time\"\n-)\n-\n-func main() {\n-    // Create a limiter struct.\n-    limiter := tollbooth.NewLimiter(1, nil)\n-\n-    r := chi.NewRouter()\n-\n-    r.Use(tollbooth_chi.LimitHandler(limiter))\n-\n-    r.Get(\"/\", func(w http.ResponseWriter, r *http.Request) {\n-        w.Write([]byte(\"Hello, world!\"))\n-    })\n-\n-    http.ListenAndServe(\":12345\", r)\n-}\n-```"
    },
    {
      "sha": "7e45c9a62699fd722239675f5cb862d83becc334",
      "filename": "backend/vendor/github.com/didip/tollbooth_chi/tollbooth_chi.go",
      "status": "removed",
      "additions": 0,
      "deletions": 45,
      "changes": 45,
      "blob_url": "https://github.com/umputun/remark42/blob/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth_chi%2Ftollbooth_chi.go",
      "raw_url": "https://github.com/umputun/remark42/raw/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth_chi%2Ftollbooth_chi.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fdidip%2Ftollbooth_chi%2Ftollbooth_chi.go?ref=b4511427905c02e55730d7cb27036f5796decaa9",
      "patch": "@@ -1,45 +0,0 @@\n-package tollbooth_chi\n-\n-import (\n-\t\"net/http\"\n-\n-\t\"github.com/didip/tollbooth/v7\"\n-\t\"github.com/didip/tollbooth/v7/limiter\"\n-)\n-\n-func LimitHandler(lmt *limiter.Limiter) func(http.Handler) http.Handler {\n-\treturn func(handler http.Handler) http.Handler {\n-\t\twrapper := &limiterWrapper{\n-\t\t\tlmt: lmt,\n-\t\t}\n-\n-\t\twrapper.handler = handler\n-\t\treturn wrapper\n-\t}\n-}\n-\n-type limiterWrapper struct {\n-\tlmt     *limiter.Limiter\n-\thandler http.Handler\n-}\n-\n-func (l *limiterWrapper) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\tselect {\n-\tcase <-ctx.Done():\n-\t\thttp.Error(w, \"Context was canceled\", http.StatusServiceUnavailable)\n-\t\treturn\n-\n-\tdefault:\n-\t\thttpError := tollbooth.LimitByRequest(l.lmt, w, r)\n-\t\tif httpError != nil {\n-\t\t\tl.lmt.ExecOnLimitReached(w, r)\n-\t\t\tw.Header().Add(\"Content-Type\", l.lmt.GetMessageContentType())\n-\t\t\tw.WriteHeader(httpError.StatusCode)\n-\t\t\tw.Write([]byte(httpError.Message))\n-\t\t\treturn\n-\t\t}\n-\n-\t\tl.handler.ServeHTTP(w, r)\n-\t}\n-}"
    },
    {
      "sha": "1053c5b9fcf8ba5e1bf7b7332f7134bbb2f72c95",
      "filename": "backend/vendor/github.com/go-pkgz/auth/v2/provider/service.go",
      "status": "modified",
      "additions": 9,
      "deletions": 8,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fauth%2Fv2%2Fprovider%2Fservice.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fauth%2Fv2%2Fprovider%2Fservice.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fauth%2Fv2%2Fprovider%2Fservice.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -7,6 +7,7 @@ import (\n \t\"net/http\"\n \t\"strings\"\n \n+\t\"github.com/go-pkgz/auth/v2/avatar\"\n \t\"github.com/go-pkgz/auth/v2/token\"\n )\n \n@@ -71,15 +72,15 @@ func (p Service) Handler(w http.ResponseWriter, r *http.Request) {\n \n // setAvatar saves avatar and puts proxied URL to u.Picture\n func setAvatar(ava AvatarSaver, u token.User, client *http.Client) (token.User, error) {\n-\tif ava != nil {\n-\t\tavatarURL, e := ava.Put(u, client)\n-\t\tif e != nil {\n-\t\t\treturn u, fmt.Errorf(\"failed to save avatar for: %w\", e)\n-\t\t}\n-\t\tu.Picture = avatarURL\n-\t\treturn u, nil\n+\tif ava == nil || ava == (*avatar.Proxy)(nil) {\n+\t\treturn u, nil // empty AvatarSaver ok, just skipped\n \t}\n-\treturn u, nil // empty AvatarSaver ok, just skipped\n+\tavatarURL, e := ava.Put(u, client)\n+\tif e != nil {\n+\t\treturn u, fmt.Errorf(\"failed to save avatar for: %w\", e)\n+\t}\n+\tu.Picture = avatarURL\n+\treturn u, nil\n }\n \n func randToken() (string, error) {"
    },
    {
      "sha": "a283374e9ee1835f067e07a27c3b6168988247ff",
      "filename": "backend/vendor/github.com/go-pkgz/email/.golangci.yml",
      "status": "modified",
      "additions": 35,
      "deletions": 39,
      "changes": 74,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Femail%2F.golangci.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Femail%2F.golangci.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Femail%2F.golangci.yml?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -1,57 +1,53 @@\n+version: \"2\"\n run:\n   timeout: 5m\n-  output:\n-    format: tab\n-  skip-dirs:\n-    - vendor\n-\n-linters-settings:\n-  govet:\n-    check-shadowing: true\n-  maligned:\n-    suggest-new: true\n-  goconst:\n-    min-len: 2\n-    min-occurrences: 2\n-  misspell:\n-    locale: US\n-  lll:\n-    line-length: 140\n-  gocritic:\n-    enabled-tags:\n-      - performance\n-      - style\n-      - experimental\n-    disabled-checks:\n-      - wrapperFunc\n-      - hugeParam\n-      - rangeValCopy\n-      - singleCaseSwitch\n-      - ifElseChain\n-\n+  concurrency: 4\n linters:\n+  default: none\n   enable:\n     - dupl\n-    - exportloopref\n-    - gas\n     - gochecknoinits\n     - gocritic\n     - gocyclo\n-    - gosimple\n+    - gosec\n     - govet\n     - ineffassign\n-    - megacheck\n     - misspell\n     - nakedret\n     - prealloc\n     - revive\n-    - stylecheck\n-    - typecheck\n+    - staticcheck\n     - unconvert\n     - unparam\n     - unused\n-  fast: false\n-  disable-all: true\n+  settings:\n+    goconst:\n+      min-len: 2\n+      min-occurrences: 2\n+    gocritic:\n+      disabled-checks:\n+        - wrapperFunc\n+        - hugeParam\n+        - rangeValCopy\n+        - singleCaseSwitch\n+        - ifElseChain\n+      enabled-tags:\n+        - performance\n+        - style\n+        - experimental\n+    govet:\n+      enable-all: true\n+      disable:\n+        - fieldalignment\n+    lll:\n+      line-length: 140\n+    misspell:\n+      locale: US\n \n-issues:\n-  exclude-use-default: false\n+  exclusions:\n+    generated: lax\n+    paths:\n+      - vendor\n+      - third_party\n+      - builtin\n+      - examples"
    },
    {
      "sha": "f62eda5035d87c1e783ced5cd52db2da88674d3e",
      "filename": "backend/vendor/github.com/go-pkgz/email/auth.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Femail%2Fauth.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Femail%2Fauth.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Femail%2Fauth.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -51,7 +51,7 @@ func (a *loginAuth) Start(server *smtp.ServerInfo) (proto string, toServer []byt\n \treturn \"LOGIN\", []byte(a.user), nil\n }\n \n-func (a *loginAuth) Next(fromServer []byte, more bool) (toServer []byte, err error) {\n+func (a *loginAuth) Next(_ []byte, more bool) (toServer []byte, err error) {\n \tif more {\n \t\treturn []byte(a.password), nil\n \t}"
    },
    {
      "sha": "f7c7c44e6275119ef495886291995e284faffe62",
      "filename": "backend/vendor/github.com/go-pkgz/email/email.go",
      "status": "modified",
      "additions": 25,
      "deletions": 12,
      "changes": 37,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Femail%2Femail.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Femail%2Femail.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Femail%2Femail.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -13,10 +13,12 @@ import (\n \t\"mime/quotedprintable\"\n \t\"net\"\n \t\"net/http\"\n+\t\"net/mail\"\n \t\"net/smtp\"\n \t\"net/textproto\"\n \t\"os\"\n \t\"path/filepath\"\n+\t\"strconv\"\n \t\"strings\"\n \t\"time\"\n )\n@@ -30,10 +32,10 @@ type Sender struct {\n \tlogger             Logger\n \thost               string     // SMTP host\n \tport               int        // SMTP port\n-\tcontentType        string     // Content type, optional. Will trigger MIME and Content-Type headers\n+\tcontentType        string     // content type, optional. Will trigger MIME and Content-Type headers\n \ttls                bool       // TLS auth\n-\tstarttls           bool       // StartTLS\n-\tinsecureSkipVerify bool       // Insecure Skip Verify\n+\tstarttls           bool       // startTLS\n+\tinsecureSkipVerify bool       // insecure Skip Verify\n \tsmtpUserName       string     // username\n \tsmtpPassword       string     // password\n \tauthMethod         authMethod // auth method\n@@ -44,12 +46,12 @@ type Sender struct {\n \n // Params contains all user-defined parameters to send emails\n type Params struct {\n-\tFrom            string   // From email field\n-\tTo              []string // From email field\n-\tSubject         string   // Email subject\n+\tFrom            string   // from email field\n+\tTo              []string // from email field\n+\tSubject         string   // email subject\n \tUnsubscribeLink string   // POST, https://support.google.com/mail/answer/81126 -> \"Use one-click unsubscribe\"\n-\tInReplyTo       string   // Identifier for email group (category), used for email grouping\n-\tAttachments     []string // Attachments path\n+\tInReplyTo       string   // identifier for email group (category), used for email grouping\n+\tAttachments     []string // attachments path\n \tInlineImages    []string // InlineImages images path\n }\n \n@@ -130,12 +132,12 @@ func (em *Sender) Send(text string, params Params) error {\n \t\t}\n \t}\n \n-\tif err := client.Mail(params.From); err != nil {\n+\tif err := client.Mail(extractEmailAddress(params.From)); err != nil {\n \t\treturn fmt.Errorf(\"bad from address %q: %w\", params.From, err)\n \t}\n \n \tfor _, rcpt := range params.To {\n-\t\tif err := client.Rcpt(rcpt); err != nil {\n+\t\tif err := client.Rcpt(extractEmailAddress(rcpt)); err != nil {\n \t\t\treturn fmt.Errorf(\"bad to address %q: %w\", params.To, err)\n \t\t}\n \t}\n@@ -165,13 +167,24 @@ func (em *Sender) Send(text string, params Params) error {\n \treturn nil\n }\n \n+// extractEmailAddress extracts the email address from a string that may contain a display name.\n+// For example, it converts `\"John Doe\" <john@example.com>` to `john@example.com`.\n+// If parsing fails, it returns the original string unchanged.\n+func extractEmailAddress(from string) string {\n+\taddr, err := mail.ParseAddress(strings.TrimSpace(from))\n+\tif err != nil {\n+\t\treturn from\n+\t}\n+\treturn addr.Address\n+}\n+\n func (em *Sender) String() string {\n \treturn fmt.Sprintf(\"smtp://%s:%d, auth:%v, tls:%v, starttls:%v, insecureSkipVerify:%v, timeout:%v, content-type:%q, charset:%q\",\n \t\tem.host, em.port, em.smtpUserName != \"\", em.tls, em.starttls, em.insecureSkipVerify, em.timeOut, em.contentType, em.contentCharset)\n }\n \n func (em *Sender) client() (c *smtp.Client, err error) {\n-\tsrvAddress := fmt.Sprintf(\"%s:%d\", em.host, em.port)\n+\tsrvAddress := net.JoinHostPort(em.host, strconv.Itoa(em.port))\n \t// #nosec G402\n \ttlsConf := &tls.Config{\n \t\tInsecureSkipVerify: em.insecureSkipVerify, // #nosec G402\n@@ -366,4 +379,4 @@ func (em *Sender) writeFiles(mp *multipart.Writer, files []string, disposition s\n \n type nopLogger struct{}\n \n-func (nopLogger) Logf(format string, args ...interface{}) {}\n+func (nopLogger) Logf(_ string, _ ...interface{}) {}"
    },
    {
      "sha": "4b6d9b4efa5b07ba759386b06fab43aad24f0b2f",
      "filename": "backend/vendor/github.com/go-pkgz/expirable-cache/v3/cache.go",
      "status": "modified",
      "additions": 17,
      "deletions": 21,
      "changes": 38,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fexpirable-cache%2Fv3%2Fcache.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fexpirable-cache%2Fv3%2Fcache.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fexpirable-cache%2Fv3%2Fcache.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -94,13 +94,12 @@ func (c *cacheImpl[K, V]) Set(key K, value V, ttl time.Duration) {\n // Returns false if there was no eviction: the item was already in the cache,\n // or the size was not exceeded.\n func (c *cacheImpl[K, V]) addWithTTL(key K, value V, ttl time.Duration) (evicted bool) {\n-\tc.Lock()\n-\tdefer c.Unlock()\n-\tnow := time.Now()\n \tif ttl == 0 {\n \t\tttl = c.ttl\n \t}\n-\n+\tnow := time.Now()\n+\tc.Lock()\n+\tdefer c.Unlock()\n \t// Check for existing item\n \tif ent, ok := c.items[key]; ok {\n \t\tc.evictList.MoveToFront(ent)\n@@ -117,7 +116,10 @@ func (c *cacheImpl[K, V]) addWithTTL(key K, value V, ttl time.Duration) (evicted\n \n \t// Remove the oldest entry if it is expired, only in case of non-default TTL.\n \tif c.ttl != noEvictionTTL || ttl != noEvictionTTL {\n-\t\tc.removeOldestIfExpired()\n+\t\tent := c.evictList.Back()\n+\t\tif ent != nil && now.After(ent.Value.(*cacheItem[K, V]).expiresAt) {\n+\t\t\tc.removeElement(ent)\n+\t\t}\n \t}\n \n \tevict := c.maxKeys > 0 && len(c.items) > c.maxKeys\n@@ -197,15 +199,14 @@ func (c *cacheImpl[K, V]) Keys() []K {\n // Values returns a slice of the values in the cache, from oldest to newest.\n // Expired entries are filtered out.\n func (c *cacheImpl[K, V]) Values() []V {\n-\tc.Lock()\n-\tdefer c.Unlock()\n \tvalues := make([]V, 0, len(c.items))\n \tnow := time.Now()\n+\tc.Lock()\n+\tdefer c.Unlock()\n \tfor ent := c.evictList.Back(); ent != nil; ent = ent.Prev() {\n-\t\tif now.After(ent.Value.(*cacheItem[K, V]).expiresAt) {\n-\t\t\tcontinue\n+\t\tif !now.After(ent.Value.(*cacheItem[K, V]).expiresAt) {\n+\t\t\tvalues = append(values, ent.Value.(*cacheItem[K, V]).value)\n \t\t}\n-\t\tvalues = append(values, ent.Value.(*cacheItem[K, V]).value)\n \t}\n \treturn values\n }\n@@ -291,11 +292,14 @@ func (c *cacheImpl[K, V]) GetOldest() (key K, value V, ok bool) {\n \n // DeleteExpired clears cache of expired items\n func (c *cacheImpl[K, V]) DeleteExpired() {\n+\tnow := time.Now()\n \tc.Lock()\n \tdefer c.Unlock()\n-\tfor _, key := range c.keys() {\n-\t\tif time.Now().After(c.items[key].Value.(*cacheItem[K, V]).expiresAt) {\n-\t\t\tc.removeElement(c.items[key])\n+\tvar nextEnt *list.Element\n+\tfor ent := c.evictList.Back(); ent != nil; ent = nextEnt {\n+\t\tnextEnt = ent.Prev()\n+\t\tif now.After(ent.Value.(*cacheItem[K, V]).expiresAt) {\n+\t\t\tc.removeElement(ent)\n \t\t}\n \t}\n }\n@@ -344,14 +348,6 @@ func (c *cacheImpl[K, V]) removeOldest() {\n \t}\n }\n \n-// removeOldest removes the oldest item from the cache in case it's already expired. Has to be called with lock!\n-func (c *cacheImpl[K, V]) removeOldestIfExpired() {\n-\tent := c.evictList.Back()\n-\tif ent != nil && time.Now().After(ent.Value.(*cacheItem[K, V]).expiresAt) {\n-\t\tc.removeElement(ent)\n-\t}\n-}\n-\n // removeElement is used to remove a given list element from the cache. Has to be called with lock!\n func (c *cacheImpl[K, V]) removeElement(e *list.Element) {\n \tc.evictList.Remove(e)"
    },
    {
      "sha": "07dfb712b2c4af0804ec3df97b01a9345c5778b8",
      "filename": "backend/vendor/github.com/go-pkgz/notify/.golangci.yml",
      "status": "modified",
      "additions": 61,
      "deletions": 55,
      "changes": 116,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2F.golangci.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2F.golangci.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2F.golangci.yml?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -1,62 +1,68 @@\n-run:\n-  timeout: 5m\n-\n-linters-settings:\n-  govet:\n-    check-shadowing: true\n-  goconst:\n-    min-len: 2\n-    min-occurrences: 2\n-  misspell:\n-    locale: US\n-  lll:\n-    line-length: 140\n-  gocritic:\n-    enabled-tags:\n-      - performance\n-      - style\n-      - experimental\n-    disabled-checks:\n-      - wrapperFunc\n-      - hugeParam\n-      - rangeValCopy\n-      - singleCaseSwitch\n-      - ifElseChain\n-\n+version: \"2\"\n linters:\n+  default: none\n   enable:\n-    - revive\n-    - govet\n-    - unconvert\n-    - staticcheck\n-    - gosec\n-    - unused\n-    - gocyclo\n+    - bodyclose\n+    - copyloopvar\n     - dupl\n-    - misspell\n-    - unparam\n-    - typecheck\n-    - ineffassign\n-    - stylecheck\n     - gochecknoinits\n-    - exportloopref\n+    - gocognit\n     - gocritic\n+    - gosec\n+    - govet\n+    - ineffassign\n+    - misspell\n     - nakedret\n-    - gosimple\n+    - nolintlint\n     - prealloc\n-    - whitespace\n-  fast: false\n-  disable-all: true\n-\n-issues:\n-  exclude-rules:\n-    - text: \"at least one file in a package should have a package comment\"\n-      linters:\n-        - stylecheck\n-    - path: _test\\.go\n-      linters:\n-        - gosec\n-        - dupl\n-  exclude-use-default: false\n-  exclude-dirs:\n-    - vendor\n+    - revive\n+    - staticcheck\n+    - testifylint\n+    - unconvert\n+    - unparam\n+    - unused\n+  settings:\n+    goconst:\n+      min-len: 2\n+      min-occurrences: 2\n+    revive:\n+      enable-all-rules: true\n+      rules:\n+        - name: unused-receiver\n+          disabled: true\n+        - name: line-length-limit\n+          disabled: true\n+        - name: add-constant\n+          disabled: true\n+        - name: cognitive-complexity\n+          disabled: true\n+        - name: function-length\n+          disabled: true\n+        - name: cyclomatic\n+          disabled: true\n+        - name: nested-structs\n+          disabled: true\n+    gocritic:\n+      disabled-checks:\n+        - hugeParam\n+      enabled-tags:\n+        - performance\n+        - style\n+        - experimental\n+    govet:\n+      enable:\n+        - shadow\n+    lll:\n+      line-length: 140\n+    misspell:\n+      locale: US\n+formatters:\n+  enable:\n+    - gofmt\n+    - goimports\n+  exclusions:\n+    generated: lax\n+    paths:\n+      - third_party$\n+      - builtin$\n+      - examples$\n\\ No newline at end of file"
    },
    {
      "sha": "22ecf702bcacb039fc9b0778deeeb35a160c78ed",
      "filename": "backend/vendor/github.com/go-pkgz/notify/email.go",
      "status": "modified",
      "additions": 11,
      "deletions": 11,
      "changes": 22,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2Femail.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2Femail.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2Femail.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -13,17 +13,17 @@ import (\n \n // SMTPParams contain settings for smtp server connection\n type SMTPParams struct {\n-\tHost        string        // SMTP host\n-\tPort        int           // SMTP port\n-\tTLS         bool          // TLS auth\n-\tStartTLS    bool          // StartTLS auth\n-\tInsecureSkipVerify bool\t  // skip certificate verification\n-\tContentType string        // Content type\n-\tCharset     string        // Character set\n-\tLoginAuth   bool          // LOGIN auth method instead of default PLAIN, needed for Office 365 and outlook.com\n-\tUsername    string        // username\n-\tPassword    string        // password\n-\tTimeOut     time.Duration // TCP connection timeout\n+\tHost               string        // SMTP host\n+\tPort               int           // SMTP port\n+\tTLS                bool          // TLS auth\n+\tStartTLS           bool          // StartTLS auth\n+\tInsecureSkipVerify bool          // skip certificate verification\n+\tContentType        string        // Content type\n+\tCharset            string        // Character set\n+\tLoginAuth          bool          // LOGIN auth method instead of default PLAIN, needed for Office 365 and outlook.com\n+\tUsername           string        // username\n+\tPassword           string        // password\n+\tTimeOut            time.Duration // TCP connection timeout\n }\n \n // Email notifications client"
    },
    {
      "sha": "91cac6240352625b6ea63af48053ae26884d8d48",
      "filename": "backend/vendor/github.com/go-pkgz/notify/slack.go",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2Fslack.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2Fslack.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2Fslack.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -93,9 +93,9 @@ func (s *Slack) findChannelIDByName(name string) (string, error) {\n \t\t\treturn \"\", err\n \t\t}\n \n-\t\tfor _, channel := range channels {\n-\t\t\tif channel.Name == name {\n-\t\t\t\treturn channel.ID, nil\n+\t\tfor i := range channels {\n+\t\t\tif channels[i].Name == name {\n+\t\t\t\treturn channels[i].ID, nil\n \t\t\t}\n \t\t}\n "
    },
    {
      "sha": "f6ddb7d9bfb530d73f8d3d4eb5a327557e206d81",
      "filename": "backend/vendor/github.com/go-pkgz/notify/telegram.go",
      "status": "modified",
      "additions": 7,
      "deletions": 7,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2Ftelegram.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2Ftelegram.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Fnotify%2Ftelegram.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -170,23 +170,23 @@ func adjustHTMLTags(htmlText string) string {\n \t\t\tswitch token.Data {\n \t\t\tcase \"h1\", \"h2\", \"h3\":\n \t\t\t\tif token.Type == html.StartTagToken {\n-\t\t\t\t\tbuff.WriteString(\"<b>\")\n+\t\t\t\t\t_, _ = buff.WriteString(\"<b>\")\n \t\t\t\t}\n \t\t\t\tif token.Type == html.EndTagToken {\n-\t\t\t\t\tbuff.WriteString(\"</b>\")\n+\t\t\t\t\t_, _ = buff.WriteString(\"</b>\")\n \t\t\t\t}\n \t\t\tcase \"h4\", \"h5\", \"h6\":\n \t\t\t\tif token.Type == html.StartTagToken {\n-\t\t\t\t\tbuff.WriteString(\"<i><b>\")\n+\t\t\t\t\t_, _ = buff.WriteString(\"<i><b>\")\n \t\t\t\t}\n \t\t\t\tif token.Type == html.EndTagToken {\n-\t\t\t\t\tbuff.WriteString(\"</b></i>\")\n+\t\t\t\t\t_, _ = buff.WriteString(\"</b></i>\")\n \t\t\t\t}\n \t\t\tdefault:\n-\t\t\t\tbuff.WriteString(token.String())\n+\t\t\t\t_, _ = buff.WriteString(token.String())\n \t\t\t}\n \t\tdefault:\n-\t\t\tbuff.WriteString(token.String())\n+\t\t\t_, _ = buff.WriteString(token.String())\n \t\t}\n \t}\n }\n@@ -435,7 +435,7 @@ func (t *Telegram) botInfo(ctx context.Context) (*TelegramBotInfo, error) {\n }\n \n // Request makes a request to the Telegram API and return the result\n-func (t *Telegram) Request(ctx context.Context, method string, b []byte, data interface{}) error {\n+func (t *Telegram) Request(ctx context.Context, method string, b []byte, data any) error {\n \treturn repeater.NewDefault(3, time.Millisecond*250).Do(ctx, func() error {\n \t\turl := fmt.Sprintf(\"%s%s/%s\", t.apiPrefix, t.Token, method)\n "
    },
    {
      "sha": "f1c181ec9c5c921245027c6b452ecfc1d3626364",
      "filename": "backend/vendor/github.com/go-pkgz/repeater/v2/.gitignore",
      "status": "added",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2F.gitignore",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2F.gitignore",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2F.gitignore?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,12 @@\n+# Binaries for programs and plugins\n+*.exe\n+*.exe~\n+*.dll\n+*.so\n+*.dylib\n+\n+# Test binary, build with `go test -c`\n+*.test\n+\n+# Output of the go coverage tool, specifically when used with LiteIDE\n+*.out"
    },
    {
      "sha": "bdac9f80fbe040c00ef329070bc6b95e6f0cf4a0",
      "filename": "backend/vendor/github.com/go-pkgz/repeater/v2/.golangci.yml",
      "status": "added",
      "additions": 84,
      "deletions": 0,
      "changes": 84,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2F.golangci.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2F.golangci.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2F.golangci.yml?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,84 @@\n+version: \"2\"\n+run:\n+  concurrency: 4\n+linters:\n+  default: none\n+  enable:\n+    - contextcheck\n+    - copyloopvar\n+    - decorder\n+    - errorlint\n+    - exptostd\n+    - gochecknoglobals\n+    - gochecknoinits\n+    - gocritic\n+    - gosec\n+    - govet\n+    - ineffassign\n+    - nakedret\n+    - nilerr\n+    - prealloc\n+    - predeclared\n+    - revive\n+    - staticcheck\n+    - testifylint\n+    - thelper\n+    - unconvert\n+    - unparam\n+    - unused\n+    - nestif\n+    - wrapcheck\n+  settings:\n+    goconst:\n+      min-len: 2\n+      min-occurrences: 2\n+    gocritic:\n+      disabled-checks:\n+        - wrapperFunc\n+      enabled-tags:\n+        - performance\n+        - style\n+        - experimental\n+    gocyclo:\n+      min-complexity: 15\n+    govet:\n+      enable-all: true\n+      disable:\n+        - fieldalignment\n+    lll:\n+      line-length: 140\n+    misspell:\n+      locale: US\n+  exclusions:\n+    generated: lax\n+    rules:\n+      - linters:\n+          - gosec\n+        text: 'G114: Use of net/http serve function that has no support for setting timeouts'\n+      - linters:\n+          - revive\n+          - unparam\n+        path: _test\\.go$\n+        text: unused-parameter\n+      - linters:\n+          - prealloc\n+        path: _test\\.go$\n+        text: Consider pre-allocating\n+      - linters:\n+          - gosec\n+          - intrange\n+        path: _test\\.go$\n+    paths:\n+      - third_party$\n+      - builtin$\n+      - examples$\n+formatters:\n+  enable:\n+    - gofmt\n+    - goimports\n+  exclusions:\n+    generated: lax\n+    paths:\n+      - third_party$\n+      - builtin$\n+      - examples$"
    },
    {
      "sha": "5f768edaa149dd1ae710e74f7e80b3ac942364aa",
      "filename": "backend/vendor/github.com/go-pkgz/repeater/v2/README.md",
      "status": "added",
      "additions": 259,
      "deletions": 0,
      "changes": 259,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2FREADME.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2FREADME.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2FREADME.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,259 @@\n+# Repeater\n+\n+[![Build Status](https://github.com/go-pkgz/repeater/workflows/build/badge.svg)](https://github.com/go-pkgz/repeater/actions) [![Go Report Card](https://goreportcard.com/badge/github.com/go-pkgz/repeater)](https://goreportcard.com/report/github.com/go-pkgz/repeater) [![Coverage Status](https://coveralls.io/repos/github/go-pkgz/repeater/badge.svg?branch=master)](https://coveralls.io/github/go-pkgz/repeater?branch=master)\n+\n+Package repeater implements a functional mechanism to repeat operations with different retry strategies.\n+\n+## Install and update\n+\n+`go get -u github.com/go-pkgz/repeater`\n+\n+## Usage\n+\n+### Basic Example with Exponential Backoff\n+\n+```go\n+// create repeater with exponential backoff\n+r := repeater.NewBackoff(5, time.Second) // 5 attempts starting with 1s delay\n+\n+err := r.Do(ctx, func() error {\n+// do something that may fail\n+return nil\n+})\n+```\n+\n+### Fixed Delay with Critical Error\n+\n+```go\n+// create repeater with fixed delay\n+r := repeater.NewFixed(3, 100*time.Millisecond)\n+\n+criticalErr := errors.New(\"critical error\")\n+\n+err := r.Do(ctx, func() error {\n+// do something that may fail\n+return fmt.Errorf(\"temp error\")\n+}, criticalErr) // will stop immediately if criticalErr returned\n+```\n+\n+### Custom Backoff Strategy\n+\n+```go\n+r := repeater.NewBackoff(5, time.Second,\n+repeater.WithMaxDelay(10*time.Second),\n+repeater.WithBackoffType(repeater.BackoffLinear),\n+repeater.WithJitter(0.1),\n+)\n+\n+ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n+defer cancel()\n+\n+err := r.Do(ctx, func() error {\n+// do something that may fail\n+return nil\n+})\n+```\n+\n+### Stop on Any Error\n+\n+```go\n+r := repeater.NewFixed(3, time.Millisecond)\n+\n+err := r.Do(ctx, func() error {\n+return errors.New(\"some error\")\n+}, repeater.ErrAny)  // will stop on any error\n+```\n+\n+## Strategies\n+\n+The package provides several retry strategies:\n+\n+1. **Fixed Delay** - each retry happens after a fixed time interval\n+2. **Backoff** - delay between retries increases according to the chosen algorithm:\n+   - Constant - same delay between attempts\n+   - Linear - delay increases linearly\n+   - Exponential - delay doubles with each attempt\n+\n+Backoff strategy can be customized with:\n+- Maximum delay cap\n+- Jitter to prevent thundering herd\n+- Different backoff types (constant/linear/exponential)\n+\n+### Custom Strategies\n+\n+You can implement your own retry strategy by implementing the Strategy interface:\n+\n+```go\n+type Strategy interface {\n+    // NextDelay returns delay for the next attempt\n+    // attempt starts from 1\n+    NextDelay(attempt int) time.Duration\n+}\n+```\n+\n+Example of a custom strategy that increases delay by a custom factor:\n+\n+```go\n+// CustomStrategy implements Strategy with custom factor-based delays\n+type CustomStrategy struct {\n+    Initial time.Duration\n+    Factor  float64\n+}\n+\n+func (s CustomStrategy) NextDelay(attempt int) time.Duration {\n+    if attempt <= 0 {\n+        return 0\n+    }\n+    delay := time.Duration(float64(s.Initial) * math.Pow(s.Factor, float64(attempt-1)))\n+    return delay\n+}\n+\n+// Usage\n+strategy := &CustomStrategy{Initial: time.Second, Factor: 1.5}\n+r := repeater.NewWithStrategy(5, strategy)\n+err := r.Do(ctx, func() error {\n+    // attempts will be delayed by: 1s, 1.5s, 2.25s, 3.37s, 5.06s\n+    return nil\n+})\n+```\n+\n+## Options\n+\n+For backoff strategy, several options are available:\n+\n+```go\n+WithMaxDelay(time.Duration)   // set maximum delay between retries\n+WithBackoffType(BackoffType)  // set backoff type (constant/linear/exponential)\n+WithJitter(float64)           // add randomness to delays (0-1.0)\n+```\n+\n+## Error Handling\n+\n+- Stops on context cancellation\n+- Can stop on specific errors (pass them as additional parameters to Do)\n+- Special `ErrAny` to stop on any error\n+- Returns last error if all attempts fail\n+- Custom error classification via `SetErrorClassifier`\n+\n+### Error Classification\n+\n+You can provide a custom error classifier function to dynamically determine if an error should trigger a retry or stop immediately. This is particularly useful for API clients where different error types require different handling:\n+\n+```go\n+// Define what errors are retryable\n+isRetryable := func(err error) bool {\n+    if err == nil {\n+        return false\n+    }\n+    \n+    errStr := strings.ToLower(err.Error())\n+    \n+    // Retryable patterns\n+    if strings.Contains(errStr, \"429\") ||\n+       strings.Contains(errStr, \"rate limit\") ||\n+       strings.Contains(errStr, \"timeout\") ||\n+       strings.Contains(errStr, \"503\") {\n+        return true\n+    }\n+    \n+    // Non-retryable patterns\n+    if strings.Contains(errStr, \"401\") ||\n+       strings.Contains(errStr, \"authentication\") ||\n+       strings.Contains(errStr, \"token limit\") {\n+        return false\n+    }\n+    \n+    return true // default to retry\n+}\n+\n+// Use with any repeater strategy\n+r := repeater.NewBackoff(5, time.Second)\n+r.SetErrorClassifier(isRetryable)\n+\n+err := r.Do(ctx, func() error {\n+    // API call that might fail\n+    return apiClient.Call()\n+})\n+```\n+\n+When an error classifier is set:\n+- After each error, the classifier function is called\n+- If it returns `false`, the operation stops immediately\n+- If it returns `true`, the retry logic continues\n+- The classifier takes precedence over the critical errors list\n+\n+This feature works with all repeater strategies (NewFixed, NewBackoff, NewWithStrategy).\n+\n+## Execution Statistics\n+\n+The repeater tracks execution statistics that can be accessed after calling `Do()`:\n+\n+```go\n+r := repeater.NewFixed(5, 100*time.Millisecond)\n+\n+err := r.Do(ctx, func() error {\n+    // operation that might fail\n+    return someOperation()\n+})\n+\n+// Get execution statistics\n+stats := r.Stats()\n+\n+fmt.Printf(\"Attempts: %d\\n\", stats.Attempts)\n+fmt.Printf(\"Success: %v\\n\", stats.Success)\n+fmt.Printf(\"Total Duration: %v\\n\", stats.TotalDuration)\n+fmt.Printf(\"Work Duration: %v\\n\", stats.WorkDuration)\n+fmt.Printf(\"Delay Duration: %v\\n\", stats.DelayDuration)\n+if stats.LastError != nil {\n+    fmt.Printf(\"Last Error: %v\\n\", stats.LastError)\n+}\n+```\n+\n+### Available Statistics\n+\n+The `Stats` struct provides the following information:\n+\n+- `Attempts` - Number of attempts made (including successful ones)\n+- `Success` - Whether the operation eventually succeeded\n+- `TotalDuration` - Total elapsed time from start to finish\n+- `WorkDuration` - Time spent executing the function (excluding delays)\n+- `DelayDuration` - Time spent in delays between attempts\n+- `LastError` - Last error encountered (nil if succeeded)\n+- `StartedAt` - When the repeater started\n+- `FinishedAt` - When the repeater finished\n+\n+### Usage Example\n+\n+```go\n+r := repeater.NewBackoff(3, time.Second)\n+\n+start := time.Now()\n+err := r.Do(ctx, func() error {\n+    // Simulate work that takes time\n+    time.Sleep(200 * time.Millisecond)\n+    \n+    // Randomly fail\n+    if rand.Float32() < 0.7 {\n+        return errors.New(\"temporary error\")\n+    }\n+    return nil\n+})\n+\n+stats := r.Stats()\n+\n+// Log detailed statistics\n+log.Printf(\"Operation completed in %v with %d attempts\", \n+    stats.TotalDuration, stats.Attempts)\n+log.Printf(\"Time spent working: %v\", stats.WorkDuration)\n+log.Printf(\"Time spent waiting: %v\", stats.DelayDuration)\n+\n+if err != nil {\n+    log.Printf(\"Failed after %d attempts: %v\", stats.Attempts, err)\n+} else {\n+    log.Printf(\"Succeeded after %d attempts\", stats.Attempts)\n+}\n+```\n+\n+### Thread Safety\n+\n+Note that the `Repeater` is not thread-safe. Each `Repeater` instance should not be used concurrently for different functions. Create separate `Repeater` instances for concurrent operations."
    },
    {
      "sha": "1ff35dea140b34351fdf2fe7c5b1b52fd29f87f1",
      "filename": "backend/vendor/github.com/go-pkgz/repeater/v2/repeater.go",
      "status": "added",
      "additions": 167,
      "deletions": 0,
      "changes": 167,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2Frepeater.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2Frepeater.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2Frepeater.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,167 @@\n+// Package repeater implements retry functionality with different strategies.\n+// It provides fixed delays and various backoff strategies (constant, linear, exponential) with jitter support.\n+// The package allows custom retry strategies and error-specific handling. Context-aware implementation\n+// supports cancellation and timeouts.\n+package repeater\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"time\"\n+)\n+\n+// ErrAny is a special sentinel error that, when passed as a critical error to Do,\n+// makes it fail on any error from the function\n+var ErrAny = errors.New(\"any error\")\n+\n+// ErrorClassifier determines if an error should be retried.\n+// Returns true if the error should trigger a retry, false to stop immediately.\n+type ErrorClassifier func(error) bool\n+\n+// Stats holds execution statistics for a repeater run\n+type Stats struct {\n+\tLastError     error         // last error encountered (nil if succeeded)\n+\tStartedAt     time.Time     // when the repeater started\n+\tFinishedAt    time.Time     // when the repeater finished\n+\tTotalDuration time.Duration // total elapsed time from start to finish\n+\tWorkDuration  time.Duration // time spent executing the function (excluding delays)\n+\tDelayDuration time.Duration // time spent in delays between attempts\n+\tAttempts      int           // number of attempts made (including the successful one)\n+\tSuccess       bool          // whether the operation eventually succeeded\n+}\n+\n+// Repeater holds configuration for retry operations.\n+// Note: Repeater is not thread-safe. Each Repeater instance should not be used\n+// concurrently for different functions. Create separate Repeater instances for\n+// concurrent operations.\n+type Repeater struct {\n+\tstrategy   Strategy\n+\tstats      Stats\n+\tattempts   int\n+\tclassifier ErrorClassifier\n+}\n+\n+// NewWithStrategy creates a repeater with a custom retry strategy\n+func NewWithStrategy(attempts int, strategy Strategy) *Repeater {\n+\tif attempts <= 0 {\n+\t\tattempts = 1\n+\t}\n+\tif strategy == nil {\n+\t\tstrategy = NewFixedDelay(time.Second)\n+\t}\n+\treturn &Repeater{\n+\t\tattempts: attempts,\n+\t\tstrategy: strategy,\n+\t}\n+}\n+\n+// NewBackoff creates a repeater with backoff strategy\n+// Default settings (can be overridden with options):\n+//   - 30s max delay\n+//   - exponential backoff\n+//   - 10% jitter\n+func NewBackoff(attempts int, initial time.Duration, opts ...backoffOption) *Repeater {\n+\treturn NewWithStrategy(attempts, newBackoff(initial, opts...))\n+}\n+\n+// NewFixed creates a repeater with fixed delay strategy\n+func NewFixed(attempts int, delay time.Duration) *Repeater {\n+\treturn NewWithStrategy(attempts, NewFixedDelay(delay))\n+}\n+\n+// Do repeats fun until it succeeds or max attempts reached\n+// terminates immediately on context cancellation or if err matches any in termErrs.\n+// if errs contains ErrAny, terminates on any error.\n+func (r *Repeater) Do(ctx context.Context, fun func() error, termErrs ...error) error {\n+\tvar lastErr error\n+\n+\t// reset and initialize stats\n+\tr.stats = Stats{\n+\t\tStartedAt: time.Now(),\n+\t}\n+\n+\t// finalizeStats updates the stats before returning\n+\tfinalizeStats := func(attempts int, err error) {\n+\t\tr.stats.Attempts = attempts\n+\t\tr.stats.LastError = err\n+\t\tr.stats.FinishedAt = time.Now()\n+\t\tr.stats.TotalDuration = r.stats.FinishedAt.Sub(r.stats.StartedAt)\n+\t}\n+\n+\tinErrors := func(err error) bool {\n+\t\tfor _, e := range termErrs {\n+\t\t\tif errors.Is(e, ErrAny) {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t\tif errors.Is(err, e) {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t}\n+\t\treturn false\n+\t}\n+\n+\tfor attempt := 0; attempt < r.attempts; attempt++ {\n+\t\t// check context before each attempt\n+\t\tif err := ctx.Err(); err != nil {\n+\t\t\tfinalizeStats(attempt, err)\n+\t\t\treturn err //nolint:wrapcheck // context errors are standard and don't need wrapping\n+\t\t}\n+\n+\t\tworkStart := time.Now()\n+\t\tvar err error\n+\t\tif err = fun(); err == nil {\n+\t\t\tr.stats.WorkDuration += time.Since(workStart)\n+\t\t\tr.stats.Success = true\n+\t\t\tfinalizeStats(attempt+1, nil)\n+\t\t\treturn nil\n+\t\t}\n+\n+\t\tr.stats.WorkDuration += time.Since(workStart)\n+\n+\t\tlastErr = err\n+\n+\t\t// if classifier is set, use it to determine if we should retry\n+\t\tif r.classifier != nil {\n+\t\t\tif !r.classifier(err) {\n+\t\t\t\tfinalizeStats(attempt+1, err)\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t} else if inErrors(err) {\n+\t\t\t// fall back to critical errors list if no classifier\n+\t\t\tfinalizeStats(attempt+1, err)\n+\t\t\treturn err\n+\t\t}\n+\n+\t\t// don't sleep after the last attempt\n+\t\tif attempt < r.attempts-1 {\n+\t\t\tdelay := r.strategy.NextDelay(attempt + 1)\n+\t\t\tif delay > 0 {\n+\t\t\t\tdelayStart := time.Now()\n+\t\t\t\tselect {\n+\t\t\t\tcase <-ctx.Done():\n+\t\t\t\t\tr.stats.DelayDuration += time.Since(delayStart)\n+\t\t\t\t\tfinalizeStats(attempt+1, ctx.Err())\n+\t\t\t\t\treturn ctx.Err() //nolint:wrapcheck // context errors are standard and don't need wrapping\n+\t\t\t\tcase <-time.After(delay):\n+\t\t\t\t\tr.stats.DelayDuration += time.Since(delayStart)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tfinalizeStats(r.attempts, lastErr)\n+\treturn lastErr\n+}\n+\n+// SetErrorClassifier sets a function to determine if errors are retryable.\n+// This can be used with any repeater strategy (NewFixed, NewBackoff, NewWithStrategy).\n+// When set, the classifier takes precedence over the critical errors list.\n+// Returns true to retry, false to stop immediately.\n+func (r *Repeater) SetErrorClassifier(classifier ErrorClassifier) {\n+\tr.classifier = classifier\n+}\n+\n+// Stats returns the execution statistics from the last Do() call\n+func (r *Repeater) Stats() Stats {\n+\treturn r.stats\n+}"
    },
    {
      "sha": "c049100d578563efafc327e5411e329810db94d3",
      "filename": "backend/vendor/github.com/go-pkgz/repeater/v2/strategy.go",
      "status": "added",
      "additions": 113,
      "deletions": 0,
      "changes": 113,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2Fstrategy.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2Fstrategy.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fgo-pkgz%2Frepeater%2Fv2%2Fstrategy.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,113 @@\n+package repeater\n+\n+import (\n+\t\"math/rand\"\n+\t\"time\"\n+)\n+\n+// Strategy defines how to calculate delays between retries\n+type Strategy interface {\n+\t// NextDelay returns delay for the next attempt, attempt starts from 1\n+\tNextDelay(attempt int) time.Duration\n+}\n+\n+// FixedDelay implements fixed time delay between attempts\n+type FixedDelay struct {\n+\tDelay time.Duration\n+}\n+\n+// NewFixedDelay creates a new FixedDelay strategy\n+func NewFixedDelay(delay time.Duration) FixedDelay {\n+\treturn FixedDelay{Delay: delay}\n+}\n+\n+// NextDelay returns fixed delay\n+func (s FixedDelay) NextDelay(_ int) time.Duration {\n+\treturn s.Delay\n+}\n+\n+// BackoffType represents the backoff strategy type\n+type BackoffType int\n+\n+const (\n+\t// BackoffConstant keeps delays the same between attempts\n+\tBackoffConstant BackoffType = iota\n+\t// BackoffLinear increases delays linearly between attempts\n+\tBackoffLinear\n+\t// BackoffExponential increases delays exponentially between attempts\n+\tBackoffExponential\n+)\n+\n+// backoff implements various backoff strategies with optional jitter\n+type backoff struct {\n+\tinitial  time.Duration\n+\tmaxDelay time.Duration\n+\tbtype    BackoffType\n+\tjitter   float64\n+}\n+\n+type backoffOption func(*backoff)\n+\n+// WithMaxDelay sets maximum delay for the backoff strategy\n+func WithMaxDelay(d time.Duration) backoffOption { //nolint:revive // unexported type is used in the same package\n+\treturn func(b *backoff) {\n+\t\tb.maxDelay = d\n+\t}\n+}\n+\n+// WithBackoffType sets backoff type for the strategy\n+func WithBackoffType(t BackoffType) backoffOption { //nolint:revive // unexported type is used in the same package\n+\treturn func(b *backoff) {\n+\t\tb.btype = t\n+\t}\n+}\n+\n+// WithJitter sets jitter factor for the backoff strategy\n+func WithJitter(factor float64) backoffOption { //nolint:revive // unexported type is used in the same package\n+\treturn func(b *backoff) {\n+\t\tb.jitter = factor\n+\t}\n+}\n+\n+func newBackoff(initial time.Duration, opts ...backoffOption) *backoff {\n+\tb := &backoff{\n+\t\tinitial:  initial,\n+\t\tmaxDelay: 30 * time.Second,\n+\t\tbtype:    BackoffExponential,\n+\t\tjitter:   0.1,\n+\t}\n+\n+\tfor _, opt := range opts {\n+\t\topt(b)\n+\t}\n+\n+\treturn b\n+}\n+\n+// NextDelay returns delay for the next attempt\n+func (s backoff) NextDelay(attempt int) time.Duration {\n+\tif attempt <= 0 {\n+\t\treturn 0\n+\t}\n+\n+\tvar delay time.Duration\n+\tswitch s.btype {\n+\tcase BackoffConstant:\n+\t\tdelay = s.initial\n+\tcase BackoffLinear:\n+\t\tdelay = s.initial * time.Duration(attempt)\n+\tcase BackoffExponential:\n+\t\tdelay = s.initial * time.Duration(1<<(attempt-1))\n+\t}\n+\n+\tif s.maxDelay > 0 && delay > s.maxDelay {\n+\t\tdelay = s.maxDelay\n+\t}\n+\n+\tif s.jitter > 0 {\n+\t\tjitter := float64(delay) * s.jitter\n+\t\tdelay = time.Duration(float64(delay) + (rand.Float64()*jitter - jitter/2)) //nolint:gosec // no need for secure random here\n+\t}\n+\n+\treturn delay\n+}"
    },
    {
      "sha": "af2ef639536db0ada5459f260067a9b3f61d6f2f",
      "filename": "backend/vendor/github.com/klauspost/compress/README.md",
      "status": "modified",
      "additions": 16,
      "deletions": 1,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2FREADME.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2FREADME.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2FREADME.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -27,6 +27,16 @@ Use the links above for more information on each.\n \r\n # changelog\r\n \r\n+* Oct 20, 2025 - [1.18.1](https://github.com/klauspost/compress/releases/tag/v1.18.1)\r\n+  * zstd: Add simple zstd EncodeTo/DecodeTo functions  https://github.com/klauspost/compress/pull/1079\r\n+  * zstd: Fix incorrect buffer size in dictionary encodes https://github.com/klauspost/compress/pull/1059\r\n+  * s2: check for cap, not len of buffer in EncodeBetter/Best by @vdarulis in https://github.com/klauspost/compress/pull/1080\r\n+  * zlib: Avoiding extra allocation in zlib.reader.Reset by @travelpolicy in https://github.com/klauspost/compress/pull/1086\r\n+  * gzhttp: remove redundant err check in zstdReader by @ryanfowler in https://github.com/klauspost/compress/pull/1090\r\n+  * flate: Faster load+store https://github.com/klauspost/compress/pull/1104\r\n+  * flate: Simplify matchlen https://github.com/klauspost/compress/pull/1101\r\n+  * flate: Use exact sizes for huffman tables https://github.com/klauspost/compress/pull/1103\r\n+\r\n * Feb 19th, 2025 - [1.18.0](https://github.com/klauspost/compress/releases/tag/v1.18.0)\r\n   * Add unsafe little endian loaders https://github.com/klauspost/compress/pull/1036\r\n   * fix: check `r.err != nil` but return a nil value error `err` by @alingse in https://github.com/klauspost/compress/pull/1028\r\n@@ -36,6 +46,9 @@ Use the links above for more information on each.\n   * flate: Fix matchlen L5+L6 https://github.com/klauspost/compress/pull/1049\r\n   * flate: Cleanup & reduce casts https://github.com/klauspost/compress/pull/1050\r\n \r\n+<details>\r\n+\t<summary>See changes to v1.17.x</summary>\r\n+\r\n * Oct 11th, 2024 - [1.17.11](https://github.com/klauspost/compress/releases/tag/v1.17.11)\r\n   * zstd: Fix extra CRC written with multiple Close calls https://github.com/klauspost/compress/pull/1017\r\n   * s2: Don't use stack for index tables https://github.com/klauspost/compress/pull/1014\r\n@@ -102,7 +115,8 @@ https://github.com/klauspost/compress/pull/919 https://github.com/klauspost/comp\n \t* s2: Do 2 overlapping match checks https://github.com/klauspost/compress/pull/839\r\n \t* flate: Add amd64 assembly matchlen https://github.com/klauspost/compress/pull/837\r\n \t* gzip: Copy bufio.Reader on Reset by @thatguystone in https://github.com/klauspost/compress/pull/860\r\n-\r\n+   \r\n+</details>\r\n <details>\r\n \t<summary>See changes to v1.16.x</summary>\r\n \r\n@@ -669,3 +683,4 @@ Here are other packages of good quality and pure Go (no cgo wrappers or autoconv\n # license\r\n \r\n This code is licensed under the same conditions as the original Go code. See LICENSE file.\r\n+\r"
    },
    {
      "sha": "d58b3fe4237cbf67a07373d2ef723abe2e1f221d",
      "filename": "backend/vendor/github.com/klauspost/compress/fse/bitwriter.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Ffse%2Fbitwriter.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Ffse%2Fbitwriter.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Ffse%2Fbitwriter.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -143,7 +143,7 @@ func (b *bitWriter) flush32() {\n // flushAlign will flush remaining full bytes and align to next byte boundary.\n func (b *bitWriter) flushAlign() {\n \tnbBytes := (b.nBits + 7) >> 3\n-\tfor i := uint8(0); i < nbBytes; i++ {\n+\tfor i := range nbBytes {\n \t\tb.out = append(b.out, byte(b.bitContainer>>(i*8)))\n \t}\n \tb.nBits = 0"
    },
    {
      "sha": "8c8baa4fc2c426fd51c8ff749694533ce46dc2ab",
      "filename": "backend/vendor/github.com/klauspost/compress/fse/compress.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Ffse%2Fcompress.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Ffse%2Fcompress.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Ffse%2Fcompress.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -396,7 +396,7 @@ func (s *Scratch) buildCTable() error {\n \t\t\tif v > largeLimit {\n \t\t\t\ts.zeroBits = true\n \t\t\t}\n-\t\t\tfor nbOccurrences := int16(0); nbOccurrences < v; nbOccurrences++ {\n+\t\t\tfor range v {\n \t\t\t\ttableSymbol[position] = symbol\n \t\t\t\tposition = (position + step) & tableMask\n \t\t\t\tfor position > highThreshold {"
    },
    {
      "sha": "41db94cded089fba249e6522ce5b58ec6ff0d8a3",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/bitwriter.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fbitwriter.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fbitwriter.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fbitwriter.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -85,7 +85,7 @@ func (b *bitWriter) flush32() {\n // flushAlign will flush remaining full bytes and align to next byte boundary.\n func (b *bitWriter) flushAlign() {\n \tnbBytes := (b.nBits + 7) >> 3\n-\tfor i := uint8(0); i < nbBytes; i++ {\n+\tfor i := range nbBytes {\n \t\tb.out = append(b.out, byte(b.bitContainer>>(i*8)))\n \t}\n \tb.nBits = 0"
    },
    {
      "sha": "a97cf1b5d3593b82a25ecfe8fad67436abf25989",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/compress.go",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fcompress.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fcompress.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fcompress.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -276,7 +276,7 @@ func (s *Scratch) compress4X(src []byte) ([]byte, error) {\n \toffsetIdx := len(s.Out)\n \ts.Out = append(s.Out, sixZeros[:]...)\n \n-\tfor i := 0; i < 4; i++ {\n+\tfor i := range 4 {\n \t\ttoDo := src\n \t\tif len(toDo) > segmentSize {\n \t\t\ttoDo = toDo[:segmentSize]\n@@ -312,7 +312,7 @@ func (s *Scratch) compress4Xp(src []byte) ([]byte, error) {\n \tsegmentSize := (len(src) + 3) / 4\n \tvar wg sync.WaitGroup\n \twg.Add(4)\n-\tfor i := 0; i < 4; i++ {\n+\tfor i := range 4 {\n \t\ttoDo := src\n \t\tif len(toDo) > segmentSize {\n \t\t\ttoDo = toDo[:segmentSize]\n@@ -326,7 +326,7 @@ func (s *Scratch) compress4Xp(src []byte) ([]byte, error) {\n \t\t}(i)\n \t}\n \twg.Wait()\n-\tfor i := 0; i < 4; i++ {\n+\tfor i := range 4 {\n \t\to := s.tmpOut[i]\n \t\tif len(o) > math.MaxUint16 {\n \t\t\t// We cannot store the size in the jump table"
    },
    {
      "sha": "7d0efa8818a7f630950a9773dbe84317212905fc",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/decompress.go",
      "status": "modified",
      "additions": 4,
      "deletions": 10,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fdecompress.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fdecompress.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fdecompress.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -626,7 +626,7 @@ func (d *Decoder) decompress4X8bit(dst, src []byte) ([]byte, error) {\n \n \tvar br [4]bitReaderBytes\n \tstart := 6\n-\tfor i := 0; i < 3; i++ {\n+\tfor i := range 3 {\n \t\tlength := int(src[i*2]) | (int(src[i*2+1]) << 8)\n \t\tif start+length >= len(src) {\n \t\t\treturn nil, errors.New(\"truncated input (or invalid offset)\")\n@@ -798,10 +798,7 @@ func (d *Decoder) decompress4X8bit(dst, src []byte) ([]byte, error) {\n \tremainBytes := dstEvery - (decoded / 4)\n \tfor i := range br {\n \t\toffset := dstEvery * i\n-\t\tendsAt := offset + remainBytes\n-\t\tif endsAt > len(out) {\n-\t\t\tendsAt = len(out)\n-\t\t}\n+\t\tendsAt := min(offset+remainBytes, len(out))\n \t\tbr := &br[i]\n \t\tbitsLeft := br.remaining()\n \t\tfor bitsLeft > 0 {\n@@ -864,7 +861,7 @@ func (d *Decoder) decompress4X8bit(dst, src []byte) ([]byte, error) {\n func (d *Decoder) decompress4X8bitExactly(dst, src []byte) ([]byte, error) {\n \tvar br [4]bitReaderBytes\n \tstart := 6\n-\tfor i := 0; i < 3; i++ {\n+\tfor i := range 3 {\n \t\tlength := int(src[i*2]) | (int(src[i*2+1]) << 8)\n \t\tif start+length >= len(src) {\n \t\t\treturn nil, errors.New(\"truncated input (or invalid offset)\")\n@@ -1035,10 +1032,7 @@ func (d *Decoder) decompress4X8bitExactly(dst, src []byte) ([]byte, error) {\n \tremainBytes := dstEvery - (decoded / 4)\n \tfor i := range br {\n \t\toffset := dstEvery * i\n-\t\tendsAt := offset + remainBytes\n-\t\tif endsAt > len(out) {\n-\t\t\tendsAt = len(out)\n-\t\t}\n+\t\tendsAt := min(offset+remainBytes, len(out))\n \t\tbr := &br[i]\n \t\tbitsLeft := br.remaining()\n \t\tfor bitsLeft > 0 {"
    },
    {
      "sha": "99ddd4af97c79d5a27d01dbf26d9983ad1384c9a",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/decompress_amd64.go",
      "status": "modified",
      "additions": 2,
      "deletions": 5,
      "changes": 7,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fdecompress_amd64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fdecompress_amd64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fdecompress_amd64.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -58,7 +58,7 @@ func (d *Decoder) Decompress4X(dst, src []byte) ([]byte, error) {\n \tvar br [4]bitReaderShifted\n \t// Decode \"jump table\"\n \tstart := 6\n-\tfor i := 0; i < 3; i++ {\n+\tfor i := range 3 {\n \t\tlength := int(src[i*2]) | (int(src[i*2+1]) << 8)\n \t\tif start+length >= len(src) {\n \t\t\treturn nil, errors.New(\"truncated input (or invalid offset)\")\n@@ -109,10 +109,7 @@ func (d *Decoder) Decompress4X(dst, src []byte) ([]byte, error) {\n \tremainBytes := dstEvery - (decoded / 4)\n \tfor i := range br {\n \t\toffset := dstEvery * i\n-\t\tendsAt := offset + remainBytes\n-\t\tif endsAt > len(out) {\n-\t\t\tendsAt = len(out)\n-\t\t}\n+\t\tendsAt := min(offset+remainBytes, len(out))\n \t\tbr := &br[i]\n \t\tbitsLeft := br.remaining()\n \t\tfor bitsLeft > 0 {"
    },
    {
      "sha": "67d9e05b6cc333876c6a8704bd3b4cb519f6a98a",
      "filename": "backend/vendor/github.com/klauspost/compress/huff0/huff0.go",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fhuff0.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fhuff0.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fhuff0%2Fhuff0.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -201,7 +201,7 @@ func (c cTable) write(s *Scratch) error {\n \tfor i := range hist[:16] {\n \t\thist[i] = 0\n \t}\n-\tfor n := uint8(0); n < maxSymbolValue; n++ {\n+\tfor n := range maxSymbolValue {\n \t\tv := bitsToWeight[c[n].nBits] & 15\n \t\thuffWeight[n] = v\n \t\thist[v]++\n@@ -271,7 +271,7 @@ func (c cTable) estTableSize(s *Scratch) (sz int, err error) {\n \tfor i := range hist[:16] {\n \t\thist[i] = 0\n \t}\n-\tfor n := uint8(0); n < maxSymbolValue; n++ {\n+\tfor n := range maxSymbolValue {\n \t\tv := bitsToWeight[c[n].nBits] & 15\n \t\thuffWeight[n] = v\n \t\thist[v]++"
    },
    {
      "sha": "4f2a0d8c58d93f635cdcba09b5865f157101eafc",
      "filename": "backend/vendor/github.com/klauspost/compress/internal/le/unsafe_disabled.go",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fle%2Funsafe_disabled.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fle%2Funsafe_disabled.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fle%2Funsafe_disabled.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -37,6 +37,6 @@ func Store32(b []byte, v uint32) {\n }\n \n // Store64 will store v at b.\n-func Store64(b []byte, v uint64) {\n-\tbinary.LittleEndian.PutUint64(b, v)\n+func Store64[I Indexer](b []byte, i I, v uint64) {\n+\tbinary.LittleEndian.PutUint64(b[i:], v)\n }"
    },
    {
      "sha": "218a38bc4a5ae14feb1a3c027746c13f9b9508d4",
      "filename": "backend/vendor/github.com/klauspost/compress/internal/le/unsafe_enabled.go",
      "status": "modified",
      "additions": 3,
      "deletions": 6,
      "changes": 9,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fle%2Funsafe_enabled.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fle%2Funsafe_enabled.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fle%2Funsafe_enabled.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -38,18 +38,15 @@ func Load64[I Indexer](b []byte, i I) uint64 {\n \n // Store16 will store v at b.\n func Store16(b []byte, v uint16) {\n-\t//binary.LittleEndian.PutUint16(b, v)\n \t*(*uint16)(unsafe.Pointer(unsafe.SliceData(b))) = v\n }\n \n // Store32 will store v at b.\n func Store32(b []byte, v uint32) {\n-\t//binary.LittleEndian.PutUint32(b, v)\n \t*(*uint32)(unsafe.Pointer(unsafe.SliceData(b))) = v\n }\n \n-// Store64 will store v at b.\n-func Store64(b []byte, v uint64) {\n-\t//binary.LittleEndian.PutUint64(b, v)\n-\t*(*uint64)(unsafe.Pointer(unsafe.SliceData(b))) = v\n+// Store64 will store v at b[i:].\n+func Store64[I Indexer](b []byte, i I, v uint64) {\n+\t*(*uint64)(unsafe.Add(unsafe.Pointer(unsafe.SliceData(b)), i)) = v\n }"
    },
    {
      "sha": "a2c82fcd22636178bc87528cce761923a080e956",
      "filename": "backend/vendor/github.com/klauspost/compress/internal/snapref/decode.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fsnapref%2Fdecode.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fsnapref%2Fdecode.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fsnapref%2Fdecode.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -209,7 +209,7 @@ func (r *Reader) fill() error {\n \t\t\tif !r.readFull(r.buf[:len(magicBody)], false) {\n \t\t\t\treturn r.err\n \t\t\t}\n-\t\t\tfor i := 0; i < len(magicBody); i++ {\n+\t\t\tfor i := range len(magicBody) {\n \t\t\t\tif r.buf[i] != magicBody[i] {\n \t\t\t\t\tr.err = ErrCorrupt\n \t\t\t\t\treturn r.err"
    },
    {
      "sha": "860a994167a0defa473131c5b6ae7e0ccd683a54",
      "filename": "backend/vendor/github.com/klauspost/compress/internal/snapref/encode.go",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fsnapref%2Fencode.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fsnapref%2Fencode.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Finternal%2Fsnapref%2Fencode.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -20,8 +20,10 @@ import (\n func Encode(dst, src []byte) []byte {\n \tif n := MaxEncodedLen(len(src)); n < 0 {\n \t\tpanic(ErrTooLarge)\n-\t} else if len(dst) < n {\n+\t} else if cap(dst) < n {\n \t\tdst = make([]byte, n)\n+\t} else {\n+\t\tdst = dst[:n]\n \t}\n \n \t// The block starts with the varint-encoded length of the decompressed bytes."
    },
    {
      "sha": "b22b297e62acf46245877e4c271e984dad7d4b12",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/bitwriter.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fbitwriter.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fbitwriter.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fbitwriter.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -88,7 +88,7 @@ func (b *bitWriter) flush32() {\n // flushAlign will flush remaining full bytes and align to next byte boundary.\n func (b *bitWriter) flushAlign() {\n \tnbBytes := (b.nBits + 7) >> 3\n-\tfor i := uint8(0); i < nbBytes; i++ {\n+\tfor i := range nbBytes {\n \t\tb.out = append(b.out, byte(b.bitContainer>>(i*8)))\n \t}\n \tb.nBits = 0"
    },
    {
      "sha": "2329e996f86ed783812053e46d769a53f56981f1",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/blockdec.go",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fblockdec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fblockdec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fblockdec.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -54,11 +54,11 @@ const (\n )\n \n var (\n-\thuffDecoderPool = sync.Pool{New: func() interface{} {\n+\thuffDecoderPool = sync.Pool{New: func() any {\n \t\treturn &huff0.Scratch{}\n \t}}\n \n-\tfseDecoderPool = sync.Pool{New: func() interface{} {\n+\tfseDecoderPool = sync.Pool{New: func() any {\n \t\treturn &fseDecoder{}\n \t}}\n )\n@@ -553,7 +553,7 @@ func (b *blockDec) prepareSequences(in []byte, hist *history) (err error) {\n \t\tif compMode&3 != 0 {\n \t\t\treturn errors.New(\"corrupt block: reserved bits not zero\")\n \t\t}\n-\t\tfor i := uint(0); i < 3; i++ {\n+\t\tfor i := range uint(3) {\n \t\t\tmode := seqCompMode((compMode >> (6 - i*2)) & 3)\n \t\t\tif debugDecoder {\n \t\t\t\tprintln(\"Table\", tableIndex(i), \"is\", mode)"
    },
    {
      "sha": "30df5513d5607a149883f014f3190a5fe0c1d257",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/decoder.go",
      "status": "modified",
      "additions": 3,
      "deletions": 5,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fdecoder.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fdecoder.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fdecoder.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -373,11 +373,9 @@ func (d *Decoder) DecodeAll(input, dst []byte) ([]byte, error) {\n \t\tif cap(dst) == 0 && !d.o.limitToCap {\n \t\t\t// Allocate len(input) * 2 by default if nothing is provided\n \t\t\t// and we didn't get frame content size.\n-\t\t\tsize := len(input) * 2\n-\t\t\t// Cap to 1 MB.\n-\t\t\tif size > 1<<20 {\n-\t\t\t\tsize = 1 << 20\n-\t\t\t}\n+\t\t\tsize := min(\n+\t\t\t\t// Cap to 1 MB.\n+\t\t\t\tlen(input)*2, 1<<20)\n \t\t\tif uint64(size) > d.o.maxDecodedSize {\n \t\t\t\tsize = int(d.o.maxDecodedSize)\n \t\t\t}"
    },
    {
      "sha": "2ffbfdf379e691ff1b9cb06267faeea1a9ef2c79",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/dict.go",
      "status": "modified",
      "additions": 7,
      "deletions": 13,
      "changes": 20,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fdict.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fdict.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fdict.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -194,17 +194,17 @@ func BuildDict(o BuildDictOptions) ([]byte, error) {\n \thist := o.History\n \tcontents := o.Contents\n \tdebug := o.DebugOut != nil\n-\tprintln := func(args ...interface{}) {\n+\tprintln := func(args ...any) {\n \t\tif o.DebugOut != nil {\n \t\t\tfmt.Fprintln(o.DebugOut, args...)\n \t\t}\n \t}\n-\tprintf := func(s string, args ...interface{}) {\n+\tprintf := func(s string, args ...any) {\n \t\tif o.DebugOut != nil {\n \t\t\tfmt.Fprintf(o.DebugOut, s, args...)\n \t\t}\n \t}\n-\tprint := func(args ...interface{}) {\n+\tprint := func(args ...any) {\n \t\tif o.DebugOut != nil {\n \t\t\tfmt.Fprint(o.DebugOut, args...)\n \t\t}\n@@ -424,16 +424,10 @@ func BuildDict(o BuildDictOptions) ([]byte, error) {\n \t}\n \n \t// Literal table\n-\tavgSize := litTotal\n-\tif avgSize > huff0.BlockSizeMax/2 {\n-\t\tavgSize = huff0.BlockSizeMax / 2\n-\t}\n+\tavgSize := min(litTotal, huff0.BlockSizeMax/2)\n \thuffBuff := make([]byte, 0, avgSize)\n \t// Target size\n-\tdiv := litTotal / avgSize\n-\tif div < 1 {\n-\t\tdiv = 1\n-\t}\n+\tdiv := max(litTotal/avgSize, 1)\n \tif debug {\n \t\tprintln(\"Huffman weights:\")\n \t}\n@@ -454,7 +448,7 @@ func BuildDict(o BuildDictOptions) ([]byte, error) {\n \t\thuffBuff = append(huffBuff, 255)\n \t}\n \tscratch := &huff0.Scratch{TableLog: 11}\n-\tfor tries := 0; tries < 255; tries++ {\n+\tfor tries := range 255 {\n \t\tscratch = &huff0.Scratch{TableLog: 11}\n \t\t_, _, err = huff0.Compress1X(huffBuff, scratch)\n \t\tif err == nil {\n@@ -471,7 +465,7 @@ func BuildDict(o BuildDictOptions) ([]byte, error) {\n \n \t\t\t// Bail out.... Just generate something\n \t\t\thuffBuff = append(huffBuff, bytes.Repeat([]byte{255}, 10000)...)\n-\t\t\tfor i := 0; i < 128; i++ {\n+\t\t\tfor i := range 128 {\n \t\t\t\thuffBuff = append(huffBuff, byte(i))\n \t\t\t}\n \t\t\tcontinue"
    },
    {
      "sha": "c1192ec38f4db97340c931e3b3577d2d959d3216",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/enc_base.go",
      "status": "modified",
      "additions": 4,
      "deletions": 6,
      "changes": 10,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_base.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_base.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_base.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -8,7 +8,7 @@ import (\n )\n \n const (\n-\tdictShardBits = 6\n+\tdictShardBits = 7\n )\n \n type fastBase struct {\n@@ -41,11 +41,9 @@ func (e *fastBase) AppendCRC(dst []byte) []byte {\n // or a window size small enough to contain the input size, if > 0.\n func (e *fastBase) WindowSize(size int64) int32 {\n \tif size > 0 && size < int64(e.maxMatchOff) {\n-\t\tb := int32(1) << uint(bits.Len(uint(size)))\n-\t\t// Keep minimum window.\n-\t\tif b < 1024 {\n-\t\t\tb = 1024\n-\t\t}\n+\t\tb := max(\n+\t\t\t// Keep minimum window.\n+\t\t\tint32(1)<<uint(bits.Len(uint(size))), 1024)\n \t\treturn b\n \t}\n \treturn e.maxMatchOff"
    },
    {
      "sha": "c1581cfcb8b4bbdc743e0805b54d3a177983e9d9",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/enc_best.go",
      "status": "modified",
      "additions": 6,
      "deletions": 17,
      "changes": 23,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_best.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_best.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_best.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -158,11 +158,9 @@ func (e *bestFastEncoder) Encode(blk *blockEnc, src []byte) {\n \n \t// Use this to estimate literal cost.\n \t// Scaled by 10 bits.\n-\tbitsPerByte := int32((compress.ShannonEntropyBits(src) * 1024) / len(src))\n-\t// Huffman can never go < 1 bit/byte\n-\tif bitsPerByte < 1024 {\n-\t\tbitsPerByte = 1024\n-\t}\n+\tbitsPerByte := max(\n+\t\t// Huffman can never go < 1 bit/byte\n+\t\tint32((compress.ShannonEntropyBits(src)*1024)/len(src)), 1024)\n \n \t// Override src\n \tsrc = e.hist\n@@ -235,10 +233,7 @@ encodeLoop:\n \t\t\t\t// Extend candidate match backwards as far as possible.\n \t\t\t\t// Do not extend repeats as we can assume they are optimal\n \t\t\t\t// and offsets change if s == nextEmit.\n-\t\t\t\ttMin := s - e.maxMatchOff\n-\t\t\t\tif tMin < 0 {\n-\t\t\t\t\ttMin = 0\n-\t\t\t\t}\n+\t\t\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\t\t\tfor offset > tMin && s > nextEmit && src[offset-1] == src[s-1] && l < maxMatchLength {\n \t\t\t\t\ts--\n \t\t\t\t\toffset--\n@@ -382,10 +377,7 @@ encodeLoop:\n \t\t\tnextEmit = s\n \n \t\t\t// Index skipped...\n-\t\t\tend := s\n-\t\t\tif s > sLimit+4 {\n-\t\t\t\tend = sLimit + 4\n-\t\t\t}\n+\t\t\tend := min(s, sLimit+4)\n \t\t\toff := index0 + e.cur\n \t\t\tfor index0 < end {\n \t\t\t\tcv0 := load6432(src, index0)\n@@ -444,10 +436,7 @@ encodeLoop:\n \t\tnextEmit = s\n \n \t\t// Index old s + 1 -> s - 1 or sLimit\n-\t\tend := s\n-\t\tif s > sLimit-4 {\n-\t\t\tend = sLimit - 4\n-\t\t}\n+\t\tend := min(s, sLimit-4)\n \n \t\toff := index0 + e.cur\n \t\tfor index0 < end {"
    },
    {
      "sha": "85dcd28c32eb2a0e4c643cd4ae9aee1460e4324a",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/enc_better.go",
      "status": "modified",
      "additions": 6,
      "deletions": 24,
      "changes": 30,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_better.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_better.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_better.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -190,10 +190,7 @@ encodeLoop:\n \t\t\t\t\t// and have to do special offset treatment.\n \t\t\t\t\tstartLimit := nextEmit + 1\n \n-\t\t\t\t\ttMin := s - e.maxMatchOff\n-\t\t\t\t\tif tMin < 0 {\n-\t\t\t\t\t\ttMin = 0\n-\t\t\t\t\t}\n+\t\t\t\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\t\t\t\tfor repIndex > tMin && start > startLimit && src[repIndex-1] == src[start-1] && seq.matchLen < maxMatchLength-zstdMinMatch-1 {\n \t\t\t\t\t\trepIndex--\n \t\t\t\t\t\tstart--\n@@ -252,10 +249,7 @@ encodeLoop:\n \t\t\t\t\t// and have to do special offset treatment.\n \t\t\t\t\tstartLimit := nextEmit + 1\n \n-\t\t\t\t\ttMin := s - e.maxMatchOff\n-\t\t\t\t\tif tMin < 0 {\n-\t\t\t\t\t\ttMin = 0\n-\t\t\t\t\t}\n+\t\t\t\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\t\t\t\tfor repIndex > tMin && start > startLimit && src[repIndex-1] == src[start-1] && seq.matchLen < maxMatchLength-zstdMinMatch-1 {\n \t\t\t\t\t\trepIndex--\n \t\t\t\t\t\tstart--\n@@ -480,10 +474,7 @@ encodeLoop:\n \t\tl := matched\n \n \t\t// Extend backwards\n-\t\ttMin := s - e.maxMatchOff\n-\t\tif tMin < 0 {\n-\t\t\ttMin = 0\n-\t\t}\n+\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] && l < maxMatchLength {\n \t\t\ts--\n \t\t\tt--\n@@ -719,10 +710,7 @@ encodeLoop:\n \t\t\t\t\t// and have to do special offset treatment.\n \t\t\t\t\tstartLimit := nextEmit + 1\n \n-\t\t\t\t\ttMin := s - e.maxMatchOff\n-\t\t\t\t\tif tMin < 0 {\n-\t\t\t\t\t\ttMin = 0\n-\t\t\t\t\t}\n+\t\t\t\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\t\t\t\tfor repIndex > tMin && start > startLimit && src[repIndex-1] == src[start-1] && seq.matchLen < maxMatchLength-zstdMinMatch-1 {\n \t\t\t\t\t\trepIndex--\n \t\t\t\t\t\tstart--\n@@ -783,10 +771,7 @@ encodeLoop:\n \t\t\t\t\t// and have to do special offset treatment.\n \t\t\t\t\tstartLimit := nextEmit + 1\n \n-\t\t\t\t\ttMin := s - e.maxMatchOff\n-\t\t\t\t\tif tMin < 0 {\n-\t\t\t\t\t\ttMin = 0\n-\t\t\t\t\t}\n+\t\t\t\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\t\t\t\tfor repIndex > tMin && start > startLimit && src[repIndex-1] == src[start-1] && seq.matchLen < maxMatchLength-zstdMinMatch-1 {\n \t\t\t\t\t\trepIndex--\n \t\t\t\t\t\tstart--\n@@ -1005,10 +990,7 @@ encodeLoop:\n \t\tl := matched\n \n \t\t// Extend backwards\n-\t\ttMin := s - e.maxMatchOff\n-\t\tif tMin < 0 {\n-\t\t\ttMin = 0\n-\t\t}\n+\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] && l < maxMatchLength {\n \t\t\ts--\n \t\t\tt--"
    },
    {
      "sha": "cf8cad00dcf747c1ef7ee45ca9af09a83757151c",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/enc_dfast.go",
      "status": "modified",
      "additions": 7,
      "deletions": 25,
      "changes": 32,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_dfast.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_dfast.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_dfast.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -13,7 +13,7 @@ const (\n \tdFastLongLen       = 8                       // Bytes used for table hash\n \n \tdLongTableShardCnt  = 1 << (dFastLongTableBits - dictShardBits) // Number of shards in the table\n-\tdLongTableShardSize = dFastLongTableSize / tableShardCnt        // Size of an individual shard\n+\tdLongTableShardSize = dFastLongTableSize / dLongTableShardCnt   // Size of an individual shard\n \n \tdFastShortTableBits = tableBits                // Bits used in the short match table\n \tdFastShortTableSize = 1 << dFastShortTableBits // Size of the table\n@@ -149,10 +149,7 @@ encodeLoop:\n \t\t\t\t\t// and have to do special offset treatment.\n \t\t\t\t\tstartLimit := nextEmit + 1\n \n-\t\t\t\t\ttMin := s - e.maxMatchOff\n-\t\t\t\t\tif tMin < 0 {\n-\t\t\t\t\t\ttMin = 0\n-\t\t\t\t\t}\n+\t\t\t\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\t\t\t\tfor repIndex > tMin && start > startLimit && src[repIndex-1] == src[start-1] && seq.matchLen < maxMatchLength-zstdMinMatch-1 {\n \t\t\t\t\t\trepIndex--\n \t\t\t\t\t\tstart--\n@@ -266,10 +263,7 @@ encodeLoop:\n \t\tl := e.matchlen(s+4, t+4, src) + 4\n \n \t\t// Extend backwards\n-\t\ttMin := s - e.maxMatchOff\n-\t\tif tMin < 0 {\n-\t\t\ttMin = 0\n-\t\t}\n+\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] && l < maxMatchLength {\n \t\t\ts--\n \t\t\tt--\n@@ -462,10 +456,7 @@ encodeLoop:\n \t\t\t\t\t// and have to do special offset treatment.\n \t\t\t\t\tstartLimit := nextEmit + 1\n \n-\t\t\t\t\ttMin := s - e.maxMatchOff\n-\t\t\t\t\tif tMin < 0 {\n-\t\t\t\t\t\ttMin = 0\n-\t\t\t\t\t}\n+\t\t\t\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\t\t\t\tfor repIndex > tMin && start > startLimit && src[repIndex-1] == src[start-1] {\n \t\t\t\t\t\trepIndex--\n \t\t\t\t\t\tstart--\n@@ -576,10 +567,7 @@ encodeLoop:\n \t\tl := int32(matchLen(src[s+4:], src[t+4:])) + 4\n \n \t\t// Extend backwards\n-\t\ttMin := s - e.maxMatchOff\n-\t\tif tMin < 0 {\n-\t\t\ttMin = 0\n-\t\t}\n+\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] {\n \t\t\ts--\n \t\t\tt--\n@@ -809,10 +797,7 @@ encodeLoop:\n \t\t\t\t\t// and have to do special offset treatment.\n \t\t\t\t\tstartLimit := nextEmit + 1\n \n-\t\t\t\t\ttMin := s - e.maxMatchOff\n-\t\t\t\t\tif tMin < 0 {\n-\t\t\t\t\t\ttMin = 0\n-\t\t\t\t\t}\n+\t\t\t\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\t\t\t\tfor repIndex > tMin && start > startLimit && src[repIndex-1] == src[start-1] && seq.matchLen < maxMatchLength-zstdMinMatch-1 {\n \t\t\t\t\t\trepIndex--\n \t\t\t\t\t\tstart--\n@@ -927,10 +912,7 @@ encodeLoop:\n \t\tl := e.matchlen(s+4, t+4, src) + 4\n \n \t\t// Extend backwards\n-\t\ttMin := s - e.maxMatchOff\n-\t\tif tMin < 0 {\n-\t\t\ttMin = 0\n-\t\t}\n+\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] && l < maxMatchLength {\n \t\t\ts--\n \t\t\tt--"
    },
    {
      "sha": "9180a3a58203c7132806374b4bad92a7f34d98c4",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/enc_fast.go",
      "status": "modified",
      "additions": 6,
      "deletions": 24,
      "changes": 30,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_fast.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_fast.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fenc_fast.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -143,10 +143,7 @@ encodeLoop:\n \t\t\t\t// and have to do special offset treatment.\n \t\t\t\tstartLimit := nextEmit + 1\n \n-\t\t\t\tsMin := s - e.maxMatchOff\n-\t\t\t\tif sMin < 0 {\n-\t\t\t\t\tsMin = 0\n-\t\t\t\t}\n+\t\t\t\tsMin := max(s-e.maxMatchOff, 0)\n \t\t\t\tfor repIndex > sMin && start > startLimit && src[repIndex-1] == src[start-1] && seq.matchLen < maxMatchLength-zstdMinMatch {\n \t\t\t\t\trepIndex--\n \t\t\t\t\tstart--\n@@ -223,10 +220,7 @@ encodeLoop:\n \t\tl := e.matchlen(s+4, t+4, src) + 4\n \n \t\t// Extend backwards\n-\t\ttMin := s - e.maxMatchOff\n-\t\tif tMin < 0 {\n-\t\t\ttMin = 0\n-\t\t}\n+\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] && l < maxMatchLength {\n \t\t\ts--\n \t\t\tt--\n@@ -387,10 +381,7 @@ encodeLoop:\n \t\t\t\t// and have to do special offset treatment.\n \t\t\t\tstartLimit := nextEmit + 1\n \n-\t\t\t\tsMin := s - e.maxMatchOff\n-\t\t\t\tif sMin < 0 {\n-\t\t\t\t\tsMin = 0\n-\t\t\t\t}\n+\t\t\t\tsMin := max(s-e.maxMatchOff, 0)\n \t\t\t\tfor repIndex > sMin && start > startLimit && src[repIndex-1] == src[start-1] {\n \t\t\t\t\trepIndex--\n \t\t\t\t\tstart--\n@@ -469,10 +460,7 @@ encodeLoop:\n \t\tl := e.matchlen(s+4, t+4, src) + 4\n \n \t\t// Extend backwards\n-\t\ttMin := s - e.maxMatchOff\n-\t\tif tMin < 0 {\n-\t\t\ttMin = 0\n-\t\t}\n+\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] {\n \t\t\ts--\n \t\t\tt--\n@@ -655,10 +643,7 @@ encodeLoop:\n \t\t\t\t// and have to do special offset treatment.\n \t\t\t\tstartLimit := nextEmit + 1\n \n-\t\t\t\tsMin := s - e.maxMatchOff\n-\t\t\t\tif sMin < 0 {\n-\t\t\t\t\tsMin = 0\n-\t\t\t\t}\n+\t\t\t\tsMin := max(s-e.maxMatchOff, 0)\n \t\t\t\tfor repIndex > sMin && start > startLimit && src[repIndex-1] == src[start-1] && seq.matchLen < maxMatchLength-zstdMinMatch {\n \t\t\t\t\trepIndex--\n \t\t\t\t\tstart--\n@@ -735,10 +720,7 @@ encodeLoop:\n \t\tl := e.matchlen(s+4, t+4, src) + 4\n \n \t\t// Extend backwards\n-\t\ttMin := s - e.maxMatchOff\n-\t\tif tMin < 0 {\n-\t\t\ttMin = 0\n-\t\t}\n+\t\ttMin := max(s-e.maxMatchOff, 0)\n \t\tfor t > tMin && s > nextEmit && src[t-1] == src[s-1] && l < maxMatchLength {\n \t\t\ts--\n \t\t\tt--"
    },
    {
      "sha": "d88f067e5c2e3e36c0d9ab5804f10a8ce49316b8",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/framedec.go",
      "status": "modified",
      "additions": 1,
      "deletions": 4,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fframedec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fframedec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fframedec.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -238,10 +238,7 @@ func (d *frameDec) reset(br byteBuffer) error {\n \n \tif d.WindowSize == 0 && d.SingleSegment {\n \t\t// We may not need window in this case.\n-\t\td.WindowSize = d.FrameContentSize\n-\t\tif d.WindowSize < MinWindowSize {\n-\t\t\td.WindowSize = MinWindowSize\n-\t\t}\n+\t\td.WindowSize = max(d.FrameContentSize, MinWindowSize)\n \t\tif d.WindowSize > d.o.maxDecodedSize {\n \t\t\tif debugDecoder {\n \t\t\t\tprintf(\"window size %d > max %d\\n\", d.WindowSize, d.o.maxWindowSize)"
    },
    {
      "sha": "3a0f4e7fbe6ace6fc873cbd29f18add4a4bc83c9",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/fse_encoder.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Ffse_encoder.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Ffse_encoder.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Ffse_encoder.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -149,7 +149,7 @@ func (s *fseEncoder) buildCTable() error {\n \t\t\tif v > largeLimit {\n \t\t\t\ts.zeroBits = true\n \t\t\t}\n-\t\t\tfor nbOccurrences := int16(0); nbOccurrences < v; nbOccurrences++ {\n+\t\t\tfor range v {\n \t\t\t\ttableSymbol[position] = symbol\n \t\t\t\tposition = (position + step) & tableMask\n \t\t\t\tfor position > highThreshold {"
    },
    {
      "sha": "0bfb0e43c7b657f7be6ba1313a04b03f0ab84f9d",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/seqdec.go",
      "status": "modified",
      "additions": 1,
      "deletions": 4,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fseqdec.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fseqdec.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fseqdec.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -231,10 +231,7 @@ func (s *sequenceDecs) decodeSync(hist []byte) error {\n \tllTable, mlTable, ofTable := s.litLengths.fse.dt[:maxTablesize], s.matchLengths.fse.dt[:maxTablesize], s.offsets.fse.dt[:maxTablesize]\n \tllState, mlState, ofState := s.litLengths.state.state, s.matchLengths.state.state, s.offsets.state.state\n \tout := s.out\n-\tmaxBlockSize := maxCompressedBlockSize\n-\tif s.windowSize < maxBlockSize {\n-\t\tmaxBlockSize = s.windowSize\n-\t}\n+\tmaxBlockSize := min(s.windowSize, maxCompressedBlockSize)\n \n \tif debugDecoder {\n \t\tprintln(\"decodeSync: decoding\", seqs, \"sequences\", br.remain(), \"bits remain on stream\")"
    },
    {
      "sha": "1f8c3cec28c849618b0523f3f3cd7f19c29e8b4a",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/seqdec_amd64.go",
      "status": "modified",
      "additions": 2,
      "deletions": 8,
      "changes": 10,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fseqdec_amd64.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fseqdec_amd64.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fseqdec_amd64.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -79,10 +79,7 @@ func (s *sequenceDecs) decodeSyncSimple(hist []byte) (bool, error) {\n \n \tbr := s.br\n \n-\tmaxBlockSize := maxCompressedBlockSize\n-\tif s.windowSize < maxBlockSize {\n-\t\tmaxBlockSize = s.windowSize\n-\t}\n+\tmaxBlockSize := min(s.windowSize, maxCompressedBlockSize)\n \n \tctx := decodeSyncAsmContext{\n \t\tllTable:     s.litLengths.fse.dt[:maxTablesize],\n@@ -237,10 +234,7 @@ func sequenceDecs_decode_56_bmi2(s *sequenceDecs, br *bitReader, ctx *decodeAsmC\n func (s *sequenceDecs) decode(seqs []seqVals) error {\n \tbr := s.br\n \n-\tmaxBlockSize := maxCompressedBlockSize\n-\tif s.windowSize < maxBlockSize {\n-\t\tmaxBlockSize = s.windowSize\n-\t}\n+\tmaxBlockSize := min(s.windowSize, maxCompressedBlockSize)\n \n \tctx := decodeAsmContext{\n \t\tllTable:   s.litLengths.fse.dt[:maxTablesize],"
    },
    {
      "sha": "2efc0497bf95a2294a2703ac1fdcbe909a6cfa2c",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/simple_go124.go",
      "status": "added",
      "additions": 56,
      "deletions": 0,
      "changes": 56,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fsimple_go124.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fsimple_go124.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fsimple_go124.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,56 @@\n+// Copyright 2025+ Klaus Post. All rights reserved.\n+// License information can be found in the LICENSE file.\n+\n+//go:build go1.24\n+\n+package zstd\n+\n+import (\n+\t\"errors\"\n+\t\"runtime\"\n+\t\"sync\"\n+\t\"weak\"\n+)\n+\n+var weakMu sync.Mutex\n+var simpleEnc weak.Pointer[Encoder]\n+var simpleDec weak.Pointer[Decoder]\n+\n+// EncodeTo appends the encoded data from src to dst.\n+func EncodeTo(dst []byte, src []byte) []byte {\n+\tweakMu.Lock()\n+\tenc := simpleEnc.Value()\n+\tif enc == nil {\n+\t\tvar err error\n+\t\tenc, err = NewWriter(nil, WithEncoderConcurrency(runtime.NumCPU()), WithWindowSize(1<<20), WithLowerEncoderMem(true), WithZeroFrames(true))\n+\t\tif err != nil {\n+\t\t\tpanic(\"failed to create simple encoder: \" + err.Error())\n+\t\t}\n+\t\tsimpleEnc = weak.Make(enc)\n+\t}\n+\tweakMu.Unlock()\n+\n+\treturn enc.EncodeAll(src, dst)\n+}\n+\n+// DecodeTo appends the decoded data from src to dst.\n+// The maximum decoded size is 1GiB,\n+// not including what may already be in dst.\n+func DecodeTo(dst []byte, src []byte) ([]byte, error) {\n+\tweakMu.Lock()\n+\tdec := simpleDec.Value()\n+\tif dec == nil {\n+\t\tvar err error\n+\t\tdec, err = NewReader(nil, WithDecoderConcurrency(runtime.NumCPU()), WithDecoderLowmem(true), WithDecoderMaxMemory(1<<30))\n+\t\tif err != nil {\n+\t\t\tweakMu.Unlock()\n+\t\t\treturn nil, errors.New(\"failed to create simple decoder: \" + err.Error())\n+\t\t}\n+\t\truntime.SetFinalizer(dec, func(d *Decoder) {\n+\t\t\td.Close()\n+\t\t})\n+\t\tsimpleDec = weak.Make(dec)\n+\t}\n+\tweakMu.Unlock()\n+\treturn dec.DecodeAll(src, dst)\n+}"
    },
    {
      "sha": "336c28893045b8c29ce1a820a495feccec44108f",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/snappy.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fsnappy.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fsnappy.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fsnappy.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -257,7 +257,7 @@ func (r *SnappyConverter) Convert(in io.Reader, w io.Writer) (int64, error) {\n \t\t\tif !r.readFull(r.buf[:len(snappyMagicBody)], false) {\n \t\t\t\treturn written, r.err\n \t\t\t}\n-\t\t\tfor i := 0; i < len(snappyMagicBody); i++ {\n+\t\t\tfor i := range len(snappyMagicBody) {\n \t\t\t\tif r.buf[i] != snappyMagicBody[i] {\n \t\t\t\t\tprintln(\"r.buf[i] != snappyMagicBody[i]\", r.buf[i], snappyMagicBody[i], i)\n \t\t\t\t\tr.err = ErrSnappyCorrupt"
    },
    {
      "sha": "3198d718926c7ba03215870229fe3112575341df",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/zip.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fzip.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fzip.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fzip.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -19,7 +19,7 @@ const ZipMethodWinZip = 93\n const ZipMethodPKWare = 20\n \n // zipReaderPool is the default reader pool.\n-var zipReaderPool = sync.Pool{New: func() interface{} {\n+var zipReaderPool = sync.Pool{New: func() any {\n \tz, err := NewReader(nil, WithDecoderLowmem(true), WithDecoderMaxWindow(128<<20), WithDecoderConcurrency(1))\n \tif err != nil {\n \t\tpanic(err)"
    },
    {
      "sha": "1a869710d2c35c17c36a9d71612f95b8ff076f06",
      "filename": "backend/vendor/github.com/klauspost/compress/zstd/zstd.go",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fzstd.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fzstd.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fklauspost%2Fcompress%2Fzstd%2Fzstd.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -98,13 +98,13 @@ var (\n \tErrDecoderNilInput = errors.New(\"nil input provided as reader\")\n )\n \n-func println(a ...interface{}) {\n+func println(a ...any) {\n \tif debug || debugDecoder || debugEncoder {\n \t\tlog.Println(a...)\n \t}\n }\n \n-func printf(format string, a ...interface{}) {\n+func printf(format string, a ...any) {\n \tif debug || debugDecoder || debugEncoder {\n \t\tlog.Printf(format, a...)\n \t}"
    },
    {
      "sha": "00710d5077df70f0316aa49cc6964d6e2869a04a",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/.gitignore",
      "status": "modified",
      "additions": 10,
      "deletions": 1,
      "changes": 11,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2F.gitignore",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2F.gitignore",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2F.gitignore?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -3,4 +3,13 @@ testdata/*\n .idea/\n .DS_Store\n *.tar.gz\n-*.dic\n\\ No newline at end of file\n+*.dic\n+redis8tests.sh\n+coverage.txt\n+**/coverage.txt\n+.vscode\n+tmp/*\n+*.test\n+\n+# maintenanceNotifications upgrade documentation (temporary)\n+maintenanceNotifications/docs/"
    },
    {
      "sha": "872454ff7f11c2453a68be6f1f2ac33df6d1a724",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/.golangci.yml",
      "status": "modified",
      "additions": 31,
      "deletions": 0,
      "changes": 31,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2F.golangci.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2F.golangci.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2F.golangci.yml?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -1,3 +1,34 @@\n+version: \"2\"\n run:\n   timeout: 5m\n   tests: false\n+linters:\n+  settings:\n+    staticcheck:\n+      checks:\n+        - all\n+        # Incorrect or missing package comment.\n+        # https://staticcheck.dev/docs/checks/#ST1000\n+        - -ST1000\n+        # Omit embedded fields from selector expression.\n+        # https://staticcheck.dev/docs/checks/#QF1008\n+        - -QF1008\n+        - -ST1003\n+  exclusions:\n+    generated: lax\n+    presets:\n+      - comments\n+      - common-false-positives\n+      - legacy\n+      - std-error-handling\n+    paths:\n+      - third_party$\n+      - builtin$\n+      - examples$\n+formatters:\n+  exclusions:\n+    generated: lax\n+    paths:\n+      - third_party$\n+      - builtin$\n+      - examples$"
    },
    {
      "sha": "e1652b179ad3c121e1fd1e5e367e178e09851055",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/CHANGELOG.md",
      "status": "removed",
      "additions": 0,
      "deletions": 133,
      "changes": 133,
      "blob_url": "https://github.com/umputun/remark42/blob/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FCHANGELOG.md",
      "raw_url": "https://github.com/umputun/remark42/raw/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FCHANGELOG.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FCHANGELOG.md?ref=b4511427905c02e55730d7cb27036f5796decaa9",
      "patch": "@@ -1,133 +0,0 @@\n-## Unreleased\n-\n-### Changed\n-\n-* `go-redis` won't skip span creation if the parent spans is not recording. ([#2980](https://github.com/redis/go-redis/issues/2980))\n-  Users can use the OpenTelemetry sampler to control the sampling behavior.\n-  For instance, you can use the `ParentBased(NeverSample())` sampler from `go.opentelemetry.io/otel/sdk/trace` to keep\n-  a similar behavior (drop orphan spans) of `go-redis` as before.\n-\n-## [9.0.5](https://github.com/redis/go-redis/compare/v9.0.4...v9.0.5) (2023-05-29)\n-\n-\n-### Features\n-\n-* Add ACL LOG ([#2536](https://github.com/redis/go-redis/issues/2536)) ([31ba855](https://github.com/redis/go-redis/commit/31ba855ddebc38fbcc69a75d9d4fb769417cf602))\n-* add field protocol to setupClusterQueryParams ([#2600](https://github.com/redis/go-redis/issues/2600)) ([840c25c](https://github.com/redis/go-redis/commit/840c25cb6f320501886a82a5e75f47b491e46fbe))\n-* add protocol option ([#2598](https://github.com/redis/go-redis/issues/2598)) ([3917988](https://github.com/redis/go-redis/commit/391798880cfb915c4660f6c3ba63e0c1a459e2af))\n-\n-\n-\n-## [9.0.4](https://github.com/redis/go-redis/compare/v9.0.3...v9.0.4) (2023-05-01)\n-\n-\n-### Bug Fixes\n-\n-* reader float parser ([#2513](https://github.com/redis/go-redis/issues/2513)) ([46f2450](https://github.com/redis/go-redis/commit/46f245075e6e3a8bd8471f9ca67ea95fd675e241))\n-\n-\n-### Features\n-\n-* add client info command ([#2483](https://github.com/redis/go-redis/issues/2483)) ([b8c7317](https://github.com/redis/go-redis/commit/b8c7317cc6af444603731f7017c602347c0ba61e))\n-* no longer verify HELLO error messages ([#2515](https://github.com/redis/go-redis/issues/2515)) ([7b4f217](https://github.com/redis/go-redis/commit/7b4f2179cb5dba3d3c6b0c6f10db52b837c912c8))\n-* read the structure to increase the judgment of the omitempty op ([#2529](https://github.com/redis/go-redis/issues/2529)) ([37c057b](https://github.com/redis/go-redis/commit/37c057b8e597c5e8a0e372337f6a8ad27f6030af))\n-\n-\n-\n-## [9.0.3](https://github.com/redis/go-redis/compare/v9.0.2...v9.0.3) (2023-04-02)\n-\n-### New Features\n-\n-- feat(scan): scan time.Time sets the default decoding (#2413)\n-- Add support for CLUSTER LINKS command (#2504)\n-- Add support for acl dryrun command (#2502)\n-- Add support for COMMAND GETKEYS & COMMAND GETKEYSANDFLAGS (#2500)\n-- Add support for LCS Command (#2480)\n-- Add support for BZMPOP (#2456)\n-- Adding support for ZMPOP command (#2408)\n-- Add support for LMPOP (#2440)\n-- feat: remove pool unused fields (#2438)\n-- Expiretime and PExpireTime (#2426)\n-- Implement `FUNCTION` group of commands (#2475)\n-- feat(zadd): add ZAddLT and ZAddGT (#2429)\n-- Add: Support for COMMAND LIST command (#2491)\n-- Add support for BLMPOP (#2442)\n-- feat: check pipeline.Do to prevent confusion with Exec (#2517)\n-- Function stats, function kill, fcall and fcall_ro (#2486)\n-- feat: Add support for CLUSTER SHARDS command (#2507)\n-- feat(cmd): support for adding byte,bit parameters to the bitpos command (#2498)\n-\n-### Fixed\n-\n-- fix: eval api cmd.SetFirstKeyPos (#2501)\n-- fix: limit the number of connections created (#2441)\n-- fixed #2462  v9 continue support dragonfly,  it's Hello command return \"NOAUTH Authentication required\" error (#2479)\n-- Fix for internal/hscan/structmap.go:89:23: undefined: reflect.Pointer (#2458)\n-- fix: group lag can be null (#2448)\n-\n-### Maintenance\n-\n-- Updating to the latest version of redis (#2508)\n-- Allowing for running tests on a port other than the fixed 6380 (#2466)\n-- redis 7.0.8 in tests (#2450)\n-- docs: Update redisotel example for v9 (#2425)\n-- chore: update go mod, Upgrade golang.org/x/net version to 0.7.0 (#2476)\n-- chore: add Chinese translation (#2436)\n-- chore(deps): bump github.com/bsm/gomega from 1.20.0 to 1.26.0 (#2421)\n-- chore(deps): bump github.com/bsm/ginkgo/v2 from 2.5.0 to 2.7.0 (#2420)\n-- chore(deps): bump actions/setup-go from 3 to 4 (#2495)\n-- docs: add instructions for the HSet api (#2503)\n-- docs: add reading lag field comment (#2451)\n-- test: update go mod before testing(go mod tidy) (#2423)\n-- docs: fix comment typo (#2505)\n-- test: remove testify (#2463)\n-- refactor: change ListElementCmd to KeyValuesCmd. (#2443)\n-- fix(appendArg): appendArg case special type (#2489)\n-\n-## [9.0.2](https://github.com/redis/go-redis/compare/v9.0.1...v9.0.2) (2023-02-01)\n-\n-### Features\n-\n-* upgrade OpenTelemetry, use the new metrics API. ([#2410](https://github.com/redis/go-redis/issues/2410)) ([e29e42c](https://github.com/redis/go-redis/commit/e29e42cde2755ab910d04185025dc43ce6f59c65))\n-\n-## v9 2023-01-30\n-\n-### Breaking\n-\n-- Changed Pipelines to not be thread-safe any more.\n-\n-### Added\n-\n-- Added support for [RESP3](https://github.com/antirez/RESP3/blob/master/spec.md) protocol. It was\n-  contributed by @monkey92t who has done the majority of work in this release.\n-- Added `ContextTimeoutEnabled` option that controls whether the client respects context timeouts\n-  and deadlines. See\n-  [Redis Timeouts](https://redis.uptrace.dev/guide/go-redis-debugging.html#timeouts) for details.\n-- Added `ParseClusterURL` to parse URLs into `ClusterOptions`, for example,\n-  `redis://user:password@localhost:6789?dial_timeout=3&read_timeout=6s&addr=localhost:6790&addr=localhost:6791`.\n-- Added metrics instrumentation using `redisotel.IstrumentMetrics`. See\n-  [documentation](https://redis.uptrace.dev/guide/go-redis-monitoring.html)\n-- Added `redis.HasErrorPrefix` to help working with errors.\n-\n-### Changed\n-\n-- Removed asynchronous cancellation based on the context timeout. It was racy in v8 and is\n-  completely gone in v9.\n-- Reworked hook interface and added `DialHook`.\n-- Replaced `redisotel.NewTracingHook` with `redisotel.InstrumentTracing`. See\n-  [example](example/otel) and\n-  [documentation](https://redis.uptrace.dev/guide/go-redis-monitoring.html).\n-- Replaced `*redis.Z` with `redis.Z` since it is small enough to be passed as value without making\n-  an allocation.\n-- Renamed the option `MaxConnAge` to `ConnMaxLifetime`.\n-- Renamed the option `IdleTimeout` to `ConnMaxIdleTime`.\n-- Removed connection reaper in favor of `MaxIdleConns`.\n-- Removed `WithContext` since `context.Context` can be passed directly as an arg.\n-- Removed `Pipeline.Close` since there is no real need to explicitly manage pipeline resources and\n-  it can be safely reused via `sync.Pool` etc. `Pipeline.Discard` is still available if you want to\n-  reset commands for some reason.\n-\n-### Fixed\n-\n-- Improved and fixed pipeline retries.\n-- As usually, added support for more commands and fixed some bugs."
    },
    {
      "sha": "8c68c522e5f08c8738ac073805dcf0b09a753c57",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/CONTRIBUTING.md",
      "status": "modified",
      "additions": 22,
      "deletions": 5,
      "changes": 27,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FCONTRIBUTING.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FCONTRIBUTING.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FCONTRIBUTING.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -32,20 +32,33 @@ Here's how to get started with your code contribution:\n \n 1.  Create your own fork of go-redis\n 2.  Do the changes in your fork\n-3.  If you need a development environment, run `make test`. Note: this clones and builds the latest release of [redis](https://redis.io). You also need a redis-stack-server docker, in order to run the capabilities tests. This can be started by running:\n-    ```docker run -p 6379:6379 -it redis/redis-stack-server:edge```\n-4.  While developing, make sure the tests pass by running `make tests`\n+3.  If you need a development environment, run `make docker.start`.\n+ \n+> Note: this clones and builds the docker containers specified in `docker-compose.yml`, to understand more about\n+> the infrastructure that will be started you can check the `docker-compose.yml`. You also have the possiblity\n+> to specify the redis image that will be pulled with the env variable `CLIENT_LIBS_TEST_IMAGE`.\n+> By default the docker image that will be pulled and started is `redislabs/client-libs-test:8.2.1-pre`.\n+> If you want to test with newer Redis version, using a newer version of `redislabs/client-libs-test` should work out of the box.\n+\n+4.  While developing, make sure the tests pass by running `make test` (if you have the docker containers running, `make test.ci` may be sufficient).\n+> Note: `make test` will try to start all containers, run the tests with `make test.ci` and then stop all containers.\n 5.  If you like the change and think the project could use it, send a\n     pull request\n \n To see what else is part of the automation, run `invoke -l`\n \n+\n ## Testing\n \n-Call `make test` to run all tests, including linters.\n+### Setting up Docker\n+To run the tests, you need to have Docker installed and running. If you are using a host OS that does not support\n+docker host networks out of the box (e.g. Windows, OSX), you need to set up a docker desktop and enable docker host networks.\n+\n+### Running tests\n+Call `make test` to run all tests.\n \n Continuous Integration uses these same wrappers to run all of these\n-tests against multiple versions of python. Feel free to test your\n+tests against multiple versions of redis. Feel free to test your\n changes against all the go versions supported, as declared by the\n [build.yml](./.github/workflows/build.yml) file.\n \n@@ -99,3 +112,7 @@ The core team regularly looks at pull requests. We will provide\n feedback as soon as possible. After receiving our feedback, please respond\n within two weeks. After that time, we may close your PR if it isn't\n showing any activity.\n+\n+## Support\n+\n+Maintainers can provide limited support to contributors on discord: https://discord.gg/W4txy5AeKM"
    },
    {
      "sha": "c2264a4e397dfb773aa82038286f45121a9fea00",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/Makefile",
      "status": "modified",
      "additions": 60,
      "deletions": 23,
      "changes": 83,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FMakefile",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FMakefile",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FMakefile?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -1,42 +1,79 @@\n GO_MOD_DIRS := $(shell find . -type f -name 'go.mod' -exec dirname {} \\; | sort)\n+REDIS_VERSION ?= 8.4\n+RE_CLUSTER ?= false\n+RCE_DOCKER ?= true\n+CLIENT_LIBS_TEST_IMAGE ?= redislabs/client-libs-test:8.4.0\n \n-test: testdeps\n-\t$(eval GO_VERSION := $(shell go version | cut -d \" \" -f 3 | cut -d. -f2))\n+docker.start:\n+\texport RE_CLUSTER=$(RE_CLUSTER) && \\\n+\texport RCE_DOCKER=$(RCE_DOCKER) && \\\n+\texport REDIS_VERSION=$(REDIS_VERSION) && \\\n+\texport CLIENT_LIBS_TEST_IMAGE=$(CLIENT_LIBS_TEST_IMAGE) && \\\n+\tdocker compose --profile all up -d --quiet-pull\n+\n+docker.stop:\n+\tdocker compose --profile all down\n+\n+test:\n+\t$(MAKE) docker.start\n+\t@if [ -z \"$(REDIS_VERSION)\" ]; then \\\n+\t\techo \"REDIS_VERSION not set, running all tests\"; \\\n+\t\t$(MAKE) test.ci; \\\n+\telse \\\n+\t\tMAJOR_VERSION=$$(echo \"$(REDIS_VERSION)\" | cut -d. -f1); \\\n+\t\tif [ \"$$MAJOR_VERSION\" -ge 8 ]; then \\\n+\t\t\techo \"REDIS_VERSION $(REDIS_VERSION) >= 8, running all tests\"; \\\n+\t\t\t$(MAKE) test.ci; \\\n+\t\telse \\\n+\t\t\techo \"REDIS_VERSION $(REDIS_VERSION) < 8, skipping vector_sets tests\"; \\\n+\t\t\t$(MAKE) test.ci.skip-vectorsets; \\\n+\t\tfi; \\\n+\tfi\n+\t$(MAKE) docker.stop\n+\n+test.ci:\n \tset -e; for dir in $(GO_MOD_DIRS); do \\\n-\t  if echo \"$${dir}\" | grep -q \"./example\" && [ \"$(GO_VERSION)\" = \"19\" ]; then \\\n-\t    echo \"Skipping go test in $${dir} due to Go version 1.19 and dir contains ./example\"; \\\n-\t    continue; \\\n-\t  fi; \\\n \t  echo \"go test in $${dir}\"; \\\n \t  (cd \"$${dir}\" && \\\n+\t    export RE_CLUSTER=$(RE_CLUSTER) && \\\n+\t    export RCE_DOCKER=$(RCE_DOCKER) && \\\n+\t    export REDIS_VERSION=$(REDIS_VERSION) && \\\n \t    go mod tidy -compat=1.18 && \\\n-\t    go test && \\\n-\t    go test ./... -short -race && \\\n-\t    go test ./... -run=NONE -bench=. -benchmem && \\\n-\t    env GOOS=linux GOARCH=386 go test && \\\n-\t    go test -coverprofile=coverage.txt -covermode=atomic ./... && \\\n-\t    go vet); \\\n+\t    go vet && \\\n+\t    go test -v -coverprofile=coverage.txt -covermode=atomic ./... -race -skip Example); \\\n \tdone\n \tcd internal/customvet && go build .\n \tgo vet -vettool ./internal/customvet/customvet\n \n-testdeps: testdata/redis/src/redis-server\n+test.ci.skip-vectorsets:\n+\tset -e; for dir in $(GO_MOD_DIRS); do \\\n+\t  echo \"go test in $${dir} (skipping vector sets)\"; \\\n+\t  (cd \"$${dir}\" && \\\n+\t    export RE_CLUSTER=$(RE_CLUSTER) && \\\n+\t    export RCE_DOCKER=$(RCE_DOCKER) && \\\n+\t    export REDIS_VERSION=$(REDIS_VERSION) && \\\n+\t    go mod tidy -compat=1.18 && \\\n+\t    go vet && \\\n+\t    go test -v -coverprofile=coverage.txt -covermode=atomic ./... -race \\\n+\t      -run '^(?!.*(?:VectorSet|vectorset|ExampleClient_vectorset)).*$$' -skip Example); \\\n+\tdone\n+\tcd internal/customvet && go build .\n+\tgo vet -vettool ./internal/customvet/customvet\n \n-bench: testdeps\n-\tgo test ./... -test.run=NONE -test.bench=. -test.benchmem\n+bench:\n+\texport RE_CLUSTER=$(RE_CLUSTER) && \\\n+\texport RCE_DOCKER=$(RCE_DOCKER) && \\\n+\texport REDIS_VERSION=$(REDIS_VERSION) && \\\n+\tgo test ./... -test.run=NONE -test.bench=. -test.benchmem -skip Example\n \n-.PHONY: all test testdeps bench fmt\n+.PHONY: all test test.ci test.ci.skip-vectorsets bench fmt\n \n build:\n+\texport RE_CLUSTER=$(RE_CLUSTER) && \\\n+\texport RCE_DOCKER=$(RCE_DOCKER) && \\\n+\texport REDIS_VERSION=$(REDIS_VERSION) && \\\n \tgo build .\n \n-testdata/redis:\n-\tmkdir -p $@\n-\twget -qO- https://download.redis.io/releases/redis-7.4-rc2.tar.gz  | tar xvz --strip-components=1 -C $@\n-\n-testdata/redis/src/redis-server: testdata/redis\n-\tcd $< && make all\n-\n fmt:\n \tgofumpt -w ./\n \tgoimports -w  -local github.com/redis/go-redis ./"
    },
    {
      "sha": "38bd17b5835f11926b802c895b40f7d0a704fd64",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/README.md",
      "status": "modified",
      "additions": 352,
      "deletions": 48,
      "changes": 400,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FREADME.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FREADME.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FREADME.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -2,17 +2,32 @@\n \n [![build workflow](https://github.com/redis/go-redis/actions/workflows/build.yml/badge.svg)](https://github.com/redis/go-redis/actions)\n [![PkgGoDev](https://pkg.go.dev/badge/github.com/redis/go-redis/v9)](https://pkg.go.dev/github.com/redis/go-redis/v9?tab=doc)\n-[![Documentation](https://img.shields.io/badge/redis-documentation-informational)](https://redis.uptrace.dev/)\n+[![Documentation](https://img.shields.io/badge/redis-documentation-informational)](https://redis.io/docs/latest/develop/clients/go/)\n+[![Go Report Card](https://goreportcard.com/badge/github.com/redis/go-redis/v9)](https://goreportcard.com/report/github.com/redis/go-redis/v9)\n [![codecov](https://codecov.io/github/redis/go-redis/graph/badge.svg?token=tsrCZKuSSw)](https://codecov.io/github/redis/go-redis)\n-[![Chat](https://discordapp.com/api/guilds/752070105847955518/widget.png)](https://discord.gg/rWtp5Aj)\n \n-> go-redis is brought to you by :star: [**uptrace/uptrace**](https://github.com/uptrace/uptrace).\n-> Uptrace is an open-source APM tool that supports distributed tracing, metrics, and logs. You can\n-> use it to monitor applications and set up automatic alerts to receive notifications via email,\n-> Slack, Telegram, and others.\n->\n-> See [OpenTelemetry](https://github.com/redis/go-redis/tree/master/example/otel) example which\n-> demonstrates how you can use Uptrace to monitor go-redis.\n+[![Discord](https://img.shields.io/discord/697882427875393627.svg?style=social&logo=discord)](https://discord.gg/W4txy5AeKM)\n+[![Twitch](https://img.shields.io/twitch/status/redisinc?style=social)](https://www.twitch.tv/redisinc)\n+[![YouTube](https://img.shields.io/youtube/channel/views/UCD78lHSwYqMlyetR0_P4Vig?style=social)](https://www.youtube.com/redisinc)\n+[![Twitter](https://img.shields.io/twitter/follow/redisinc?style=social)](https://twitter.com/redisinc)\n+[![Stack Exchange questions](https://img.shields.io/stackexchange/stackoverflow/t/go-redis?style=social&logo=stackoverflow&label=Stackoverflow)](https://stackoverflow.com/questions/tagged/go-redis)\n+\n+> go-redis is the official Redis client library for the Go programming language. It offers a straightforward interface for interacting with Redis servers. \n+\n+## Supported versions\n+\n+In `go-redis` we are aiming to support the last three releases of Redis. Currently, this means we do support:\n+- [Redis 8.0](https://raw.githubusercontent.com/redis/redis/8.0/00-RELEASENOTES) - using Redis CE 8.0\n+- [Redis 8.2](https://raw.githubusercontent.com/redis/redis/8.2/00-RELEASENOTES) - using Redis CE 8.2 \n+- [Redis 8.4](https://raw.githubusercontent.com/redis/redis/8.4/00-RELEASENOTES) - using Redis CE 8.4\n+\n+Although the `go.mod` states it requires at minimum `go 1.18`, our CI is configured to run the tests against all three\n+versions of Redis and latest two versions of Go ([1.23](https://go.dev/doc/devel/release#go1.23.0),\n+[1.24](https://go.dev/doc/devel/release#go1.24.0)). We observe that some modules related test may not pass with\n+Redis Stack 7.2 and some commands are changed with Redis CE 8.0.\n+Although it is not officially supported, `go-redis/v9`  should be able to work with any Redis 7.0+.\n+Please do refer to the documentation and the tests if you experience any issues. We do plan to update the go version\n+in the `go.mod` to `go 1.24` in one of the next releases.\n \n ## How do I Redis?\n \n@@ -28,40 +43,39 @@\n \n [Work at Redis](https://redis.com/company/careers/jobs/)\n \n-## Documentation\n-\n-- [English](https://redis.uptrace.dev)\n-- [](https://redis.uptrace.dev/zh/)\n \n ## Resources\n \n - [Discussions](https://github.com/redis/go-redis/discussions)\n-- [Chat](https://discord.gg/rWtp5Aj)\n+- [Chat](https://discord.gg/W4txy5AeKM)\n - [Reference](https://pkg.go.dev/github.com/redis/go-redis/v9)\n - [Examples](https://pkg.go.dev/github.com/redis/go-redis/v9#pkg-examples)\n \n+## old documentation\n+\n+- [English](https://redis.uptrace.dev)\n+- [](https://redis.uptrace.dev/zh/)\n+\n ## Ecosystem\n \n-- [Redis Mock](https://github.com/go-redis/redismock)\n+- [Entra ID (Azure AD)](https://github.com/redis/go-redis-entraid)\n - [Distributed Locks](https://github.com/bsm/redislock)\n - [Redis Cache](https://github.com/go-redis/cache)\n - [Rate limiting](https://github.com/go-redis/redis_rate)\n \n-This client also works with [Kvrocks](https://github.com/apache/incubator-kvrocks), a distributed\n-key value NoSQL database that uses RocksDB as storage engine and is compatible with Redis protocol.\n-\n ## Features\n \n - Redis commands except QUIT and SYNC.\n - Automatic connection pooling.\n+- [StreamingCredentialsProvider (e.g. entra id, oauth)](#1-streaming-credentials-provider-highest-priority) (experimental)\n - [Pub/Sub](https://redis.uptrace.dev/guide/go-redis-pubsub.html).\n - [Pipelines and transactions](https://redis.uptrace.dev/guide/go-redis-pipelines.html).\n - [Scripting](https://redis.uptrace.dev/guide/lua-scripting.html).\n - [Redis Sentinel](https://redis.uptrace.dev/guide/go-redis-sentinel.html).\n - [Redis Cluster](https://redis.uptrace.dev/guide/go-redis-cluster.html).\n-- [Redis Ring](https://redis.uptrace.dev/guide/ring.html).\n - [Redis Performance Monitoring](https://redis.uptrace.dev/guide/redis-performance-monitoring.html).\n - [Redis Probabilistic [RedisStack]](https://redis.io/docs/data-types/probabilistic/)\n+- [Customizable read and write buffers size.](#custom-buffer-sizes)\n \n ## Installation\n \n@@ -122,17 +136,121 @@ func ExampleClient() {\n }\n ```\n \n-The above can be modified to specify the version of the RESP protocol by adding the `protocol`\n-option to the `Options` struct:\n+### Authentication\n+\n+The Redis client supports multiple ways to provide authentication credentials, with a clear priority order. Here are the available options:\n+\n+#### 1. Streaming Credentials Provider (Highest Priority) - Experimental feature\n+\n+The streaming credentials provider allows for dynamic credential updates during the connection lifetime. This is particularly useful for managed identity services and token-based authentication.\n \n ```go\n-    rdb := redis.NewClient(&redis.Options{\n-        Addr:     \"localhost:6379\",\n-        Password: \"\", // no password set\n-        DB:       0,  // use default DB\n-        Protocol: 3, // specify 2 for RESP 2 or 3 for RESP 3\n-    })\n+type StreamingCredentialsProvider interface {\n+    Subscribe(listener CredentialsListener) (Credentials, UnsubscribeFunc, error)\n+}\n+\n+type CredentialsListener interface {\n+    OnNext(credentials Credentials)  // Called when credentials are updated\n+    OnError(err error)              // Called when an error occurs\n+}\n+\n+type Credentials interface {\n+    BasicAuth() (username string, password string)\n+    RawCredentials() string\n+}\n+```\n+\n+Example usage:\n+```go\n+rdb := redis.NewClient(&redis.Options{\n+    Addr: \"localhost:6379\",\n+    StreamingCredentialsProvider: &MyCredentialsProvider{},\n+})\n+```\n+\n+**Note:** The streaming credentials provider can be used with [go-redis-entraid](https://github.com/redis/go-redis-entraid) to enable Entra ID (formerly Azure AD) authentication. This allows for seamless integration with Azure's managed identity services and token-based authentication.\n \n+Example with Entra ID:\n+```go\n+import (\n+    \"github.com/redis/go-redis/v9\"\n+    \"github.com/redis/go-redis-entraid\"\n+)\n+\n+// Create an Entra ID credentials provider\n+provider := entraid.NewDefaultAzureIdentityProvider()\n+\n+// Configure Redis client with Entra ID authentication\n+rdb := redis.NewClient(&redis.Options{\n+    Addr: \"your-redis-server.redis.cache.windows.net:6380\",\n+    StreamingCredentialsProvider: provider,\n+    TLSConfig: &tls.Config{\n+        MinVersion: tls.VersionTLS12,\n+    },\n+})\n+```\n+\n+#### 2. Context-based Credentials Provider\n+\n+The context-based provider allows credentials to be determined at the time of each operation, using the context.\n+\n+```go\n+rdb := redis.NewClient(&redis.Options{\n+    Addr: \"localhost:6379\",\n+    CredentialsProviderContext: func(ctx context.Context) (string, string, error) {\n+        // Return username, password, and any error\n+        return \"user\", \"pass\", nil\n+    },\n+})\n+```\n+\n+#### 3. Regular Credentials Provider\n+\n+A simple function-based provider that returns static credentials.\n+\n+```go\n+rdb := redis.NewClient(&redis.Options{\n+    Addr: \"localhost:6379\",\n+    CredentialsProvider: func() (string, string) {\n+        // Return username and password\n+        return \"user\", \"pass\"\n+    },\n+})\n+```\n+\n+#### 4. Username/Password Fields (Lowest Priority)\n+\n+The most basic way to provide credentials is through the `Username` and `Password` fields in the options.\n+\n+```go\n+rdb := redis.NewClient(&redis.Options{\n+    Addr:     \"localhost:6379\",\n+    Username: \"user\",\n+    Password: \"pass\",\n+})\n+```\n+\n+#### Priority Order\n+\n+The client will use credentials in the following priority order:\n+1. Streaming Credentials Provider (if set)\n+2. Context-based Credentials Provider (if set)\n+3. Regular Credentials Provider (if set)\n+4. Username/Password fields (if set)\n+\n+If none of these are set, the client will attempt to connect without authentication.\n+\n+### Protocol Version\n+\n+The client supports both RESP2 and RESP3 protocols. You can specify the protocol version in the options:\n+\n+```go\n+rdb := redis.NewClient(&redis.Options{\n+    Addr:     \"localhost:6379\",\n+    Password: \"\", // no password set\n+    DB:       0,  // use default DB\n+    Protocol: 3,  // specify 2 for RESP 2 or 3 for RESP 3\n+})\n ```\n \n ### Connecting via a redis url\n@@ -159,6 +277,36 @@ func ExampleClient() *redis.Client {\n \n ```\n \n+### Instrument with OpenTelemetry\n+\n+```go\n+import (\n+    \"github.com/redis/go-redis/v9\"\n+    \"github.com/redis/go-redis/extra/redisotel/v9\"\n+    \"errors\"\n+)\n+\n+func main() {\n+    ...\n+    rdb := redis.NewClient(&redis.Options{...})\n+\n+    if err := errors.Join(redisotel.InstrumentTracing(rdb), redisotel.InstrumentMetrics(rdb)); err != nil {\n+        log.Fatal(err)\n+    }\n+```\n+\n+\n+### Buffer Size Configuration\n+\n+go-redis uses 32KiB read and write buffers by default for optimal performance. For high-throughput applications or large pipelines, you can customize buffer sizes:\n+\n+```go\n+rdb := redis.NewClient(&redis.Options{\n+    Addr:            \"localhost:6379\",\n+    ReadBufferSize:  1024 * 1024, // 1MiB read buffer\n+    WriteBufferSize: 1024 * 1024, // 1MiB write buffer\n+})\n+```\n \n ### Advanced Configuration\n \n@@ -203,9 +351,45 @@ res1, err := client.FTSearchWithArgs(ctx, \"txt\", \"foo bar\", &redis.FTSearchOptio\n val1 := client.FTSearchWithArgs(ctx, \"txt\", \"foo bar\", &redis.FTSearchOptions{}).RawVal()\n ```\n \n-## Contributing\n+#### Redis-Search Default Dialect\n+\n+In the Redis-Search module, **the default dialect is 2**. If needed, you can explicitly specify a different dialect using the appropriate configuration in your queries.\n+\n+**Important**: Be aware that the query dialect may impact the results returned. If needed, you can revert to a different dialect version by passing the desired dialect in the arguments of the command you want to execute.\n+For example:\n+```\n+\tres2, err := rdb.FTSearchWithArgs(ctx,\n+\t\t\"idx:bicycle\",\n+\t\t\"@pickup_zone:[CONTAINS $bike]\",\n+\t\t&redis.FTSearchOptions{\n+\t\t\tParams: map[string]interface{}{\n+\t\t\t\t\"bike\": \"POINT(-0.1278 51.5074)\",\n+\t\t\t},\n+\t\t\tDialectVersion: 3,\n+\t\t},\n+\t).Result()\n+```\n+You can find further details in the [query dialect documentation](https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/dialects/).\n \n-Please see [out contributing guidelines](CONTRIBUTING.md) to help us improve this library!\n+#### Custom buffer sizes\n+Prior to v9.12, the buffer size was the default go value of 4096 bytes. Starting from v9.12, \n+go-redis uses 32KiB read and write buffers by default for optimal performance.\n+For high-throughput applications or large pipelines, you can customize buffer sizes:\n+\n+```go\n+rdb := redis.NewClient(&redis.Options{\n+    Addr:            \"localhost:6379\",\n+    ReadBufferSize:  1024 * 1024, // 1MiB read buffer\n+    WriteBufferSize: 1024 * 1024, // 1MiB write buffer\n+})\n+```\n+\n+**Important**: If you experience any issues with the default buffer sizes, please try setting them to the go default of 4096 bytes.\n+\n+## Contributing\n+We welcome contributions to the go-redis library! If you have a bug fix, feature request, or improvement, please open an issue or pull request on GitHub.\n+We appreciate your help in making go-redis better for everyone.\n+If you are interested in contributing to the go-redis library, please check out our [contributing guidelines](CONTRIBUTING.md) for more information on how to get started.\n \n ## Look and feel\n \n@@ -242,38 +426,150 @@ vals, err := rdb.Eval(ctx, \"return {KEYS[1],ARGV[1]}\", []string{\"key\"}, \"hello\")\n res, err := rdb.Do(ctx, \"set\", \"key\", \"value\").Result()\n ```\n \n-## Run the test\n-\n-go-redis will start a redis-server and run the test cases.\n+## Typed Errors\n \n-The paths of redis-server bin file and redis config file are defined in `main_test.go`:\n+go-redis provides typed error checking functions for common Redis errors:\n \n ```go\n-var (\n-\tredisServerBin, _  = filepath.Abs(filepath.Join(\"testdata\", \"redis\", \"src\", \"redis-server\"))\n-\tredisServerConf, _ = filepath.Abs(filepath.Join(\"testdata\", \"redis\", \"redis.conf\"))\n-)\n+// Cluster and replication errors\n+redis.IsLoadingError(err)        // Redis is loading the dataset\n+redis.IsReadOnlyError(err)       // Write to read-only replica\n+redis.IsClusterDownError(err)    // Cluster is down\n+redis.IsTryAgainError(err)       // Command should be retried\n+redis.IsMasterDownError(err)     // Master is down\n+redis.IsMovedError(err)          // Returns (address, true) if key moved\n+redis.IsAskError(err)            // Returns (address, true) if key being migrated\n+\n+// Connection and resource errors\n+redis.IsMaxClientsError(err)     // Maximum clients reached\n+redis.IsAuthError(err)           // Authentication failed (NOAUTH, WRONGPASS, unauthenticated)\n+redis.IsPermissionError(err)     // Permission denied (NOPERM)\n+redis.IsOOMError(err)            // Out of memory (OOM)\n+\n+// Transaction errors\n+redis.IsExecAbortError(err)      // Transaction aborted (EXECABORT)\n ```\n \n-For local testing, you can change the variables to refer to your local files, or create a soft link\n-to the corresponding folder for redis-server and copy the config file to `testdata/redis/`:\n+### Error Wrapping in Hooks\n \n-```shell\n-ln -s /usr/bin/redis-server ./go-redis/testdata/redis/src\n-cp ./go-redis/testdata/redis.conf ./go-redis/testdata/redis/\n+When wrapping errors in hooks, use custom error types with `Unwrap()` method (preferred) or `fmt.Errorf` with `%w`. Always call `cmd.SetErr()` to preserve error type information:\n+\n+```go\n+// Custom error type (preferred)\n+type AppError struct {\n+    Code      string\n+    RequestID string\n+    Err       error\n+}\n+\n+func (e *AppError) Error() string {\n+    return fmt.Sprintf(\"[%s] request_id=%s: %v\", e.Code, e.RequestID, e.Err)\n+}\n+\n+func (e *AppError) Unwrap() error {\n+    return e.Err\n+}\n+\n+// Hook implementation\n+func (h MyHook) ProcessHook(next redis.ProcessHook) redis.ProcessHook {\n+    return func(ctx context.Context, cmd redis.Cmder) error {\n+        err := next(ctx, cmd)\n+        if err != nil {\n+            // Wrap with custom error type\n+            wrappedErr := &AppError{\n+                Code:      \"REDIS_ERROR\",\n+                RequestID: getRequestID(ctx),\n+                Err:       err,\n+            }\n+            cmd.SetErr(wrappedErr)\n+            return wrappedErr  // Return wrapped error to preserve it\n+        }\n+        return nil\n+    }\n+}\n+\n+// Typed error detection works through wrappers\n+if redis.IsLoadingError(err) {\n+    // Retry logic\n+}\n+\n+// Extract custom error if needed\n+var appErr *AppError\n+if errors.As(err, &appErr) {\n+    log.Printf(\"Request: %s\", appErr.RequestID)\n+}\n ```\n \n-Lastly, run:\n+Alternatively, use `fmt.Errorf` with `%w`:\n+```go\n+wrappedErr := fmt.Errorf(\"context: %w\", err)\n+cmd.SetErr(wrappedErr)\n+```\n \n-```shell\n-go test\n+### Pipeline Hook Example\n+\n+For pipeline operations, use `ProcessPipelineHook`:\n+\n+```go\n+type PipelineLoggingHook struct{}\n+\n+func (h PipelineLoggingHook) DialHook(next redis.DialHook) redis.DialHook {\n+    return next\n+}\n+\n+func (h PipelineLoggingHook) ProcessHook(next redis.ProcessHook) redis.ProcessHook {\n+    return next\n+}\n+\n+func (h PipelineLoggingHook) ProcessPipelineHook(next redis.ProcessPipelineHook) redis.ProcessPipelineHook {\n+    return func(ctx context.Context, cmds []redis.Cmder) error {\n+        start := time.Now()\n+\n+        // Execute the pipeline\n+        err := next(ctx, cmds)\n+\n+        duration := time.Since(start)\n+        log.Printf(\"Pipeline executed %d commands in %v\", len(cmds), duration)\n+\n+        // Process individual command errors\n+        // Note: Individual command errors are already set on each cmd by the pipeline execution\n+        for _, cmd := range cmds {\n+            if cmdErr := cmd.Err(); cmdErr != nil {\n+                // Check for specific error types using typed error functions\n+                if redis.IsAuthError(cmdErr) {\n+                    log.Printf(\"Auth error in pipeline command %s: %v\", cmd.Name(), cmdErr)\n+                } else if redis.IsPermissionError(cmdErr) {\n+                    log.Printf(\"Permission error in pipeline command %s: %v\", cmd.Name(), cmdErr)\n+                }\n+\n+                // Optionally wrap individual command errors to add context\n+                // The wrapped error preserves type information through errors.As()\n+                wrappedErr := fmt.Errorf(\"pipeline cmd %s failed: %w\", cmd.Name(), cmdErr)\n+                cmd.SetErr(wrappedErr)\n+            }\n+        }\n+\n+        // Return the pipeline-level error (connection errors, etc.)\n+        // You can wrap it if needed, or return it as-is\n+        return err\n+    }\n+}\n+\n+// Register the hook\n+rdb.AddHook(PipelineLoggingHook{})\n+\n+// Use pipeline - errors are still properly typed\n+pipe := rdb.Pipeline()\n+pipe.Set(ctx, \"key1\", \"value1\", 0)\n+pipe.Get(ctx, \"key2\")\n+_, err := pipe.Exec(ctx)\n ```\n \n-Another option is to run your specific tests with an already running redis. The example below, tests\n-against a redis running on port 9999.:\n+## Run the test\n \n+Recommended to use Docker, just need to run:\n ```shell\n-REDIS_PORT=9999 go test <your options>\n+make test\n ```\n \n ## See also\n@@ -285,6 +581,14 @@ REDIS_PORT=9999 go test <your options>\n \n ## Contributors\n \n+> The go-redis project was originally initiated by :star: [**uptrace/uptrace**](https://github.com/uptrace/uptrace).\n+> Uptrace is an open-source APM tool that supports distributed tracing, metrics, and logs. You can\n+> use it to monitor applications and set up automatic alerts to receive notifications via email,\n+> Slack, Telegram, and others.\n+>\n+> See [OpenTelemetry](https://github.com/redis/go-redis/tree/master/example/otel) example which\n+> demonstrates how you can use Uptrace to monitor go-redis.\n+\n Thanks to all the people who already contributed!\n \n <a href=\"https://github.com/redis/go-redis/graphs/contributors\">"
    },
    {
      "sha": "e38ade4424db2f86700dc01c211abc2bdda639c3",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/RELEASE-NOTES.md",
      "status": "added",
      "additions": 705,
      "deletions": 0,
      "changes": 705,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FRELEASE-NOTES.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FRELEASE-NOTES.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2FRELEASE-NOTES.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,705 @@\n+# Release Notes\n+\n+# 9.17.2 (2025-12-01)\n+\n+##  Bug Fixes\n+\n+- **Connection Pool**: Fixed critical race condition in turn management that could cause connection leaks when dial goroutines complete after request timeout ([#3626](https://github.com/redis/go-redis/pull/3626)) by [@cyningsun](https://github.com/cyningsun)\n+- **Context Timeout**: Improved context timeout calculation to use minimum of remaining time and DialTimeout, preventing goroutines from waiting longer than necessary ([#3626](https://github.com/redis/go-redis/pull/3626)) by [@cyningsun](https://github.com/cyningsun)\n+\n+##  Maintenance\n+\n+- chore(deps): bump rojopolis/spellcheck-github-actions from 0.54.0 to 0.55.0 ([#3627](https://github.com/redis/go-redis/pull/3627))\n+\n+## Contributors\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@cyningsun](https://github.com/cyningsun) and [@ndyakov](https://github.com/ndyakov)\n+\n+---\n+\n+**Full Changelog**: https://github.com/redis/go-redis/compare/v9.17.1...v9.17.2\n+\n+# 9.17.1 (2025-11-25)\n+\n+##  Bug Fixes\n+\n+- add wait to keyless commands list ([#3615](https://github.com/redis/go-redis/pull/3615)) by [@marcoferrer](https://github.com/marcoferrer)\n+- fix(time): remove cached time optimization ([#3611](https://github.com/redis/go-redis/pull/3611)) by [@ndyakov](https://github.com/ndyakov)\n+\n+##  Maintenance\n+\n+- chore(deps): bump golangci/golangci-lint-action from 9.0.0 to 9.1.0 ([#3609](https://github.com/redis/go-redis/pull/3609))\n+- chore(deps): bump actions/checkout from 5 to 6 ([#3610](https://github.com/redis/go-redis/pull/3610))\n+- chore(script): fix help call in tag.sh ([#3606](https://github.com/redis/go-redis/pull/3606)) by [@ndyakov](https://github.com/ndyakov)\n+\n+## Contributors\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@marcoferrer](https://github.com/marcoferrer) and [@ndyakov](https://github.com/ndyakov)\n+\n+---\n+\n+**Full Changelog**: https://github.com/redis/go-redis/compare/v9.17.0...v9.17.1\n+\n+# 9.17.0 (2025-11-19)\n+\n+##  Highlights\n+\n+### Redis 8.4 Support\n+Added support for Redis 8.4, including new commands and features ([#3572](https://github.com/redis/go-redis/pull/3572))\n+\n+### Typed Errors\n+Introduced typed errors for better error handling using `errors.As` instead of string checks. Errors can now be wrapped and set to commands in hooks without breaking library functionality ([#3602](https://github.com/redis/go-redis/pull/3602))\n+\n+### New Commands\n+- **CAS/CAD Commands**: Added support for Compare-And-Set/Compare-And-Delete operations with conditional matching (`IFEQ`, `IFNE`, `IFDEQ`, `IFDNE`) ([#3583](https://github.com/redis/go-redis/pull/3583), [#3595](https://github.com/redis/go-redis/pull/3595))\n+- **MSETEX**: Atomically set multiple key-value pairs with expiration options and conditional modes ([#3580](https://github.com/redis/go-redis/pull/3580))\n+- **XReadGroup CLAIM**: Consume both incoming and idle pending entries from streams in a single call ([#3578](https://github.com/redis/go-redis/pull/3578))\n+- **ACL Commands**: Added `ACLGenPass`, `ACLUsers`, and `ACLWhoAmI` ([#3576](https://github.com/redis/go-redis/pull/3576))\n+- **SLOWLOG Commands**: Added `SLOWLOG LEN` and `SLOWLOG RESET` ([#3585](https://github.com/redis/go-redis/pull/3585))\n+- **LATENCY Commands**: Added `LATENCY LATEST` and `LATENCY RESET` ([#3584](https://github.com/redis/go-redis/pull/3584))\n+\n+### Search & Vector Improvements\n+- **Hybrid Search**: Added  **EXPERIMENTAL** support for the new `FT.HYBRID` command ([#3573](https://github.com/redis/go-redis/pull/3573))\n+- **Vector Range**: Added `VRANGE` command for vector sets ([#3543](https://github.com/redis/go-redis/pull/3543))\n+- **FT.INFO Enhancements**: Added vector-specific attributes in FT.INFO response ([#3596](https://github.com/redis/go-redis/pull/3596))\n+\n+### Connection Pool Improvements\n+- **Improved Connection Success Rate**: Implemented FIFO queue-based fairness and context pattern for connection creation to prevent premature cancellation under high concurrency ([#3518](https://github.com/redis/go-redis/pull/3518))\n+- **Connection State Machine**: Resolved race conditions and improved pool performance with proper state tracking ([#3559](https://github.com/redis/go-redis/pull/3559))\n+- **Pool Performance**: Significant performance improvements with faster semaphores, lockless hook manager, and reduced allocations (47-67% faster Get/Put operations) ([#3565](https://github.com/redis/go-redis/pull/3565))\n+\n+### Metrics & Observability\n+- **Canceled Metric Attribute**: Added 'canceled' metrics attribute to distinguish context cancellation errors from other errors ([#3566](https://github.com/redis/go-redis/pull/3566))\n+\n+##  New Features\n+\n+- Typed errors with wrapping support ([#3602](https://github.com/redis/go-redis/pull/3602)) by [@ndyakov](https://github.com/ndyakov)\n+- CAS/CAD commands (marked as experimental) ([#3583](https://github.com/redis/go-redis/pull/3583), [#3595](https://github.com/redis/go-redis/pull/3595)) by [@ndyakov](https://github.com/ndyakov), [@htemelski-redis](https://github.com/htemelski-redis)\n+- MSETEX command support ([#3580](https://github.com/redis/go-redis/pull/3580)) by [@ofekshenawa](https://github.com/ofekshenawa)\n+- XReadGroup CLAIM argument ([#3578](https://github.com/redis/go-redis/pull/3578)) by [@ofekshenawa](https://github.com/ofekshenawa)\n+- ACL commands: GenPass, Users, WhoAmI ([#3576](https://github.com/redis/go-redis/pull/3576)) by [@destinyoooo](https://github.com/destinyoooo)\n+- SLOWLOG commands: LEN, RESET ([#3585](https://github.com/redis/go-redis/pull/3585)) by [@destinyoooo](https://github.com/destinyoooo)\n+- LATENCY commands: LATEST, RESET ([#3584](https://github.com/redis/go-redis/pull/3584)) by [@destinyoooo](https://github.com/destinyoooo)\n+- Hybrid search command (FT.HYBRID) ([#3573](https://github.com/redis/go-redis/pull/3573)) by [@htemelski-redis](https://github.com/htemelski-redis)\n+- Vector range command (VRANGE) ([#3543](https://github.com/redis/go-redis/pull/3543)) by [@cxljs](https://github.com/cxljs)\n+- Vector-specific attributes in FT.INFO ([#3596](https://github.com/redis/go-redis/pull/3596)) by [@ndyakov](https://github.com/ndyakov)\n+- Improved connection pool success rate with FIFO queue ([#3518](https://github.com/redis/go-redis/pull/3518)) by [@cyningsun](https://github.com/cyningsun)\n+- Canceled metrics attribute for context errors ([#3566](https://github.com/redis/go-redis/pull/3566)) by [@pvragov](https://github.com/pvragov)\n+\n+##  Bug Fixes\n+\n+- Fixed Failover Client MaintNotificationsConfig ([#3600](https://github.com/redis/go-redis/pull/3600)) by [@ajax16384](https://github.com/ajax16384)\n+- Fixed ACLGenPass function to use the bit parameter ([#3597](https://github.com/redis/go-redis/pull/3597)) by [@destinyoooo](https://github.com/destinyoooo)\n+- Return error instead of panic from commands ([#3568](https://github.com/redis/go-redis/pull/3568)) by [@dragneelfps](https://github.com/dragneelfps)\n+- Safety harness in `joinErrors` to prevent panic ([#3577](https://github.com/redis/go-redis/pull/3577)) by [@manisharma](https://github.com/manisharma)\n+\n+##  Performance\n+\n+- Connection state machine with race condition fixes ([#3559](https://github.com/redis/go-redis/pull/3559)) by [@ndyakov](https://github.com/ndyakov)\n+- Pool performance improvements: 47-67% faster Get/Put, 33% less memory, 50% fewer allocations ([#3565](https://github.com/redis/go-redis/pull/3565)) by [@ndyakov](https://github.com/ndyakov)\n+\n+##  Testing & Infrastructure\n+\n+- Updated to Redis 8.4.0 image ([#3603](https://github.com/redis/go-redis/pull/3603)) by [@ndyakov](https://github.com/ndyakov)\n+- Added Redis 8.4-RC1-pre to CI ([#3572](https://github.com/redis/go-redis/pull/3572)) by [@ndyakov](https://github.com/ndyakov)\n+- Refactored tests for idiomatic Go ([#3561](https://github.com/redis/go-redis/pull/3561), [#3562](https://github.com/redis/go-redis/pull/3562), [#3563](https://github.com/redis/go-redis/pull/3563)) by [@12ya](https://github.com/12ya)\n+\n+##  Contributors\n+\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@12ya](https://github.com/12ya), [@ajax16384](https://github.com/ajax16384), [@cxljs](https://github.com/cxljs), [@cyningsun](https://github.com/cyningsun), [@destinyoooo](https://github.com/destinyoooo), [@dragneelfps](https://github.com/dragneelfps), [@htemelski-redis](https://github.com/htemelski-redis), [@manisharma](https://github.com/manisharma), [@ndyakov](https://github.com/ndyakov), [@ofekshenawa](https://github.com/ofekshenawa), [@pvragov](https://github.com/pvragov)\n+\n+---\n+\n+**Full Changelog**: https://github.com/redis/go-redis/compare/v9.16.0...v9.17.0\n+\n+# 9.16.0 (2025-10-23)\n+\n+##  Highlights\n+\n+### Maintenance Notifications Support\n+\n+This release introduces comprehensive support for Redis maintenance notifications, enabling applications to handle server maintenance events gracefully. The new `maintnotifications` package provides:\n+\n+- **RESP3 Push Notifications**: Full support for Redis RESP3 protocol push notifications\n+- **Connection Handoff**: Automatic connection migration during server maintenance with configurable retry policies and circuit breakers\n+- **Graceful Degradation**: Configurable timeout relaxation during maintenance windows to prevent false failures\n+- **Event-Driven Architecture**: Background workers with on-demand scaling for efficient handoff processing\n+- **Production-Ready**: Comprehensive E2E testing framework and monitoring capabilities\n+\n+For detailed usage examples and configuration options, see the [maintenance notifications documentation](maintnotifications/README.md).\n+\n+##  New Features\n+\n+- **Trace Filtering**: Add support for filtering traces for specific commands, including pipeline operations and dial operations ([#3519](https://github.com/redis/go-redis/pull/3519), [#3550](https://github.com/redis/go-redis/pull/3550))\n+  - New `TraceCmdFilter` option to selectively trace commands\n+  - Reduces overhead by excluding high-frequency or low-value commands from traces\n+\n+##  Bug Fixes\n+\n+- **Pipeline Error Handling**: Fix issue where pipeline repeatedly sets the same error ([#3525](https://github.com/redis/go-redis/pull/3525))\n+- **Connection Pool**: Ensure re-authentication does not interfere with connection handoff operations ([#3547](https://github.com/redis/go-redis/pull/3547))\n+\n+##  Improvements\n+\n+- **Hash Commands**: Update hash command implementations ([#3523](https://github.com/redis/go-redis/pull/3523))\n+- **OpenTelemetry**: Use `metric.WithAttributeSet` to avoid unnecessary attribute copying in redisotel ([#3552](https://github.com/redis/go-redis/pull/3552))\n+\n+##  Documentation\n+\n+- **Cluster Client**: Add explanation for why `MaxRetries` is disabled for `ClusterClient` ([#3551](https://github.com/redis/go-redis/pull/3551))\n+\n+##  Testing & Infrastructure\n+\n+- **E2E Testing**: Upgrade E2E testing framework with improved reliability and coverage ([#3541](https://github.com/redis/go-redis/pull/3541))\n+- **Release Process**: Improved resiliency of the release process ([#3530](https://github.com/redis/go-redis/pull/3530))\n+\n+##  Dependencies\n+\n+- Bump `rojopolis/spellcheck-github-actions` from 0.51.0 to 0.52.0 ([#3520](https://github.com/redis/go-redis/pull/3520))\n+- Bump `github/codeql-action` from 3 to 4 ([#3544](https://github.com/redis/go-redis/pull/3544))\n+\n+##  Contributors\n+\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@ndyakov](https://github.com/ndyakov), [@htemelski-redis](https://github.com/htemelski-redis), [@Sovietaced](https://github.com/Sovietaced), [@Udhayarajan](https://github.com/Udhayarajan), [@boekkooi-impossiblecloud](https://github.com/boekkooi-impossiblecloud), [@Pika-Gopher](https://github.com/Pika-Gopher), [@cxljs](https://github.com/cxljs), [@huiyifyj](https://github.com/huiyifyj), [@omid-h70](https://github.com/omid-h70)\n+\n+---\n+\n+**Full Changelog**: https://github.com/redis/go-redis/compare/v9.14.0...v9.16.0\n+\n+\n+# 9.15.0 was accidentally released. Please use version 9.16.0 instead.\n+\n+# 9.15.0-beta.3 (2025-09-26)\n+\n+## Highlights\n+This beta release includes a pre-production version of processing push notifications and hitless upgrades.\n+\n+# Changes\n+\n+- chore: Update hash_commands.go ([#3523](https://github.com/redis/go-redis/pull/3523))\n+\n+##  New Features\n+\n+- feat: RESP3 notifications support & Hitless notifications handling ([#3418](https://github.com/redis/go-redis/pull/3418))\n+\n+##  Bug Fixes\n+\n+- fix: pipeline repeatedly sets the error ([#3525](https://github.com/redis/go-redis/pull/3525))\n+\n+##  Maintenance\n+\n+- chore(deps): bump rojopolis/spellcheck-github-actions from 0.51.0 to 0.52.0 ([#3520](https://github.com/redis/go-redis/pull/3520))\n+- feat(e2e-testing): maintnotifications e2e and refactor ([#3526](https://github.com/redis/go-redis/pull/3526))\n+- feat(tag.sh): Improved resiliency of the release process ([#3530](https://github.com/redis/go-redis/pull/3530))\n+\n+## Contributors\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@cxljs](https://github.com/cxljs), [@ndyakov](https://github.com/ndyakov), [@htemelski-redis](https://github.com/htemelski-redis), and [@omid-h70](https://github.com/omid-h70)\n+\n+\n+# 9.15.0-beta.1 (2025-09-10)\n+\n+## Highlights\n+This beta release includes a pre-production version of processing push notifications and hitless upgrades.\n+\n+### Hitless Upgrades\n+Hitless upgrades is a major new feature that allows for zero-downtime upgrades in Redis clusters.\n+You can find more information in the [Hitless Upgrades documentation](https://github.com/redis/go-redis/tree/master/hitless).\n+\n+# Changes\n+\n+##  New Features\n+- [CAE-1088] & [CAE-1072] feat: RESP3 notifications support & Hitless notifications handling ([#3418](https://github.com/redis/go-redis/pull/3418))\n+\n+## Contributors\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@ndyakov](https://github.com/ndyakov), [@htemelski-redis](https://github.com/htemelski-redis), [@ofekshenawa](https://github.com/ofekshenawa)\n+\n+\n+# 9.14.0 (2025-09-10)\n+\n+## Highlights\n+- Added batch process method to the pipeline ([#3510](https://github.com/redis/go-redis/pull/3510))\n+\n+# Changes\n+\n+##  New Features\n+\n+- Added batch process method to the pipeline ([#3510](https://github.com/redis/go-redis/pull/3510))\n+\n+##  Bug Fixes\n+\n+- fix: SetErr on Cmd if the command cannot be queued correctly in multi/exec ([#3509](https://github.com/redis/go-redis/pull/3509))\n+\n+##  Maintenance\n+\n+- Updates release drafter config to exclude dependabot ([#3511](https://github.com/redis/go-redis/pull/3511))\n+- chore(deps): bump actions/setup-go from 5 to 6 ([#3504](https://github.com/redis/go-redis/pull/3504))\n+\n+## Contributors\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@elena-kolevska](https://github.com/elena-kolevksa), [@htemelski-redis](https://github.com/htemelski-redis) and [@ndyakov](https://github.com/ndyakov)\n+\n+\n+# 9.13.0 (2025-09-03)\n+\n+## Highlights\n+- Pipeliner expose queued commands ([#3496](https://github.com/redis/go-redis/pull/3496))\n+- Ensure that JSON.GET returns Nil response ([#3470](https://github.com/redis/go-redis/pull/3470))\n+- Fixes on Read and Write buffer sizes and UniversalOptions\n+\n+## Changes\n+- Pipeliner expose queued commands ([#3496](https://github.com/redis/go-redis/pull/3496))\n+- fix(test): fix a timing issue in pubsub test ([#3498](https://github.com/redis/go-redis/pull/3498))\n+- Allow users to enable read-write splitting in failover mode. ([#3482](https://github.com/redis/go-redis/pull/3482))\n+- Set the read/write buffer size of the sentinel client to 4KiB ([#3476](https://github.com/redis/go-redis/pull/3476))\n+\n+##  New Features\n+\n+- fix(otel): register wait metrics ([#3499](https://github.com/redis/go-redis/pull/3499))\n+- Support subscriptions against cluster slave nodes ([#3480](https://github.com/redis/go-redis/pull/3480))\n+- Add wait metrics to otel ([#3493](https://github.com/redis/go-redis/pull/3493))\n+- Clean failing timeout implementation ([#3472](https://github.com/redis/go-redis/pull/3472))\n+\n+##  Bug Fixes\n+\n+- Do not assume that all non-IP hosts are loopbacks ([#3085](https://github.com/redis/go-redis/pull/3085))\n+- Ensure that JSON.GET returns Nil response ([#3470](https://github.com/redis/go-redis/pull/3470))\n+\n+##  Maintenance\n+\n+- fix(otel): register wait metrics ([#3499](https://github.com/redis/go-redis/pull/3499))\n+- fix(make test): Add default env in makefile ([#3491](https://github.com/redis/go-redis/pull/3491))\n+- Update the introduction to running tests in README.md ([#3495](https://github.com/redis/go-redis/pull/3495))\n+- test: Add comprehensive edge case tests for IncrByFloat command ([#3477](https://github.com/redis/go-redis/pull/3477))\n+- Set the default read/write buffer size of Redis connection to 32KiB ([#3483](https://github.com/redis/go-redis/pull/3483))\n+- Bumps test image to 8.2.1-pre ([#3478](https://github.com/redis/go-redis/pull/3478))\n+- fix UniversalOptions miss ReadBufferSize and WriteBufferSize options ([#3485](https://github.com/redis/go-redis/pull/3485))\n+- chore(deps): bump actions/checkout from 4 to 5 ([#3484](https://github.com/redis/go-redis/pull/3484))\n+- Removes dry run for stale issues policy ([#3471](https://github.com/redis/go-redis/pull/3471))\n+- Update otel metrics URL ([#3474](https://github.com/redis/go-redis/pull/3474))\n+\n+## Contributors\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@LINKIWI](https://github.com/LINKIWI), [@cxljs](https://github.com/cxljs), [@cybersmeashish](https://github.com/cybersmeashish), [@elena-kolevska](https://github.com/elena-kolevska), [@htemelski-redis](https://github.com/htemelski-redis), [@mwhooker](https://github.com/mwhooker), [@ndyakov](https://github.com/ndyakov), [@ofekshenawa](https://github.com/ofekshenawa), [@suever](https://github.com/suever)\n+\n+\n+# 9.12.1 (2025-08-11)\n+##  Highlights\n+In the last version (9.12.0) the client introduced bigger write and read buffer sized. The default value we set was 512KiB.\n+However, users reported that this is too big for most use cases and can lead to high memory usage.\n+In this version the default value is changed to 256KiB. The `README.md` was updated to reflect the\n+correct default value and include a note that the default value can be changed.\n+\n+##  Bug Fixes\n+\n+- fix(options): Add buffer sizes to failover. Update README ([#3468](https://github.com/redis/go-redis/pull/3468))\n+\n+##  Maintenance\n+\n+- fix(options): Add buffer sizes to failover. Update README ([#3468](https://github.com/redis/go-redis/pull/3468))\n+- chore: update & fix otel example ([#3466](https://github.com/redis/go-redis/pull/3466))\n+\n+## Contributors\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@ndyakov](https://github.com/ndyakov) and [@vmihailenco](https://github.com/vmihailenco)\n+\n+# 9.12.0 (2025-08-05)\n+\n+##  Highlights\n+\n+- This release includes support for [Redis 8.2](https://redis.io/docs/latest/operate/oss_and_stack/stack-with-enterprise/release-notes/redisce/redisos-8.2-release-notes/).\n+- Introduces an experimental Query Builders for `FTSearch`, `FTAggregate` and other search commands.\n+- Adds support for `EPSILON` option in `FT.VSIM`.\n+- Includes bug fixes and improvements contributed by the community related to ring and [redisotel](https://github.com/redis/go-redis/tree/master/extra/redisotel).\n+\n+## Changes\n+- Improve stale issue workflow ([#3458](https://github.com/redis/go-redis/pull/3458))\n+- chore(ci): Add 8.2 rc2 pre build for CI ([#3459](https://github.com/redis/go-redis/pull/3459))\n+- Added new stream commands ([#3450](https://github.com/redis/go-redis/pull/3450))\n+- feat: Add \"skip_verify\" to Sentinel ([#3428](https://github.com/redis/go-redis/pull/3428))\n+- fix: `errors.Join` requires Go 1.20 or later ([#3442](https://github.com/redis/go-redis/pull/3442))\n+- DOC-4344 document quickstart examples ([#3426](https://github.com/redis/go-redis/pull/3426))\n+- feat(bitop): add support for the new bitop operations ([#3409](https://github.com/redis/go-redis/pull/3409))\n+\n+##  New Features\n+\n+- feat: recover addIdleConn may occur panic ([#2445](https://github.com/redis/go-redis/pull/2445))\n+- feat(ring): specify custom health check func via HeartbeatFn option ([#2940](https://github.com/redis/go-redis/pull/2940))\n+- Add Query Builder for RediSearch commands ([#3436](https://github.com/redis/go-redis/pull/3436))\n+- add configurable buffer sizes for Redis connections ([#3453](https://github.com/redis/go-redis/pull/3453))\n+- Add VAMANA vector type to RediSearch ([#3449](https://github.com/redis/go-redis/pull/3449))\n+- VSIM add `EPSILON` option ([#3454](https://github.com/redis/go-redis/pull/3454))\n+- Add closing support to otel metrics instrumentation ([#3444](https://github.com/redis/go-redis/pull/3444))\n+\n+##  Bug Fixes\n+\n+- fix(redisotel): fix buggy append in reportPoolStats ([#3122](https://github.com/redis/go-redis/pull/3122))\n+- fix(search): return results even if doc is empty ([#3457](https://github.com/redis/go-redis/pull/3457))\n+- [ISSUE-3402]: Ring.Pipelined return dial timeout error ([#3403](https://github.com/redis/go-redis/pull/3403))\n+\n+##  Maintenance\n+\n+- Merges stale issues jobs into one job with two steps ([#3463](https://github.com/redis/go-redis/pull/3463))\n+- improve code readability ([#3446](https://github.com/redis/go-redis/pull/3446))\n+- chore(release): 9.12.0-beta.1 ([#3460](https://github.com/redis/go-redis/pull/3460))\n+- DOC-5472 time series doc examples ([#3443](https://github.com/redis/go-redis/pull/3443))\n+- Add VAMANA compression algorithm tests ([#3461](https://github.com/redis/go-redis/pull/3461))\n+- bumped redis 8.2 version used in the CI/CD ([#3451](https://github.com/redis/go-redis/pull/3451))\n+\n+## Contributors\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@andy-stark-redis](https://github.com/andy-stark-redis), [@cxljs](https://github.com/cxljs), [@elena-kolevska](https://github.com/elena-kolevska), [@htemelski-redis](https://github.com/htemelski-redis), [@jouir](https://github.com/jouir), [@monkey92t](https://github.com/monkey92t), [@ndyakov](https://github.com/ndyakov), [@ofekshenawa](https://github.com/ofekshenawa), [@rokn](https://github.com/rokn), [@smnvdev](https://github.com/smnvdev), [@strobil](https://github.com/strobil) and [@wzy9607](https://github.com/wzy9607)\n+\n+## New Contributors\n+* [@htemelski-redis](https://github.com/htemelski-redis) made their first contribution in [#3409](https://github.com/redis/go-redis/pull/3409)\n+* [@smnvdev](https://github.com/smnvdev) made their first contribution in [#3403](https://github.com/redis/go-redis/pull/3403)\n+* [@rokn](https://github.com/rokn) made their first contribution in [#3444](https://github.com/redis/go-redis/pull/3444)\n+\n+# 9.11.0 (2025-06-24)\n+\n+##  Highlights\n+\n+Fixes TxPipeline to work correctly in cluster scenarios, allowing execution of commands\n+only in the same slot.\n+\n+# Changes\n+\n+##  New Features\n+\n+- Set cluster slot for `scan` commands, rather than random ([#2623](https://github.com/redis/go-redis/pull/2623))\n+- Add CredentialsProvider field to UniversalOptions ([#2927](https://github.com/redis/go-redis/pull/2927))\n+- feat(redisotel): add WithCallerEnabled option ([#3415](https://github.com/redis/go-redis/pull/3415))\n+\n+##  Bug Fixes\n+\n+- fix(txpipeline): keyless commands should take the slot of the keyed ([#3411](https://github.com/redis/go-redis/pull/3411))\n+- fix(loading): cache the loaded flag for slave nodes ([#3410](https://github.com/redis/go-redis/pull/3410))\n+- fix(txpipeline): should return error on multi/exec on multiple slots ([#3408](https://github.com/redis/go-redis/pull/3408))\n+- fix: check if the shard exists to avoid returning nil ([#3396](https://github.com/redis/go-redis/pull/3396))\n+\n+##  Maintenance\n+\n+- feat: optimize connection pool waitTurn ([#3412](https://github.com/redis/go-redis/pull/3412))\n+- chore(ci): update CI redis builds ([#3407](https://github.com/redis/go-redis/pull/3407))\n+- chore: remove a redundant method from `Ring`, `Client` and `ClusterClient` ([#3401](https://github.com/redis/go-redis/pull/3401))\n+- test: refactor TestBasicCredentials using table-driven tests ([#3406](https://github.com/redis/go-redis/pull/3406))\n+- perf: reduce unnecessary memory allocation operations ([#3399](https://github.com/redis/go-redis/pull/3399))\n+- fix: insert entry during iterating over a map ([#3398](https://github.com/redis/go-redis/pull/3398))\n+- DOC-5229 probabilistic data type examples ([#3413](https://github.com/redis/go-redis/pull/3413))\n+- chore(deps): bump rojopolis/spellcheck-github-actions from 0.49.0 to 0.51.0 ([#3414](https://github.com/redis/go-redis/pull/3414))\n+\n+## Contributors\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@andy-stark-redis](https://github.com/andy-stark-redis), [@boekkooi-impossiblecloud](https://github.com/boekkooi-impossiblecloud), [@cxljs](https://github.com/cxljs), [@dcherubini](https://github.com/dcherubini), [@dependabot[bot]](https://github.com/apps/dependabot), [@iamamirsalehi](https://github.com/iamamirsalehi), [@ndyakov](https://github.com/ndyakov), [@pete-woods](https://github.com/pete-woods), [@twz915](https://github.com/twz915) and [dependabot[bot]](https://github.com/apps/dependabot)\n+\n+# 9.10.0 (2025-06-06)\n+\n+##  Highlights\n+\n+`go-redis` now supports [vector sets](https://redis.io/docs/latest/develop/data-types/vector-sets/). This data type is marked\n+as \"in preview\" in Redis and its support in `go-redis` is marked as experimental. You can find examples in the documentation and\n+in the `doctests` folder.\n+\n+# Changes\n+\n+##  New Features\n+\n+- feat: support vectorset ([#3375](https://github.com/redis/go-redis/pull/3375))\n+\n+##  Maintenance\n+\n+- Add the missing NewFloatSliceResult for testing ([#3393](https://github.com/redis/go-redis/pull/3393))\n+- DOC-5078 vector set examples ([#3394](https://github.com/redis/go-redis/pull/3394))\n+\n+## Contributors\n+We'd like to thank all the contributors who worked on this release!\n+\n+[@AndBobsYourUncle](https://github.com/AndBobsYourUncle), [@andy-stark-redis](https://github.com/andy-stark-redis), [@fukua95](https://github.com/fukua95) and [@ndyakov](https://github.com/ndyakov)\n+\n+\n+\n+# 9.9.0 (2025-05-27)\n+\n+##  Highlights\n+- **Token-based Authentication**: Added `StreamingCredentialsProvider` for dynamic credential updates (experimental)\n+  - Can be used with [go-redis-entraid](https://github.com/redis/go-redis-entraid) for Azure AD authentication\n+- **Connection Statistics**: Added connection waiting statistics for better monitoring\n+- **Failover Improvements**: Added `ParseFailoverURL` for easier failover configuration\n+- **Ring Client Enhancements**: Added shard access methods for better Pub/Sub management\n+\n+##  New Features\n+- Added `StreamingCredentialsProvider` for token-based authentication ([#3320](https://github.com/redis/go-redis/pull/3320))\n+  - Supports dynamic credential updates\n+  - Includes connection close hooks\n+  - Note: Currently marked as experimental\n+- Added `ParseFailoverURL` for parsing failover URLs ([#3362](https://github.com/redis/go-redis/pull/3362))\n+- Added connection waiting statistics ([#2804](https://github.com/redis/go-redis/pull/2804))\n+- Added new utility functions:\n+  - `ParseFloat` and `MustParseFloat` in public utils package ([#3371](https://github.com/redis/go-redis/pull/3371))\n+  - Unit tests for `Atoi`, `ParseInt`, `ParseUint`, and `ParseFloat` ([#3377](https://github.com/redis/go-redis/pull/3377))\n+- Added Ring client shard access methods:\n+  - `GetShardClients()` to retrieve all active shard clients\n+  - `GetShardClientForKey(key string)` to get the shard client for a specific key ([#3388](https://github.com/redis/go-redis/pull/3388))\n+\n+##  Bug Fixes\n+- Fixed routing reads to loading slave nodes ([#3370](https://github.com/redis/go-redis/pull/3370))\n+- Added support for nil lag in XINFO GROUPS ([#3369](https://github.com/redis/go-redis/pull/3369))\n+- Fixed pool acquisition timeout issues ([#3381](https://github.com/redis/go-redis/pull/3381))\n+- Optimized unnecessary copy operations ([#3376](https://github.com/redis/go-redis/pull/3376))\n+\n+##  Documentation\n+- Updated documentation for XINFO GROUPS with nil lag support ([#3369](https://github.com/redis/go-redis/pull/3369))\n+- Added package-level comments for new features\n+\n+##  Performance and Reliability\n+- Optimized `ReplaceSpaces` function ([#3383](https://github.com/redis/go-redis/pull/3383))\n+- Set default value for `Options.Protocol` in `init()` ([#3387](https://github.com/redis/go-redis/pull/3387))\n+- Exported pool errors for public consumption ([#3380](https://github.com/redis/go-redis/pull/3380))\n+\n+##  Dependencies and Infrastructure\n+- Updated Redis CI to version 8.0.1 ([#3372](https://github.com/redis/go-redis/pull/3372))\n+- Updated spellcheck GitHub Actions ([#3389](https://github.com/redis/go-redis/pull/3389))\n+- Removed unused parameters ([#3382](https://github.com/redis/go-redis/pull/3382), [#3384](https://github.com/redis/go-redis/pull/3384))\n+\n+##  Testing\n+- Added unit tests for pool acquisition timeout ([#3381](https://github.com/redis/go-redis/pull/3381))\n+- Added unit tests for utility functions ([#3377](https://github.com/redis/go-redis/pull/3377))\n+\n+##  Contributors\n+\n+We would like to thank all the contributors who made this release possible:\n+\n+[@ndyakov](https://github.com/ndyakov), [@ofekshenawa](https://github.com/ofekshenawa), [@LINKIWI](https://github.com/LINKIWI), [@iamamirsalehi](https://github.com/iamamirsalehi), [@fukua95](https://github.com/fukua95), [@lzakharov](https://github.com/lzakharov), [@DengY11](https://github.com/DengY11)\n+\n+##  Changelog\n+\n+For a complete list of changes, see the [full changelog](https://github.com/redis/go-redis/compare/v9.8.0...v9.9.0).\n+\n+# 9.8.0 (2025-04-30)\n+\n+##  Highlights\n+- **Redis 8 Support**: Full compatibility with Redis 8.0, including testing and CI integration\n+- **Enhanced Hash Operations**: Added support for new hash commands (`HGETDEL`, `HGETEX`, `HSETEX`) and `HSTRLEN` command\n+- **Search Improvements**: Enabled Search DIALECT 2 by default and added `CountOnly` argument for `FT.Search`\n+\n+##  New Features\n+- Added support for new hash commands: `HGETDEL`, `HGETEX`, `HSETEX` ([#3305](https://github.com/redis/go-redis/pull/3305))\n+- Added `HSTRLEN` command for hash operations ([#2843](https://github.com/redis/go-redis/pull/2843))\n+- Added `Do` method for raw query by single connection from `pool.Conn()` ([#3182](https://github.com/redis/go-redis/pull/3182))\n+- Prevent false-positive marshaling by treating zero time.Time as empty in isEmptyValue ([#3273](https://github.com/redis/go-redis/pull/3273))\n+- Added FailoverClusterClient support for Universal client ([#2794](https://github.com/redis/go-redis/pull/2794))\n+- Added support for cluster mode with `IsClusterMode` config parameter ([#3255](https://github.com/redis/go-redis/pull/3255))\n+- Added client name support in `HELLO` RESP handshake ([#3294](https://github.com/redis/go-redis/pull/3294))\n+- **Enabled Search DIALECT 2 by default** ([#3213](https://github.com/redis/go-redis/pull/3213))\n+- Added read-only option for failover configurations ([#3281](https://github.com/redis/go-redis/pull/3281))\n+- Added `CountOnly` argument for `FT.Search` to use `LIMIT 0 0` ([#3338](https://github.com/redis/go-redis/pull/3338))\n+- Added `DB` option support in `NewFailoverClusterClient` ([#3342](https://github.com/redis/go-redis/pull/3342))\n+- Added `nil` check for the options when creating a client ([#3363](https://github.com/redis/go-redis/pull/3363))\n+\n+##  Bug Fixes\n+- Fixed `PubSub` concurrency safety issues ([#3360](https://github.com/redis/go-redis/pull/3360))\n+- Fixed panic caused when argument is `nil` ([#3353](https://github.com/redis/go-redis/pull/3353))\n+- Improved error handling when fetching master node from sentinels ([#3349](https://github.com/redis/go-redis/pull/3349))\n+- Fixed connection pool timeout issues and increased retries ([#3298](https://github.com/redis/go-redis/pull/3298))\n+- Fixed context cancellation error leading to connection spikes on Primary instances ([#3190](https://github.com/redis/go-redis/pull/3190))\n+- Fixed RedisCluster client to consider `MASTERDOWN` a retriable error ([#3164](https://github.com/redis/go-redis/pull/3164))\n+- Fixed tracing to show complete commands instead of truncated versions ([#3290](https://github.com/redis/go-redis/pull/3290))\n+- Fixed OpenTelemetry instrumentation to prevent multiple span reporting ([#3168](https://github.com/redis/go-redis/pull/3168))\n+- Fixed `FT.Search` Limit argument and added `CountOnly` argument for limit 0 0 ([#3338](https://github.com/redis/go-redis/pull/3338))\n+- Fixed missing command in interface ([#3344](https://github.com/redis/go-redis/pull/3344))\n+- Fixed slot calculation for `COUNTKEYSINSLOT` command ([#3327](https://github.com/redis/go-redis/pull/3327))\n+- Updated PubSub implementation with correct context ([#3329](https://github.com/redis/go-redis/pull/3329))\n+\n+##  Documentation\n+- Added hash search examples ([#3357](https://github.com/redis/go-redis/pull/3357))\n+- Fixed documentation comments ([#3351](https://github.com/redis/go-redis/pull/3351))\n+- Added `CountOnly` search example ([#3345](https://github.com/redis/go-redis/pull/3345))\n+- Added examples for list commands: `LLEN`, `LPOP`, `LPUSH`, `LRANGE`, `RPOP`, `RPUSH` ([#3234](https://github.com/redis/go-redis/pull/3234))\n+- Added `SADD` and `SMEMBERS` command examples ([#3242](https://github.com/redis/go-redis/pull/3242))\n+- Updated `README.md` to use Redis Discord guild ([#3331](https://github.com/redis/go-redis/pull/3331))\n+- Updated `HExpire` command documentation ([#3355](https://github.com/redis/go-redis/pull/3355))\n+- Featured OpenTelemetry instrumentation more prominently ([#3316](https://github.com/redis/go-redis/pull/3316))\n+- Updated `README.md` with additional information ([#310ce55](https://github.com/redis/go-redis/commit/310ce55))\n+\n+##  Performance and Reliability\n+- Bound connection pool background dials to configured dial timeout ([#3089](https://github.com/redis/go-redis/pull/3089))\n+- Ensured context isn't exhausted via concurrent query ([#3334](https://github.com/redis/go-redis/pull/3334))\n+\n+##  Dependencies and Infrastructure\n+- Updated testing image to Redis 8.0-RC2 ([#3361](https://github.com/redis/go-redis/pull/3361))\n+- Enabled CI for Redis CE 8.0 ([#3274](https://github.com/redis/go-redis/pull/3274))\n+- Updated various dependencies:\n+  - Bumped golangci/golangci-lint-action from 6.5.0 to 7.0.0 ([#3354](https://github.com/redis/go-redis/pull/3354))\n+  - Bumped rojopolis/spellcheck-github-actions ([#3336](https://github.com/redis/go-redis/pull/3336))\n+  - Bumped golang.org/x/net in example/otel ([#3308](https://github.com/redis/go-redis/pull/3308))\n+- Migrated golangci-lint configuration to v2 format ([#3354](https://github.com/redis/go-redis/pull/3354))\n+\n+##  Breaking Changes\n+- **Enabled Search DIALECT 2 by default** ([#3213](https://github.com/redis/go-redis/pull/3213))\n+- Dropped RedisGears (Triggers and Functions) support ([#3321](https://github.com/redis/go-redis/pull/3321))\n+- Dropped FT.PROFILE command that was never enabled ([#3323](https://github.com/redis/go-redis/pull/3323))\n+\n+##  Security\n+- Fixed network error handling on SETINFO (CVE-2025-29923) ([#3295](https://github.com/redis/go-redis/pull/3295))\n+\n+##  Testing\n+- Added integration tests for Redis 8 behavior changes in Redis Search ([#3337](https://github.com/redis/go-redis/pull/3337))\n+- Added vector types INT8 and UINT8 tests ([#3299](https://github.com/redis/go-redis/pull/3299))\n+- Added test codes for search_commands.go ([#3285](https://github.com/redis/go-redis/pull/3285))\n+- Fixed example test sorting ([#3292](https://github.com/redis/go-redis/pull/3292))\n+\n+##  Contributors\n+\n+We would like to thank all the contributors who made this release possible:\n+\n+[@alexander-menshchikov](https://github.com/alexander-menshchikov), [@EXPEbdodla](https://github.com/EXPEbdodla), [@afti](https://github.com/afti), [@dmaier-redislabs](https://github.com/dmaier-redislabs), [@four_leaf_clover](https://github.com/four_leaf_clover), [@alohaglenn](https://github.com/alohaglenn), [@gh73962](https://github.com/gh73962), [@justinmir](https://github.com/justinmir), [@LINKIWI](https://github.com/LINKIWI), [@liushuangbill](https://github.com/liushuangbill), [@golang88](https://github.com/golang88), [@gnpaone](https://github.com/gnpaone), [@ndyakov](https://github.com/ndyakov), [@nikolaydubina](https://github.com/nikolaydubina), [@oleglacto](https://github.com/oleglacto), [@andy-stark-redis](https://github.com/andy-stark-redis), [@rodneyosodo](https://github.com/rodneyosodo), [@dependabot](https://github.com/dependabot), [@rfyiamcool](https://github.com/rfyiamcool), [@frankxjkuang](https://github.com/frankxjkuang), [@fukua95](https://github.com/fukua95), [@soleymani-milad](https://github.com/soleymani-milad), [@ofekshenawa](https://github.com/ofekshenawa), [@khasanovbi](https://github.com/khasanovbi)\n+\n+\n+# Old Changelog\n+## Unreleased\n+\n+### Changed\n+\n+* `go-redis` won't skip span creation if the parent spans is not recording. ([#2980](https://github.com/redis/go-redis/issues/2980))\n+  Users can use the OpenTelemetry sampler to control the sampling behavior.\n+  For instance, you can use the `ParentBased(NeverSample())` sampler from `go.opentelemetry.io/otel/sdk/trace` to keep\n+  a similar behavior (drop orphan spans) of `go-redis` as before.\n+\n+## [9.0.5](https://github.com/redis/go-redis/compare/v9.0.4...v9.0.5) (2023-05-29)\n+\n+\n+### Features\n+\n+* Add ACL LOG ([#2536](https://github.com/redis/go-redis/issues/2536)) ([31ba855](https://github.com/redis/go-redis/commit/31ba855ddebc38fbcc69a75d9d4fb769417cf602))\n+* add field protocol to setupClusterQueryParams ([#2600](https://github.com/redis/go-redis/issues/2600)) ([840c25c](https://github.com/redis/go-redis/commit/840c25cb6f320501886a82a5e75f47b491e46fbe))\n+* add protocol option ([#2598](https://github.com/redis/go-redis/issues/2598)) ([3917988](https://github.com/redis/go-redis/commit/391798880cfb915c4660f6c3ba63e0c1a459e2af))\n+\n+\n+\n+## [9.0.4](https://github.com/redis/go-redis/compare/v9.0.3...v9.0.4) (2023-05-01)\n+\n+\n+### Bug Fixes\n+\n+* reader float parser ([#2513](https://github.com/redis/go-redis/issues/2513)) ([46f2450](https://github.com/redis/go-redis/commit/46f245075e6e3a8bd8471f9ca67ea95fd675e241))\n+\n+\n+### Features\n+\n+* add client info command ([#2483](https://github.com/redis/go-redis/issues/2483)) ([b8c7317](https://github.com/redis/go-redis/commit/b8c7317cc6af444603731f7017c602347c0ba61e))\n+* no longer verify HELLO error messages ([#2515](https://github.com/redis/go-redis/issues/2515)) ([7b4f217](https://github.com/redis/go-redis/commit/7b4f2179cb5dba3d3c6b0c6f10db52b837c912c8))\n+* read the structure to increase the judgment of the omitempty op ([#2529](https://github.com/redis/go-redis/issues/2529)) ([37c057b](https://github.com/redis/go-redis/commit/37c057b8e597c5e8a0e372337f6a8ad27f6030af))\n+\n+\n+\n+## [9.0.3](https://github.com/redis/go-redis/compare/v9.0.2...v9.0.3) (2023-04-02)\n+\n+### New Features\n+\n+- feat(scan): scan time.Time sets the default decoding (#2413)\n+- Add support for CLUSTER LINKS command (#2504)\n+- Add support for acl dryrun command (#2502)\n+- Add support for COMMAND GETKEYS & COMMAND GETKEYSANDFLAGS (#2500)\n+- Add support for LCS Command (#2480)\n+- Add support for BZMPOP (#2456)\n+- Adding support for ZMPOP command (#2408)\n+- Add support for LMPOP (#2440)\n+- feat: remove pool unused fields (#2438)\n+- Expiretime and PExpireTime (#2426)\n+- Implement `FUNCTION` group of commands (#2475)\n+- feat(zadd): add ZAddLT and ZAddGT (#2429)\n+- Add: Support for COMMAND LIST command (#2491)\n+- Add support for BLMPOP (#2442)\n+- feat: check pipeline.Do to prevent confusion with Exec (#2517)\n+- Function stats, function kill, fcall and fcall_ro (#2486)\n+- feat: Add support for CLUSTER SHARDS command (#2507)\n+- feat(cmd): support for adding byte,bit parameters to the bitpos command (#2498)\n+\n+### Fixed\n+\n+- fix: eval api cmd.SetFirstKeyPos (#2501)\n+- fix: limit the number of connections created (#2441)\n+- fixed #2462  v9 continue support dragonfly,  it's Hello command return \"NOAUTH Authentication required\" error (#2479)\n+- Fix for internal/hscan/structmap.go:89:23: undefined: reflect.Pointer (#2458)\n+- fix: group lag can be null (#2448)\n+\n+### Maintenance\n+\n+- Updating to the latest version of redis (#2508)\n+- Allowing for running tests on a port other than the fixed 6380 (#2466)\n+- redis 7.0.8 in tests (#2450)\n+- docs: Update redisotel example for v9 (#2425)\n+- chore: update go mod, Upgrade golang.org/x/net version to 0.7.0 (#2476)\n+- chore: add Chinese translation (#2436)\n+- chore(deps): bump github.com/bsm/gomega from 1.20.0 to 1.26.0 (#2421)\n+- chore(deps): bump github.com/bsm/ginkgo/v2 from 2.5.0 to 2.7.0 (#2420)\n+- chore(deps): bump actions/setup-go from 3 to 4 (#2495)\n+- docs: add instructions for the HSet api (#2503)\n+- docs: add reading lag field comment (#2451)\n+- test: update go mod before testing(go mod tidy) (#2423)\n+- docs: fix comment typo (#2505)\n+- test: remove testify (#2463)\n+- refactor: change ListElementCmd to KeyValuesCmd. (#2443)\n+- fix(appendArg): appendArg case special type (#2489)\n+\n+## [9.0.2](https://github.com/redis/go-redis/compare/v9.0.1...v9.0.2) (2023-02-01)\n+\n+### Features\n+\n+* upgrade OpenTelemetry, use the new metrics API. ([#2410](https://github.com/redis/go-redis/issues/2410)) ([e29e42c](https://github.com/redis/go-redis/commit/e29e42cde2755ab910d04185025dc43ce6f59c65))\n+\n+## v9 2023-01-30\n+\n+### Breaking\n+\n+- Changed Pipelines to not be thread-safe any more.\n+\n+### Added\n+\n+- Added support for [RESP3](https://github.com/antirez/RESP3/blob/master/spec.md) protocol. It was\n+  contributed by @monkey92t who has done the majority of work in this release.\n+- Added `ContextTimeoutEnabled` option that controls whether the client respects context timeouts\n+  and deadlines. See\n+  [Redis Timeouts](https://redis.uptrace.dev/guide/go-redis-debugging.html#timeouts) for details.\n+- Added `ParseClusterURL` to parse URLs into `ClusterOptions`, for example,\n+  `redis://user:password@localhost:6789?dial_timeout=3&read_timeout=6s&addr=localhost:6790&addr=localhost:6791`.\n+- Added metrics instrumentation using `redisotel.IstrumentMetrics`. See\n+  [documentation](https://redis.uptrace.dev/guide/go-redis-monitoring.html)\n+- Added `redis.HasErrorPrefix` to help working with errors.\n+\n+### Changed\n+\n+- Removed asynchronous cancellation based on the context timeout. It was racy in v8 and is\n+  completely gone in v9.\n+- Reworked hook interface and added `DialHook`.\n+- Replaced `redisotel.NewTracingHook` with `redisotel.InstrumentTracing`. See\n+  [example](example/otel) and\n+  [documentation](https://redis.uptrace.dev/guide/go-redis-monitoring.html).\n+- Replaced `*redis.Z` with `redis.Z` since it is small enough to be passed as value without making\n+  an allocation.\n+- Renamed the option `MaxConnAge` to `ConnMaxLifetime`.\n+- Renamed the option `IdleTimeout` to `ConnMaxIdleTime`.\n+- Removed connection reaper in favor of `MaxIdleConns`.\n+- Removed `WithContext` since `context.Context` can be passed directly as an arg.\n+- Removed `Pipeline.Close` since there is no real need to explicitly manage pipeline resources and\n+  it can be safely reused via `sync.Pool` etc. `Pipeline.Discard` is still available if you want to\n+  reset commands for some reason.\n+\n+### Fixed\n+\n+- Improved and fixed pipeline retries.\n+- As usually, added support for more commands and fixed some bugs."
    },
    {
      "sha": "0a8a195ceb2850a0388280deb49d5add3bb75cf1",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/acl_commands.go",
      "status": "modified",
      "additions": 81,
      "deletions": 0,
      "changes": 81,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Facl_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Facl_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Facl_commands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -4,8 +4,24 @@ import \"context\"\n \n type ACLCmdable interface {\n \tACLDryRun(ctx context.Context, username string, command ...interface{}) *StringCmd\n+\n \tACLLog(ctx context.Context, count int64) *ACLLogCmd\n \tACLLogReset(ctx context.Context) *StatusCmd\n+\n+\tACLGenPass(ctx context.Context, bit int) *StringCmd\n+\n+\tACLSetUser(ctx context.Context, username string, rules ...string) *StatusCmd\n+\tACLDelUser(ctx context.Context, username string) *IntCmd\n+\tACLUsers(ctx context.Context) *StringSliceCmd\n+\tACLWhoAmI(ctx context.Context) *StringCmd\n+\tACLList(ctx context.Context) *StringSliceCmd\n+\n+\tACLCat(ctx context.Context) *StringSliceCmd\n+\tACLCatArgs(ctx context.Context, options *ACLCatArgs) *StringSliceCmd\n+}\n+\n+type ACLCatArgs struct {\n+\tCategory string\n }\n \n func (c cmdable) ACLDryRun(ctx context.Context, username string, command ...interface{}) *StringCmd {\n@@ -33,3 +49,68 @@ func (c cmdable) ACLLogReset(ctx context.Context) *StatusCmd {\n \t_ = c(ctx, cmd)\n \treturn cmd\n }\n+\n+func (c cmdable) ACLDelUser(ctx context.Context, username string) *IntCmd {\n+\tcmd := NewIntCmd(ctx, \"acl\", \"deluser\", username)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) ACLSetUser(ctx context.Context, username string, rules ...string) *StatusCmd {\n+\targs := make([]interface{}, 3+len(rules))\n+\targs[0] = \"acl\"\n+\targs[1] = \"setuser\"\n+\targs[2] = username\n+\tfor i, rule := range rules {\n+\t\targs[i+3] = rule\n+\t}\n+\tcmd := NewStatusCmd(ctx, args...)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) ACLGenPass(ctx context.Context, bit int) *StringCmd {\n+\targs := make([]interface{}, 0, 3)\n+\targs = append(args, \"acl\", \"genpass\")\n+\tif bit > 0 {\n+\t\targs = append(args, bit)\n+\t}\n+\tcmd := NewStringCmd(ctx, args...)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) ACLUsers(ctx context.Context) *StringSliceCmd {\n+\tcmd := NewStringSliceCmd(ctx, \"acl\", \"users\")\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) ACLWhoAmI(ctx context.Context) *StringCmd {\n+\tcmd := NewStringCmd(ctx, \"acl\", \"whoami\")\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) ACLList(ctx context.Context) *StringSliceCmd {\n+\tcmd := NewStringSliceCmd(ctx, \"acl\", \"list\")\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) ACLCat(ctx context.Context) *StringSliceCmd {\n+\tcmd := NewStringSliceCmd(ctx, \"acl\", \"cat\")\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) ACLCatArgs(ctx context.Context, options *ACLCatArgs) *StringSliceCmd {\n+\t// if there is a category passed, build new cmd, if there isn't - use the ACLCat method\n+\tif options != nil && options.Category != \"\" {\n+\t\tcmd := NewStringSliceCmd(ctx, \"acl\", \"cat\", options.Category)\n+\t\t_ = c(ctx, cmd)\n+\t\treturn cmd\n+\t}\n+\n+\treturn c.ACLCat(ctx)\n+}"
    },
    {
      "sha": "4146153bf3ba4be3959468125ac2b7ee1c8086a9",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/adapters.go",
      "status": "added",
      "additions": 111,
      "deletions": 0,
      "changes": 111,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fadapters.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fadapters.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fadapters.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,111 @@\n+package redis\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"net\"\n+\t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/internal/interfaces\"\n+\t\"github.com/redis/go-redis/v9/push\"\n+)\n+\n+// ErrInvalidCommand is returned when an invalid command is passed to ExecuteCommand.\n+var ErrInvalidCommand = errors.New(\"invalid command type\")\n+\n+// ErrInvalidPool is returned when the pool type is not supported.\n+var ErrInvalidPool = errors.New(\"invalid pool type\")\n+\n+// newClientAdapter creates a new client adapter for regular Redis clients.\n+func newClientAdapter(client *baseClient) interfaces.ClientInterface {\n+\treturn &clientAdapter{client: client}\n+}\n+\n+// clientAdapter adapts a Redis client to implement interfaces.ClientInterface.\n+type clientAdapter struct {\n+\tclient *baseClient\n+}\n+\n+// GetOptions returns the client options.\n+func (ca *clientAdapter) GetOptions() interfaces.OptionsInterface {\n+\treturn &optionsAdapter{options: ca.client.opt}\n+}\n+\n+// GetPushProcessor returns the client's push notification processor.\n+func (ca *clientAdapter) GetPushProcessor() interfaces.NotificationProcessor {\n+\treturn &pushProcessorAdapter{processor: ca.client.pushProcessor}\n+}\n+\n+// optionsAdapter adapts Redis options to implement interfaces.OptionsInterface.\n+type optionsAdapter struct {\n+\toptions *Options\n+}\n+\n+// GetReadTimeout returns the read timeout.\n+func (oa *optionsAdapter) GetReadTimeout() time.Duration {\n+\treturn oa.options.ReadTimeout\n+}\n+\n+// GetWriteTimeout returns the write timeout.\n+func (oa *optionsAdapter) GetWriteTimeout() time.Duration {\n+\treturn oa.options.WriteTimeout\n+}\n+\n+// GetNetwork returns the network type.\n+func (oa *optionsAdapter) GetNetwork() string {\n+\treturn oa.options.Network\n+}\n+\n+// GetAddr returns the connection address.\n+func (oa *optionsAdapter) GetAddr() string {\n+\treturn oa.options.Addr\n+}\n+\n+// IsTLSEnabled returns true if TLS is enabled.\n+func (oa *optionsAdapter) IsTLSEnabled() bool {\n+\treturn oa.options.TLSConfig != nil\n+}\n+\n+// GetProtocol returns the protocol version.\n+func (oa *optionsAdapter) GetProtocol() int {\n+\treturn oa.options.Protocol\n+}\n+\n+// GetPoolSize returns the connection pool size.\n+func (oa *optionsAdapter) GetPoolSize() int {\n+\treturn oa.options.PoolSize\n+}\n+\n+// NewDialer returns a new dialer function for the connection.\n+func (oa *optionsAdapter) NewDialer() func(context.Context) (net.Conn, error) {\n+\tbaseDialer := oa.options.NewDialer()\n+\treturn func(ctx context.Context) (net.Conn, error) {\n+\t\t// Extract network and address from the options\n+\t\tnetwork := oa.options.Network\n+\t\taddr := oa.options.Addr\n+\t\treturn baseDialer(ctx, network, addr)\n+\t}\n+}\n+\n+// pushProcessorAdapter adapts a push.NotificationProcessor to implement interfaces.NotificationProcessor.\n+type pushProcessorAdapter struct {\n+\tprocessor push.NotificationProcessor\n+}\n+\n+// RegisterHandler registers a handler for a specific push notification name.\n+func (ppa *pushProcessorAdapter) RegisterHandler(pushNotificationName string, handler interface{}, protected bool) error {\n+\tif pushHandler, ok := handler.(push.NotificationHandler); ok {\n+\t\treturn ppa.processor.RegisterHandler(pushNotificationName, pushHandler, protected)\n+\t}\n+\treturn errors.New(\"handler must implement push.NotificationHandler\")\n+}\n+\n+// UnregisterHandler removes a handler for a specific push notification name.\n+func (ppa *pushProcessorAdapter) UnregisterHandler(pushNotificationName string) error {\n+\treturn ppa.processor.UnregisterHandler(pushNotificationName)\n+}\n+\n+// GetHandler returns the handler for a specific push notification name.\n+func (ppa *pushProcessorAdapter) GetHandler(pushNotificationName string) interface{} {\n+\treturn ppa.processor.GetHandler(pushNotificationName)\n+}"
    },
    {
      "sha": "1f5c80224858e48d465ba3a8a90ec1774d8563ef",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/auth/auth.go",
      "status": "added",
      "additions": 61,
      "deletions": 0,
      "changes": 61,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fauth%2Fauth.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fauth%2Fauth.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fauth%2Fauth.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,61 @@\n+// Package auth package provides authentication-related interfaces and types.\n+// It also includes a basic implementation of credentials using username and password.\n+package auth\n+\n+// StreamingCredentialsProvider is an interface that defines the methods for a streaming credentials provider.\n+// It is used to provide credentials for authentication.\n+// The CredentialsListener is used to receive updates when the credentials change.\n+type StreamingCredentialsProvider interface {\n+\t// Subscribe subscribes to the credentials provider for updates.\n+\t// It returns the current credentials, a cancel function to unsubscribe from the provider,\n+\t// and an error if any.\n+\t// TODO(ndyakov): Should we add context to the Subscribe method?\n+\tSubscribe(listener CredentialsListener) (Credentials, UnsubscribeFunc, error)\n+}\n+\n+// UnsubscribeFunc is a function that is used to cancel the subscription to the credentials provider.\n+// It is used to unsubscribe from the provider when the credentials are no longer needed.\n+type UnsubscribeFunc func() error\n+\n+// CredentialsListener is an interface that defines the methods for a credentials listener.\n+// It is used to receive updates when the credentials change.\n+// The OnNext method is called when the credentials change.\n+// The OnError method is called when an error occurs while requesting the credentials.\n+type CredentialsListener interface {\n+\tOnNext(credentials Credentials)\n+\tOnError(err error)\n+}\n+\n+// Credentials is an interface that defines the methods for credentials.\n+// It is used to provide the credentials for authentication.\n+type Credentials interface {\n+\t// BasicAuth returns the username and password for basic authentication.\n+\tBasicAuth() (username string, password string)\n+\t// RawCredentials returns the raw credentials as a string.\n+\t// This can be used to extract the username and password from the raw credentials or\n+\t// additional information if present in the token.\n+\tRawCredentials() string\n+}\n+\n+type basicAuth struct {\n+\tusername string\n+\tpassword string\n+}\n+\n+// RawCredentials returns the raw credentials as a string.\n+func (b *basicAuth) RawCredentials() string {\n+\treturn b.username + \":\" + b.password\n+}\n+\n+// BasicAuth returns the username and password for basic authentication.\n+func (b *basicAuth) BasicAuth() (username string, password string) {\n+\treturn b.username, b.password\n+}\n+\n+// NewBasicCredentials creates a new Credentials object from the given username and password.\n+func NewBasicCredentials(username, password string) Credentials {\n+\treturn &basicAuth{\n+\t\tusername: username,\n+\t\tpassword: password,\n+\t}\n+}"
    },
    {
      "sha": "f4b3198389e04436039780527686a2f276e251b7",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/auth/reauth_credentials_listener.go",
      "status": "added",
      "additions": 47,
      "deletions": 0,
      "changes": 47,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fauth%2Freauth_credentials_listener.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fauth%2Freauth_credentials_listener.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fauth%2Freauth_credentials_listener.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,47 @@\n+package auth\n+\n+// ReAuthCredentialsListener is a struct that implements the CredentialsListener interface.\n+// It is used to re-authenticate the credentials when they are updated.\n+// It contains:\n+// - reAuth: a function that takes the new credentials and returns an error if any.\n+// - onErr: a function that takes an error and handles it.\n+type ReAuthCredentialsListener struct {\n+\treAuth func(credentials Credentials) error\n+\tonErr  func(err error)\n+}\n+\n+// OnNext is called when the credentials are updated.\n+// It calls the reAuth function with the new credentials.\n+// If the reAuth function returns an error, it calls the onErr function with the error.\n+func (c *ReAuthCredentialsListener) OnNext(credentials Credentials) {\n+\tif c.reAuth == nil {\n+\t\treturn\n+\t}\n+\n+\terr := c.reAuth(credentials)\n+\tif err != nil {\n+\t\tc.OnError(err)\n+\t}\n+}\n+\n+// OnError is called when an error occurs.\n+// It can be called from both the credentials provider and the reAuth function.\n+func (c *ReAuthCredentialsListener) OnError(err error) {\n+\tif c.onErr == nil {\n+\t\treturn\n+\t}\n+\n+\tc.onErr(err)\n+}\n+\n+// NewReAuthCredentialsListener creates a new ReAuthCredentialsListener.\n+// Implements the auth.CredentialsListener interface.\n+func NewReAuthCredentialsListener(reAuth func(credentials Credentials) error, onErr func(err error)) *ReAuthCredentialsListener {\n+\treturn &ReAuthCredentialsListener{\n+\t\treAuth: reAuth,\n+\t\tonErr:  onErr,\n+\t}\n+}\n+\n+// Ensure ReAuthCredentialsListener implements the CredentialsListener interface.\n+var _ CredentialsListener = (*ReAuthCredentialsListener)(nil)\n\\ No newline at end of file"
    },
    {
      "sha": "86aa9b7efc698b72021270a42936144339c065d6",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/bitmap_commands.go",
      "status": "modified",
      "additions": 38,
      "deletions": 2,
      "changes": 40,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fbitmap_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fbitmap_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fbitmap_commands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -12,6 +12,10 @@ type BitMapCmdable interface {\n \tBitOpAnd(ctx context.Context, destKey string, keys ...string) *IntCmd\n \tBitOpOr(ctx context.Context, destKey string, keys ...string) *IntCmd\n \tBitOpXor(ctx context.Context, destKey string, keys ...string) *IntCmd\n+\tBitOpDiff(ctx context.Context, destKey string, keys ...string) *IntCmd\n+\tBitOpDiff1(ctx context.Context, destKey string, keys ...string) *IntCmd\n+\tBitOpAndOr(ctx context.Context, destKey string, keys ...string) *IntCmd\n+\tBitOpOne(ctx context.Context, destKey string, keys ...string) *IntCmd\n \tBitOpNot(ctx context.Context, destKey string, key string) *IntCmd\n \tBitPos(ctx context.Context, key string, bit int64, pos ...int64) *IntCmd\n \tBitPosSpan(ctx context.Context, key string, bit int8, start, end int64, span string) *IntCmd\n@@ -78,22 +82,50 @@ func (c cmdable) bitOp(ctx context.Context, op, destKey string, keys ...string)\n \treturn cmd\n }\n \n+// BitOpAnd creates a new bitmap in which users are members of all given bitmaps\n func (c cmdable) BitOpAnd(ctx context.Context, destKey string, keys ...string) *IntCmd {\n \treturn c.bitOp(ctx, \"and\", destKey, keys...)\n }\n \n+// BitOpOr creates a new bitmap in which users are member of at least one given bitmap\n func (c cmdable) BitOpOr(ctx context.Context, destKey string, keys ...string) *IntCmd {\n \treturn c.bitOp(ctx, \"or\", destKey, keys...)\n }\n \n+// BitOpXor creates a new bitmap in which users are the result of XORing all given bitmaps\n func (c cmdable) BitOpXor(ctx context.Context, destKey string, keys ...string) *IntCmd {\n \treturn c.bitOp(ctx, \"xor\", destKey, keys...)\n }\n \n+// BitOpNot creates a new bitmap in which users are not members of a given bitmap\n func (c cmdable) BitOpNot(ctx context.Context, destKey string, key string) *IntCmd {\n \treturn c.bitOp(ctx, \"not\", destKey, key)\n }\n \n+// BitOpDiff creates a new bitmap in which users are members of bitmap X but not of any of bitmaps Y1, Y2, \n+// Introduced with Redis 8.2\n+func (c cmdable) BitOpDiff(ctx context.Context, destKey string, keys ...string) *IntCmd {\n+\treturn c.bitOp(ctx, \"diff\", destKey, keys...)\n+}\n+\n+// BitOpDiff1 creates a new bitmap in which users are members of one or more of bitmaps Y1, Y2,  but not members of bitmap X\n+// Introduced with Redis 8.2\n+func (c cmdable) BitOpDiff1(ctx context.Context, destKey string, keys ...string) *IntCmd {\n+\treturn c.bitOp(ctx, \"diff1\", destKey, keys...)\n+}\n+\n+// BitOpAndOr creates a new bitmap in which users are members of bitmap X and also members of one or more of bitmaps Y1, Y2, \n+// Introduced with Redis 8.2\n+func (c cmdable) BitOpAndOr(ctx context.Context, destKey string, keys ...string) *IntCmd {\n+\treturn c.bitOp(ctx, \"andor\", destKey, keys...)\n+}\n+\n+// BitOpOne creates a new bitmap in which users are members of exactly one of the given bitmaps\n+// Introduced with Redis 8.2\n+func (c cmdable) BitOpOne(ctx context.Context, destKey string, keys ...string) *IntCmd {\n+\treturn c.bitOp(ctx, \"one\", destKey, keys...)\n+}\n+\n // BitPos is an API before Redis version 7.0, cmd: bitpos key bit start end\n // if you need the `byte | bit` parameter, please use `BitPosSpan`.\n func (c cmdable) BitPos(ctx context.Context, key string, bit int64, pos ...int64) *IntCmd {\n@@ -109,7 +141,9 @@ func (c cmdable) BitPos(ctx context.Context, key string, bit int64, pos ...int64\n \t\targs[3] = pos[0]\n \t\targs[4] = pos[1]\n \tdefault:\n-\t\tpanic(\"too many arguments\")\n+\t\tcmd := NewIntCmd(ctx)\n+\t\tcmd.SetErr(errors.New(\"too many arguments\"))\n+\t\treturn cmd\n \t}\n \tcmd := NewIntCmd(ctx, args...)\n \t_ = c(ctx, cmd)\n@@ -150,7 +184,9 @@ func (c cmdable) BitFieldRO(ctx context.Context, key string, values ...interface\n \targs[0] = \"BITFIELD_RO\"\n \targs[1] = key\n \tif len(values)%2 != 0 {\n-\t\tpanic(\"BitFieldRO: invalid number of arguments, must be even\")\n+\t\tc := NewIntSliceCmd(ctx)\n+\t\tc.SetErr(errors.New(\"BitFieldRO: invalid number of arguments, must be even\"))\n+\t\treturn c\n \t}\n \tfor i := 0; i < len(values); i += 2 {\n \t\targs = append(args, \"GET\", values[i], values[i+1])"
    },
    {
      "sha": "4857b01eaa5f4e1eeb215d9cbfd1015563e09bc9",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/cluster_commands.go",
      "status": "modified",
      "additions": 7,
      "deletions": 0,
      "changes": 7,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fcluster_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fcluster_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fcluster_commands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -4,6 +4,7 @@ import \"context\"\n \n type ClusterCmdable interface {\n \tClusterMyShardID(ctx context.Context) *StringCmd\n+\tClusterMyID(ctx context.Context) *StringCmd\n \tClusterSlots(ctx context.Context) *ClusterSlotsCmd\n \tClusterShards(ctx context.Context) *ClusterShardsCmd\n \tClusterLinks(ctx context.Context) *ClusterLinksCmd\n@@ -35,6 +36,12 @@ func (c cmdable) ClusterMyShardID(ctx context.Context) *StringCmd {\n \treturn cmd\n }\n \n+func (c cmdable) ClusterMyID(ctx context.Context) *StringCmd {\n+\tcmd := NewStringCmd(ctx, \"cluster\", \"myid\")\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n func (c cmdable) ClusterSlots(ctx context.Context) *ClusterSlotsCmd {\n \tcmd := NewClusterSlotsCmd(ctx, \"cluster\", \"slots\")\n \t_ = c(ctx, cmd)"
    },
    {
      "sha": "2dbc2ad872d1d47c8111b6e35291b80b60181306",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/command.go",
      "status": "modified",
      "additions": 390,
      "deletions": 38,
      "changes": 428,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fcommand.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fcommand.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fcommand.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -17,6 +17,56 @@ import (\n \t\"github.com/redis/go-redis/v9/internal/util\"\n )\n \n+// keylessCommands contains Redis commands that have empty key specifications (9th slot empty)\n+// Only includes core Redis commands, excludes FT.*, ts.*, timeseries.*, search.* and subcommands\n+var keylessCommands = map[string]struct{}{\n+\t\"acl\":          {},\n+\t\"asking\":       {},\n+\t\"auth\":         {},\n+\t\"bgrewriteaof\": {},\n+\t\"bgsave\":       {},\n+\t\"client\":       {},\n+\t\"cluster\":      {},\n+\t\"config\":       {},\n+\t\"debug\":        {},\n+\t\"discard\":      {},\n+\t\"echo\":         {},\n+\t\"exec\":         {},\n+\t\"failover\":     {},\n+\t\"function\":     {},\n+\t\"hello\":        {},\n+\t\"latency\":      {},\n+\t\"lolwut\":       {},\n+\t\"module\":       {},\n+\t\"monitor\":      {},\n+\t\"multi\":        {},\n+\t\"pfselftest\":   {},\n+\t\"ping\":         {},\n+\t\"psubscribe\":   {},\n+\t\"psync\":        {},\n+\t\"publish\":      {},\n+\t\"pubsub\":       {},\n+\t\"punsubscribe\": {},\n+\t\"quit\":         {},\n+\t\"readonly\":     {},\n+\t\"readwrite\":    {},\n+\t\"replconf\":     {},\n+\t\"replicaof\":    {},\n+\t\"role\":         {},\n+\t\"save\":         {},\n+\t\"script\":       {},\n+\t\"select\":       {},\n+\t\"shutdown\":     {},\n+\t\"slaveof\":      {},\n+\t\"slowlog\":      {},\n+\t\"subscribe\":    {},\n+\t\"swapdb\":       {},\n+\t\"sync\":         {},\n+\t\"unsubscribe\":  {},\n+\t\"unwatch\":      {},\n+\t\"wait\":         {},\n+}\n+\n type Cmder interface {\n \t// command name.\n \t// e.g. \"set k v ex 10\" -> \"set\", \"cluster info\" -> \"cluster\".\n@@ -75,12 +125,22 @@ func writeCmd(wr *proto.Writer, cmd Cmder) error {\n \treturn wr.WriteArgs(cmd.Args())\n }\n \n+// cmdFirstKeyPos returns the position of the first key in the command's arguments.\n+// If the command does not have a key, it returns 0.\n+// TODO: Use the data in CommandInfo to determine the first key position.\n func cmdFirstKeyPos(cmd Cmder) int {\n \tif pos := cmd.firstKeyPos(); pos != 0 {\n \t\treturn int(pos)\n \t}\n \n-\tswitch cmd.Name() {\n+\tname := cmd.Name()\n+\n+\t// first check if the command is keyless\n+\tif _, ok := keylessCommands[name]; ok {\n+\t\treturn 0\n+\t}\n+\n+\tswitch name {\n \tcase \"eval\", \"evalsha\", \"eval_ro\", \"evalsha_ro\":\n \t\tif cmd.stringArg(2) != \"0\" {\n \t\t\treturn 3\n@@ -639,6 +699,68 @@ func (cmd *IntCmd) readReply(rd *proto.Reader) (err error) {\n \n //------------------------------------------------------------------------------\n \n+// DigestCmd is a command that returns a uint64 xxh3 hash digest.\n+//\n+// This command is specifically designed for the Redis DIGEST command,\n+// which returns the xxh3 hash of a key's value as a hex string.\n+// The hex string is automatically parsed to a uint64 value.\n+//\n+// The digest can be used for optimistic locking with SetIFDEQ, SetIFDNE,\n+// and DelExArgs commands.\n+//\n+// For examples of client-side digest generation and usage patterns, see:\n+// example/digest-optimistic-locking/\n+//\n+// Redis 8.4+. See https://redis.io/commands/digest/\n+type DigestCmd struct {\n+\tbaseCmd\n+\n+\tval uint64\n+}\n+\n+var _ Cmder = (*DigestCmd)(nil)\n+\n+func NewDigestCmd(ctx context.Context, args ...interface{}) *DigestCmd {\n+\treturn &DigestCmd{\n+\t\tbaseCmd: baseCmd{\n+\t\t\tctx:  ctx,\n+\t\t\targs: args,\n+\t\t},\n+\t}\n+}\n+\n+func (cmd *DigestCmd) SetVal(val uint64) {\n+\tcmd.val = val\n+}\n+\n+func (cmd *DigestCmd) Val() uint64 {\n+\treturn cmd.val\n+}\n+\n+func (cmd *DigestCmd) Result() (uint64, error) {\n+\treturn cmd.val, cmd.err\n+}\n+\n+func (cmd *DigestCmd) String() string {\n+\treturn cmdString(cmd, cmd.val)\n+}\n+\n+func (cmd *DigestCmd) readReply(rd *proto.Reader) (err error) {\n+\t// Redis DIGEST command returns a hex string (e.g., \"a1b2c3d4e5f67890\")\n+\t// We parse it as a uint64 xxh3 hash value\n+\tvar hexStr string\n+\thexStr, err = rd.ReadString()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\t// Parse hex string to uint64\n+\tcmd.val, err = strconv.ParseUint(hexStr, 16, 64)\n+\treturn err\n+}\n+\n+//------------------------------------------------------------------------------\n+\n type IntSliceCmd struct {\n \tbaseCmd\n \n@@ -1405,27 +1527,64 @@ func (cmd *MapStringSliceInterfaceCmd) Val() map[string][]interface{} {\n }\n \n func (cmd *MapStringSliceInterfaceCmd) readReply(rd *proto.Reader) (err error) {\n-\tn, err := rd.ReadMapLen()\n+\treadType, err := rd.PeekReplyType()\n \tif err != nil {\n \t\treturn err\n \t}\n-\tcmd.val = make(map[string][]interface{}, n)\n-\tfor i := 0; i < n; i++ {\n-\t\tk, err := rd.ReadString()\n+\n+\tcmd.val = make(map[string][]interface{})\n+\n+\tswitch readType {\n+\tcase proto.RespMap:\n+\t\tn, err := rd.ReadMapLen()\n \t\tif err != nil {\n \t\t\treturn err\n \t\t}\n-\t\tnn, err := rd.ReadArrayLen()\n+\t\tfor i := 0; i < n; i++ {\n+\t\t\tk, err := rd.ReadString()\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tnn, err := rd.ReadArrayLen()\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tcmd.val[k] = make([]interface{}, nn)\n+\t\t\tfor j := 0; j < nn; j++ {\n+\t\t\t\tvalue, err := rd.ReadReply()\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn err\n+\t\t\t\t}\n+\t\t\t\tcmd.val[k][j] = value\n+\t\t\t}\n+\t\t}\n+\tcase proto.RespArray:\n+\t\t// RESP2 response\n+\t\tn, err := rd.ReadArrayLen()\n \t\tif err != nil {\n \t\t\treturn err\n \t\t}\n-\t\tcmd.val[k] = make([]interface{}, nn)\n-\t\tfor j := 0; j < nn; j++ {\n-\t\t\tvalue, err := rd.ReadReply()\n+\n+\t\tfor i := 0; i < n; i++ {\n+\t\t\t// Each entry in this array is itself an array with key details\n+\t\t\titemLen, err := rd.ReadArrayLen()\n \t\t\tif err != nil {\n \t\t\t\treturn err\n \t\t\t}\n-\t\t\tcmd.val[k][j] = value\n+\n+\t\t\tkey, err := rd.ReadString()\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tcmd.val[key] = make([]interface{}, 0, itemLen-1)\n+\t\t\tfor j := 1; j < itemLen; j++ {\n+\t\t\t\t// Read the inner array for timestamp-value pairs\n+\t\t\t\tdata, err := rd.ReadReply()\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn err\n+\t\t\t\t}\n+\t\t\t\tcmd.val[key] = append(cmd.val[key], data)\n+\t\t\t}\n \t\t}\n \t}\n \n@@ -1489,6 +1648,12 @@ func (cmd *StringStructMapCmd) readReply(rd *proto.Reader) error {\n type XMessage struct {\n \tID     string\n \tValues map[string]interface{}\n+\t// MillisElapsedFromDelivery is the number of milliseconds since the entry was last delivered.\n+\t// Only populated when using XREADGROUP with CLAIM argument for claimed entries.\n+\tMillisElapsedFromDelivery int64\n+\t// DeliveredCount is the number of times the entry was delivered.\n+\t// Only populated when using XREADGROUP with CLAIM argument for claimed entries.\n+\tDeliveredCount int64\n }\n \n type XMessageSliceCmd struct {\n@@ -1545,10 +1710,16 @@ func readXMessageSlice(rd *proto.Reader) ([]XMessage, error) {\n }\n \n func readXMessage(rd *proto.Reader) (XMessage, error) {\n-\tif err := rd.ReadFixedArrayLen(2); err != nil {\n+\t// Read array length can be 2 or 4 (with CLAIM metadata)\n+\tn, err := rd.ReadArrayLen()\n+\tif err != nil {\n \t\treturn XMessage{}, err\n \t}\n \n+\tif n != 2 && n != 4 {\n+\t\treturn XMessage{}, fmt.Errorf(\"redis: got %d elements in the XMessage array, expected 2 or 4\", n)\n+\t}\n+\n \tid, err := rd.ReadString()\n \tif err != nil {\n \t\treturn XMessage{}, err\n@@ -1561,10 +1732,24 @@ func readXMessage(rd *proto.Reader) (XMessage, error) {\n \t\t}\n \t}\n \n-\treturn XMessage{\n+\tmsg := XMessage{\n \t\tID:     id,\n \t\tValues: v,\n-\t}, nil\n+\t}\n+\n+\tif n == 4 {\n+\t\tmsg.MillisElapsedFromDelivery, err = rd.ReadInt()\n+\t\tif err != nil {\n+\t\t\treturn XMessage{}, err\n+\t\t}\n+\n+\t\tmsg.DeliveredCount, err = rd.ReadInt()\n+\t\tif err != nil {\n+\t\t\treturn XMessage{}, err\n+\t\t}\n+\t}\n+\n+\treturn msg, nil\n }\n \n func stringInterfaceMapParser(rd *proto.Reader) (map[string]interface{}, error) {\n@@ -2067,7 +2252,9 @@ type XInfoGroup struct {\n \tPending         int64\n \tLastDeliveredID string\n \tEntriesRead     int64\n-\tLag             int64\n+\t// Lag represents the number of pending messages in the stream not yet\n+\t// delivered to this consumer group. Returns -1 when the lag cannot be determined.\n+\tLag int64\n }\n \n var _ Cmder = (*XInfoGroupsCmd)(nil)\n@@ -2150,8 +2337,11 @@ func (cmd *XInfoGroupsCmd) readReply(rd *proto.Reader) error {\n \n \t\t\t\t// lag: the number of entries in the stream that are still waiting to be delivered\n \t\t\t\t// to the group's consumers, or a NULL(Nil) when that number can't be determined.\n+\t\t\t\t// In that case, we return -1.\n \t\t\t\tif err != nil && err != Nil {\n \t\t\t\t\treturn err\n+\t\t\t\t} else if err == Nil {\n+\t\t\t\t\tgroup.Lag = -1\n \t\t\t\t}\n \t\t\tdefault:\n \t\t\t\treturn fmt.Errorf(\"redis: unexpected key %q in XINFO GROUPS reply\", key)\n@@ -3542,15 +3732,14 @@ func (c *cmdsInfoCache) Get(ctx context.Context) (map[string]*CommandInfo, error\n \t\t\treturn err\n \t\t}\n \n+\t\tlowerCmds := make(map[string]*CommandInfo, len(cmds))\n+\n \t\t// Extensions have cmd names in upper case. Convert them to lower case.\n \t\tfor k, v := range cmds {\n-\t\t\tlower := internal.ToLower(k)\n-\t\t\tif lower != k {\n-\t\t\t\tcmds[lower] = v\n-\t\t\t}\n+\t\t\tlowerCmds[internal.ToLower(k)] = v\n \t\t}\n \n-\t\tc.cmds = cmds\n+\t\tc.cmds = lowerCmds\n \t\treturn nil\n \t})\n \treturn c.cmds, err\n@@ -3668,6 +3857,83 @@ func (cmd *SlowLogCmd) readReply(rd *proto.Reader) error {\n \n //-----------------------------------------------------------------------\n \n+type Latency struct {\n+\tName   string\n+\tTime   time.Time\n+\tLatest time.Duration\n+\tMax    time.Duration\n+}\n+\n+type LatencyCmd struct {\n+\tbaseCmd\n+\tval []Latency\n+}\n+\n+var _ Cmder = (*LatencyCmd)(nil)\n+\n+func NewLatencyCmd(ctx context.Context, args ...interface{}) *LatencyCmd {\n+\treturn &LatencyCmd{\n+\t\tbaseCmd: baseCmd{\n+\t\t\tctx:  ctx,\n+\t\t\targs: args,\n+\t\t},\n+\t}\n+}\n+\n+func (cmd *LatencyCmd) SetVal(val []Latency) {\n+\tcmd.val = val\n+}\n+\n+func (cmd *LatencyCmd) Val() []Latency {\n+\treturn cmd.val\n+}\n+\n+func (cmd *LatencyCmd) Result() ([]Latency, error) {\n+\treturn cmd.val, cmd.err\n+}\n+\n+func (cmd *LatencyCmd) String() string {\n+\treturn cmdString(cmd, cmd.val)\n+}\n+\n+func (cmd *LatencyCmd) readReply(rd *proto.Reader) error {\n+\tn, err := rd.ReadArrayLen()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tcmd.val = make([]Latency, n)\n+\tfor i := 0; i < len(cmd.val); i++ {\n+\t\tnn, err := rd.ReadArrayLen()\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tif nn < 3 {\n+\t\t\treturn fmt.Errorf(\"redis: got %d elements in latency get, expected at least 3\", nn)\n+\t\t}\n+\t\tif cmd.val[i].Name, err = rd.ReadString(); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tcreatedAt, err := rd.ReadInt()\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tcmd.val[i].Time = time.Unix(createdAt, 0)\n+\t\tlatest, err := rd.ReadInt()\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tcmd.val[i].Latest = time.Duration(latest) * time.Millisecond\n+\t\tmaximum, err := rd.ReadInt()\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tcmd.val[i].Max = time.Duration(maximum) * time.Millisecond\n+\t}\n+\treturn nil\n+}\n+\n+//-----------------------------------------------------------------------\n+\n type MapStringInterfaceCmd struct {\n \tbaseCmd\n \n@@ -3795,7 +4061,8 @@ func (cmd *MapStringStringSliceCmd) readReply(rd *proto.Reader) error {\n }\n \n // -----------------------------------------------------------------------\n-// MapStringInterfaceCmd represents a command that returns a map of strings to interface{}.\n+\n+// MapMapStringInterfaceCmd represents a command that returns a map of strings to interface{}.\n type MapMapStringInterfaceCmd struct {\n \tbaseCmd\n \tval map[string]interface{}\n@@ -3826,30 +4093,48 @@ func (cmd *MapMapStringInterfaceCmd) Val() map[string]interface{} {\n \treturn cmd.val\n }\n \n+// readReply will try to parse the reply from the proto.Reader for both resp2 and resp3\n func (cmd *MapMapStringInterfaceCmd) readReply(rd *proto.Reader) (err error) {\n-\tn, err := rd.ReadArrayLen()\n+\tdata, err := rd.ReadReply()\n \tif err != nil {\n \t\treturn err\n \t}\n+\tresultMap := map[string]interface{}{}\n \n-\tdata := make(map[string]interface{}, n/2)\n-\tfor i := 0; i < n; i += 2 {\n-\t\t_, err := rd.ReadArrayLen()\n-\t\tif err != nil {\n-\t\t\tcmd.err = err\n-\t\t}\n-\t\tkey, err := rd.ReadString()\n-\t\tif err != nil {\n-\t\t\tcmd.err = err\n-\t\t}\n-\t\tvalue, err := rd.ReadString()\n-\t\tif err != nil {\n-\t\t\tcmd.err = err\n+\tswitch midResponse := data.(type) {\n+\tcase map[interface{}]interface{}: // resp3 will return map\n+\t\tfor k, v := range midResponse {\n+\t\t\tstringKey, ok := k.(string)\n+\t\t\tif !ok {\n+\t\t\t\treturn fmt.Errorf(\"redis: invalid map key %#v\", k)\n+\t\t\t}\n+\t\t\tresultMap[stringKey] = v\n+\t\t}\n+\tcase []interface{}: // resp2 will return array of arrays\n+\t\tn := len(midResponse)\n+\t\tfor i := 0; i < n; i++ {\n+\t\t\tfinalArr, ok := midResponse[i].([]interface{}) // final array that we need to transform to map\n+\t\t\tif !ok {\n+\t\t\t\treturn fmt.Errorf(\"redis: unexpected response %#v\", data)\n+\t\t\t}\n+\t\t\tm := len(finalArr)\n+\t\t\tif m%2 != 0 { // since this should be map, keys should be even number\n+\t\t\t\treturn fmt.Errorf(\"redis: unexpected response %#v\", data)\n+\t\t\t}\n+\n+\t\t\tfor j := 0; j < m; j += 2 {\n+\t\t\t\tstringKey, ok := finalArr[j].(string) // the first one\n+\t\t\t\tif !ok {\n+\t\t\t\t\treturn fmt.Errorf(\"redis: invalid map key %#v\", finalArr[i])\n+\t\t\t\t}\n+\t\t\t\tresultMap[stringKey] = finalArr[j+1] // second one is value\n+\t\t\t}\n \t\t}\n-\t\tdata[key] = value\n+\tdefault:\n+\t\treturn fmt.Errorf(\"redis: unexpected response %#v\", data)\n \t}\n \n-\tcmd.val = data\n+\tcmd.val = resultMap\n \treturn nil\n }\n \n@@ -5078,6 +5363,10 @@ type ClientInfo struct {\n \tOutputListLength   int           // oll, output list length (replies are queued in this list when the buffer is full)\n \tOutputMemory       int           // omem, output buffer memory usage\n \tTotalMemory        int           // tot-mem, total memory consumed by this client in its various buffers\n+\tTotalNetIn         int           // tot-net-in, total network input\n+\tTotalNetOut        int           // tot-net-out, total network output\n+\tTotalCmds          int           // tot-cmds, total number of commands processed\n+\tIoThread           int           // io-thread id\n \tEvents             string        // file descriptor events (see below)\n \tLastCmd            string        // cmd, last command played\n \tUser               string        // the authenticated username of the client\n@@ -5242,6 +5531,12 @@ func parseClientInfo(txt string) (info *ClientInfo, err error) {\n \t\t\tinfo.OutputMemory, err = strconv.Atoi(val)\n \t\tcase \"tot-mem\":\n \t\t\tinfo.TotalMemory, err = strconv.Atoi(val)\n+\t\tcase \"tot-net-in\":\n+\t\t\tinfo.TotalNetIn, err = strconv.Atoi(val)\n+\t\tcase \"tot-net-out\":\n+\t\t\tinfo.TotalNetOut, err = strconv.Atoi(val)\n+\t\tcase \"tot-cmds\":\n+\t\t\tinfo.TotalCmds, err = strconv.Atoi(val)\n \t\tcase \"events\":\n \t\t\tinfo.Events = val\n \t\tcase \"cmd\":\n@@ -5256,6 +5551,8 @@ func parseClientInfo(txt string) (info *ClientInfo, err error) {\n \t\t\tinfo.LibName = val\n \t\tcase \"lib-ver\":\n \t\t\tinfo.LibVer = val\n+\t\tcase \"io-thread\":\n+\t\t\tinfo.IoThread, err = strconv.Atoi(val)\n \t\tdefault:\n \t\t\treturn nil, fmt.Errorf(\"redis: unexpected client info key(%s)\", key)\n \t\t}\n@@ -5435,8 +5732,6 @@ func (cmd *InfoCmd) readReply(rd *proto.Reader) error {\n \n \tsection := \"\"\n \tscanner := bufio.NewScanner(strings.NewReader(val))\n-\tmoduleRe := regexp.MustCompile(`module:name=(.+?),(.+)$`)\n-\n \tfor scanner.Scan() {\n \t\tline := scanner.Text()\n \t\tif strings.HasPrefix(line, \"#\") {\n@@ -5447,6 +5742,7 @@ func (cmd *InfoCmd) readReply(rd *proto.Reader) error {\n \t\t\tcmd.val[section] = make(map[string]string)\n \t\t} else if line != \"\" {\n \t\t\tif section == \"Modules\" {\n+\t\t\t\tmoduleRe := regexp.MustCompile(`module:name=(.+?),(.+)$`)\n \t\t\t\tkv := moduleRe.FindStringSubmatch(line)\n \t\t\t\tif len(kv) == 3 {\n \t\t\t\t\tcmd.val[section][kv[1]] = kv[2]\n@@ -5557,3 +5853,59 @@ func (cmd *MonitorCmd) Stop() {\n \tdefer cmd.mu.Unlock()\n \tcmd.status = monitorStatusStop\n }\n+\n+type VectorScoreSliceCmd struct {\n+\tbaseCmd\n+\n+\tval []VectorScore\n+}\n+\n+var _ Cmder = (*VectorScoreSliceCmd)(nil)\n+\n+func NewVectorInfoSliceCmd(ctx context.Context, args ...any) *VectorScoreSliceCmd {\n+\treturn &VectorScoreSliceCmd{\n+\t\tbaseCmd: baseCmd{\n+\t\t\tctx:  ctx,\n+\t\t\targs: args,\n+\t\t},\n+\t}\n+}\n+\n+func (cmd *VectorScoreSliceCmd) SetVal(val []VectorScore) {\n+\tcmd.val = val\n+}\n+\n+func (cmd *VectorScoreSliceCmd) Val() []VectorScore {\n+\treturn cmd.val\n+}\n+\n+func (cmd *VectorScoreSliceCmd) Result() ([]VectorScore, error) {\n+\treturn cmd.val, cmd.err\n+}\n+\n+func (cmd *VectorScoreSliceCmd) String() string {\n+\treturn cmdString(cmd, cmd.val)\n+}\n+\n+func (cmd *VectorScoreSliceCmd) readReply(rd *proto.Reader) error {\n+\tn, err := rd.ReadMapLen()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\tcmd.val = make([]VectorScore, n)\n+\tfor i := 0; i < n; i++ {\n+\t\tname, err := rd.ReadString()\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tcmd.val[i].Name = name\n+\n+\t\tscore, err := rd.ReadFloat()\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tcmd.val[i].Score = score\n+\t}\n+\treturn nil\n+}"
    },
    {
      "sha": "daee5505e147a0ac6ed491287f5a8d09ae528766",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/commands.go",
      "status": "modified",
      "additions": 70,
      "deletions": 3,
      "changes": 73,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fcommands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fcommands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fcommands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -81,6 +81,8 @@ func appendArg(dst []interface{}, arg interface{}) []interface{} {\n \t\treturn dst\n \tcase time.Time, time.Duration, encoding.BinaryMarshaler, net.IP:\n \t\treturn append(dst, arg)\n+\tcase nil:\n+\t\treturn dst\n \tdefault:\n \t\t// scan struct field\n \t\tv := reflect.ValueOf(arg)\n@@ -153,6 +155,12 @@ func isEmptyValue(v reflect.Value) bool {\n \t\treturn v.Float() == 0\n \tcase reflect.Interface, reflect.Pointer:\n \t\treturn v.IsNil()\n+\tcase reflect.Struct:\n+\t\tif v.Type() == reflect.TypeOf(time.Time{}) {\n+\t\t\treturn v.IsZero()\n+\t\t}\n+\t\t// Only supports the struct time.Time,\n+\t\t// subsequent iterations will follow the func Scan support decoder.\n \t}\n \treturn false\n }\n@@ -185,6 +193,7 @@ type Cmdable interface {\n \tClientID(ctx context.Context) *IntCmd\n \tClientUnblock(ctx context.Context, id int64) *IntCmd\n \tClientUnblockWithError(ctx context.Context, id int64) *IntCmd\n+\tClientMaintNotifications(ctx context.Context, enabled bool, endpointType string) *StatusCmd\n \tConfigGet(ctx context.Context, parameter string) *MapStringStringCmd\n \tConfigResetStat(ctx context.Context) *StatusCmd\n \tConfigSet(ctx context.Context, parameter, value string) *StatusCmd\n@@ -202,16 +211,19 @@ type Cmdable interface {\n \tShutdownNoSave(ctx context.Context) *StatusCmd\n \tSlaveOf(ctx context.Context, host, port string) *StatusCmd\n \tSlowLogGet(ctx context.Context, num int64) *SlowLogCmd\n+\tSlowLogLen(ctx context.Context) *IntCmd\n+\tSlowLogReset(ctx context.Context) *StatusCmd\n \tTime(ctx context.Context) *TimeCmd\n \tDebugObject(ctx context.Context, key string) *StringCmd\n \tMemoryUsage(ctx context.Context, key string, samples ...int) *IntCmd\n+\tLatency(ctx context.Context) *LatencyCmd\n+\tLatencyReset(ctx context.Context, events ...interface{}) *StatusCmd\n \n \tModuleLoadex(ctx context.Context, conf *ModuleLoadexConfig) *StringCmd\n \n \tACLCmdable\n \tBitMapCmdable\n \tClusterCmdable\n-\tGearsCmdable\n \tGenericCmdable\n \tGeoCmdable\n \tHashCmdable\n@@ -227,6 +239,7 @@ type Cmdable interface {\n \tStreamCmdable\n \tTimeseriesCmdable\n \tJSONCmdable\n+\tVectorSetCmdable\n }\n \n type StatefulCmdable interface {\n@@ -245,6 +258,7 @@ var (\n \t_ Cmdable = (*Tx)(nil)\n \t_ Cmdable = (*Ring)(nil)\n \t_ Cmdable = (*ClusterClient)(nil)\n+\t_ Cmdable = (*Pipeline)(nil)\n )\n \n type cmdable func(ctx context.Context, cmd Cmder) error\n@@ -331,7 +345,7 @@ func (info LibraryInfo) Validate() error {\n \treturn nil\n }\n \n-// Hello Set the resp protocol used.\n+// Hello sets the resp protocol used.\n func (c statefulCmdable) Hello(ctx context.Context,\n \tver int, username, password, clientName string,\n ) *MapStringInterfaceCmd {\n@@ -423,6 +437,12 @@ func (c cmdable) Ping(ctx context.Context) *StatusCmd {\n \treturn cmd\n }\n \n+func (c cmdable) Do(ctx context.Context, args ...interface{}) *Cmd {\n+\tcmd := NewCmd(ctx, args...)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n func (c cmdable) Quit(_ context.Context) *StatusCmd {\n \tpanic(\"not implemented\")\n }\n@@ -504,6 +524,23 @@ func (c cmdable) ClientInfo(ctx context.Context) *ClientInfoCmd {\n \treturn cmd\n }\n \n+// ClientMaintNotifications enables or disables maintenance notifications for maintenance upgrades.\n+// When enabled, the client will receive push notifications about Redis maintenance events.\n+func (c cmdable) ClientMaintNotifications(ctx context.Context, enabled bool, endpointType string) *StatusCmd {\n+\targs := []interface{}{\"client\", \"maint_notifications\"}\n+\tif enabled {\n+\t\tif endpointType == \"\" {\n+\t\t\tendpointType = \"none\"\n+\t\t}\n+\t\targs = append(args, \"on\", \"moving-endpoint-type\", endpointType)\n+\t} else {\n+\t\targs = append(args, \"off\")\n+\t}\n+\tcmd := NewStatusCmd(ctx, args...)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n // ------------------------------------------------------------------------------------------------\n \n func (c cmdable) ConfigGet(ctx context.Context, parameter string) *MapStringStringCmd {\n@@ -640,6 +677,34 @@ func (c cmdable) SlowLogGet(ctx context.Context, num int64) *SlowLogCmd {\n \treturn cmd\n }\n \n+func (c cmdable) SlowLogLen(ctx context.Context) *IntCmd {\n+\tcmd := NewIntCmd(ctx, \"slowlog\", \"len\")\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) SlowLogReset(ctx context.Context) *StatusCmd {\n+\tcmd := NewStatusCmd(ctx, \"slowlog\", \"reset\")\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) Latency(ctx context.Context) *LatencyCmd {\n+\tcmd := NewLatencyCmd(ctx, \"latency\", \"latest\")\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) LatencyReset(ctx context.Context, events ...interface{}) *StatusCmd {\n+\targs := make([]interface{}, 2+len(events))\n+\targs[0] = \"latency\"\n+\targs[1] = \"reset\"\n+\tcopy(args[2:], events)\n+\tcmd := NewStatusCmd(ctx, args...)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n func (c cmdable) Sync(_ context.Context) {\n \tpanic(\"not implemented\")\n }\n@@ -660,7 +725,9 @@ func (c cmdable) MemoryUsage(ctx context.Context, key string, samples ...int) *I\n \targs := []interface{}{\"memory\", \"usage\", key}\n \tif len(samples) > 0 {\n \t\tif len(samples) != 1 {\n-\t\t\tpanic(\"MemoryUsage expects single sample count\")\n+\t\t\tcmd := NewIntCmd(ctx)\n+\t\t\tcmd.SetErr(errors.New(\"MemoryUsage expects single sample count\"))\n+\t\t\treturn cmd\n \t\t}\n \t\targs = append(args, \"SAMPLES\", samples[0])\n \t}"
    },
    {
      "sha": "5ffedb0a5564b72beec98571ed2c1e6826645223",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/docker-compose.yml",
      "status": "added",
      "additions": 106,
      "deletions": 0,
      "changes": 106,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fdocker-compose.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fdocker-compose.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fdocker-compose.yml?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,106 @@\n+---\n+\n+services:\n+  redis:\n+    image: ${CLIENT_LIBS_TEST_IMAGE:-redislabs/client-libs-test:8.4.0}\n+    platform: linux/amd64\n+    container_name: redis-standalone\n+    environment:\n+      - TLS_ENABLED=yes\n+      - REDIS_CLUSTER=no\n+      - PORT=6379\n+      - TLS_PORT=6666\n+    command: ${REDIS_EXTRA_ARGS:---enable-debug-command yes --enable-module-command yes --tls-auth-clients optional --save \"\"}\n+    ports:\n+      - 6379:6379\n+      - 6666:6666 # TLS port\n+    volumes:\n+      - \"./dockers/standalone:/redis/work\"\n+    profiles:\n+      - standalone\n+      - sentinel\n+      - all-stack\n+      - all\n+\n+  osscluster:\n+    image: ${CLIENT_LIBS_TEST_IMAGE:-redislabs/client-libs-test:8.4.0}\n+    platform: linux/amd64\n+    container_name: redis-osscluster\n+    environment:\n+      - NODES=6\n+      - PORT=16600\n+    command: \"--cluster-enabled yes\"\n+    ports:\n+      - \"16600-16605:16600-16605\"\n+    volumes:\n+      - \"./dockers/osscluster:/redis/work\"\n+    profiles:\n+      - cluster\n+      - all-stack\n+      - all\n+\n+  sentinel-cluster:\n+    image: ${CLIENT_LIBS_TEST_IMAGE:-redislabs/client-libs-test:8.4.0}\n+    platform: linux/amd64\n+    container_name: redis-sentinel-cluster\n+    network_mode: \"host\"\n+    environment:\n+      - NODES=3\n+      - TLS_ENABLED=yes\n+      - REDIS_CLUSTER=no\n+      - PORT=9121\n+    command: ${REDIS_EXTRA_ARGS:---enable-debug-command yes --enable-module-command yes --tls-auth-clients optional --save \"\"}\n+    #ports:\n+    #  - \"9121-9123:9121-9123\"\n+    volumes:\n+      - \"./dockers/sentinel-cluster:/redis/work\"\n+    profiles:\n+      - sentinel\n+      - all-stack\n+      - all\n+\n+  sentinel:\n+    image: ${CLIENT_LIBS_TEST_IMAGE:-redislabs/client-libs-test:8.4.0}\n+    platform: linux/amd64\n+    container_name: redis-sentinel\n+    depends_on:\n+      - sentinel-cluster\n+    environment:\n+      - NODES=3\n+      - REDIS_CLUSTER=no\n+      - PORT=26379\n+    command: ${REDIS_EXTRA_ARGS:---sentinel}\n+    network_mode: \"host\"\n+    #ports:\n+    #  - 26379:26379\n+    #  - 26380:26380\n+    #  - 26381:26381\n+    volumes:\n+      - \"./dockers/sentinel.conf:/redis/config-default/redis.conf\"\n+      - \"./dockers/sentinel:/redis/work\"\n+    profiles:\n+      - sentinel\n+      - all-stack\n+      - all\n+\n+  ring-cluster:\n+    image: ${CLIENT_LIBS_TEST_IMAGE:-redislabs/client-libs-test:8.4.0}\n+    platform: linux/amd64\n+    container_name: redis-ring-cluster\n+    environment:\n+      - NODES=3\n+      - TLS_ENABLED=yes\n+      - REDIS_CLUSTER=no\n+      - PORT=6390\n+    command: ${REDIS_EXTRA_ARGS:---enable-debug-command yes --enable-module-command yes --tls-auth-clients optional --save \"\"}\n+    ports:\n+      - 6390:6390\n+      - 6391:6391\n+      - 6392:6392\n+    volumes:\n+      - \"./dockers/ring:/redis/work\"\n+    profiles:\n+      - ring\n+      - cluster\n+      - all-stack\n+      - all"
    },
    {
      "sha": "12b5604dfdcdf2b9dc10f49aaa0bf58c1a065b0d",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/error.go",
      "status": "modified",
      "additions": 224,
      "deletions": 34,
      "changes": 258,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Ferror.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Ferror.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Ferror.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -15,6 +15,19 @@ import (\n // ErrClosed performs any operation on the closed client will return this error.\n var ErrClosed = pool.ErrClosed\n \n+// ErrPoolExhausted is returned from a pool connection method\n+// when the maximum number of database connections in the pool has been reached.\n+var ErrPoolExhausted = pool.ErrPoolExhausted\n+\n+// ErrPoolTimeout timed out waiting to get a connection from the connection pool.\n+var ErrPoolTimeout = pool.ErrPoolTimeout\n+\n+// ErrCrossSlot is returned when keys are used in the same Redis command and\n+// the keys are not in the same hash slot. This error is returned by Redis\n+// Cluster and will be returned by the client when TxPipeline or TxPipelined\n+// is used on a ClusterClient with keys in different slots.\n+var ErrCrossSlot = proto.RedisError(\"CROSSSLOT Keys in request don't hash to the same slot\")\n+\n // HasErrorPrefix checks if the err is a Redis error and the message contains a prefix.\n func HasErrorPrefix(err error, prefix string) bool {\n \tvar rErr Error\n@@ -38,23 +51,83 @@ type Error interface {\n \n var _ Error = proto.RedisError(\"\")\n \n+func isContextError(err error) bool {\n+\t// Check for wrapped context errors using errors.Is\n+\treturn errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded)\n+}\n+\n+// isTimeoutError checks if an error is a timeout error, even if wrapped.\n+// Returns (isTimeout, shouldRetryOnTimeout) where:\n+// - isTimeout: true if the error is any kind of timeout error\n+// - shouldRetryOnTimeout: true if Timeout() method returns true\n+func isTimeoutError(err error) (isTimeout bool, hasTimeoutFlag bool) {\n+\t// Check for timeoutError interface (works with wrapped errors)\n+\tvar te timeoutError\n+\tif errors.As(err, &te) {\n+\t\treturn true, te.Timeout()\n+\t}\n+\n+\t// Check for net.Error specifically (common case for network timeouts)\n+\tvar netErr net.Error\n+\tif errors.As(err, &netErr) {\n+\t\treturn true, netErr.Timeout()\n+\t}\n+\n+\treturn false, false\n+}\n+\n func shouldRetry(err error, retryTimeout bool) bool {\n-\tswitch err {\n-\tcase io.EOF, io.ErrUnexpectedEOF:\n+\tif err == nil {\n+\t\treturn false\n+\t}\n+\n+\t// Check for EOF errors (works with wrapped errors)\n+\tif errors.Is(err, io.EOF) || errors.Is(err, io.ErrUnexpectedEOF) {\n \t\treturn true\n-\tcase nil, context.Canceled, context.DeadlineExceeded:\n+\t}\n+\n+\t// Check for context errors (works with wrapped errors)\n+\tif errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {\n \t\treturn false\n \t}\n \n-\tif v, ok := err.(timeoutError); ok {\n-\t\tif v.Timeout() {\n+\t// Check for pool timeout (works with wrapped errors)\n+\tif errors.Is(err, pool.ErrPoolTimeout) {\n+\t\t// connection pool timeout, increase retries. #3289\n+\t\treturn true\n+\t}\n+\n+\t// Check for timeout errors (works with wrapped errors)\n+\tif isTimeout, hasTimeoutFlag := isTimeoutError(err); isTimeout {\n+\t\tif hasTimeoutFlag {\n \t\t\treturn retryTimeout\n \t\t}\n \t\treturn true\n \t}\n \n+\t// Check for typed Redis errors using errors.As (works with wrapped errors)\n+\tif proto.IsMaxClientsError(err) {\n+\t\treturn true\n+\t}\n+\tif proto.IsLoadingError(err) {\n+\t\treturn true\n+\t}\n+\tif proto.IsReadOnlyError(err) {\n+\t\treturn true\n+\t}\n+\tif proto.IsMasterDownError(err) {\n+\t\treturn true\n+\t}\n+\tif proto.IsClusterDownError(err) {\n+\t\treturn true\n+\t}\n+\tif proto.IsTryAgainError(err) {\n+\t\treturn true\n+\t}\n+\n+\t// Fallback to string checking for backward compatibility with plain errors\n \ts := err.Error()\n-\tif s == \"ERR max number of clients reached\" {\n+\tif strings.HasPrefix(s, \"ERR max number of clients reached\") {\n \t\treturn true\n \t}\n \tif strings.HasPrefix(s, \"LOADING \") {\n@@ -69,20 +142,36 @@ func shouldRetry(err error, retryTimeout bool) bool {\n \tif strings.HasPrefix(s, \"TRYAGAIN \") {\n \t\treturn true\n \t}\n+\tif strings.HasPrefix(s, \"MASTERDOWN \") {\n+\t\treturn true\n+\t}\n \n \treturn false\n }\n \n func isRedisError(err error) bool {\n-\t_, ok := err.(proto.RedisError)\n-\treturn ok\n+\t// Check if error implements the Error interface (works with wrapped errors)\n+\tvar redisErr Error\n+\tif errors.As(err, &redisErr) {\n+\t\treturn true\n+\t}\n+\t// Also check for proto.RedisError specifically\n+\tvar protoRedisErr proto.RedisError\n+\treturn errors.As(err, &protoRedisErr)\n }\n \n func isBadConn(err error, allowTimeout bool, addr string) bool {\n-\tswitch err {\n-\tcase nil:\n+\tif err == nil {\n \t\treturn false\n-\tcase context.Canceled, context.DeadlineExceeded:\n+\t}\n+\n+\t// Check for context errors (works with wrapped errors)\n+\tif errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {\n+\t\treturn true\n+\t}\n+\n+\t// Check for pool timeout errors (works with wrapped errors)\n+\tif errors.Is(err, pool.ErrConnUnusableTimeout) {\n \t\treturn true\n \t}\n \n@@ -103,7 +192,9 @@ func isBadConn(err error, allowTimeout bool, addr string) bool {\n \t}\n \n \tif allowTimeout {\n-\t\tif netErr, ok := err.(net.Error); ok && netErr.Timeout() {\n+\t\t// Check for network timeout errors (works with wrapped errors)\n+\t\tvar netErr net.Error\n+\t\tif errors.As(err, &netErr) && netErr.Timeout() {\n \t\t\treturn false\n \t\t}\n \t}\n@@ -112,44 +203,143 @@ func isBadConn(err error, allowTimeout bool, addr string) bool {\n }\n \n func isMovedError(err error) (moved bool, ask bool, addr string) {\n-\tif !isRedisError(err) {\n-\t\treturn\n+\t// Check for typed MovedError\n+\tif movedErr, ok := proto.IsMovedError(err); ok {\n+\t\taddr = movedErr.Addr()\n+\t\taddr = internal.GetAddr(addr)\n+\t\treturn true, false, addr\n \t}\n \n-\ts := err.Error()\n-\tswitch {\n-\tcase strings.HasPrefix(s, \"MOVED \"):\n-\t\tmoved = true\n-\tcase strings.HasPrefix(s, \"ASK \"):\n-\t\task = true\n-\tdefault:\n-\t\treturn\n+\t// Check for typed AskError\n+\tif askErr, ok := proto.IsAskError(err); ok {\n+\t\taddr = askErr.Addr()\n+\t\taddr = internal.GetAddr(addr)\n+\t\treturn false, true, addr\n \t}\n \n-\tind := strings.LastIndex(s, \" \")\n-\tif ind == -1 {\n-\t\treturn false, false, \"\"\n+\t// Fallback to string checking for backward compatibility\n+\ts := err.Error()\n+\tif strings.HasPrefix(s, \"MOVED \") {\n+\t\t// Parse: MOVED 3999 127.0.0.1:6381\n+\t\tparts := strings.Split(s, \" \")\n+\t\tif len(parts) == 3 {\n+\t\t\taddr = internal.GetAddr(parts[2])\n+\t\t\treturn true, false, addr\n+\t\t}\n+\t}\n+\tif strings.HasPrefix(s, \"ASK \") {\n+\t\t// Parse: ASK 3999 127.0.0.1:6381\n+\t\tparts := strings.Split(s, \" \")\n+\t\tif len(parts) == 3 {\n+\t\t\taddr = internal.GetAddr(parts[2])\n+\t\t\treturn false, true, addr\n+\t\t}\n \t}\n \n-\taddr = s[ind+1:]\n-\taddr = internal.GetAddr(addr)\n-\treturn\n+\treturn false, false, \"\"\n }\n \n func isLoadingError(err error) bool {\n-\treturn strings.HasPrefix(err.Error(), \"LOADING \")\n+\treturn proto.IsLoadingError(err)\n }\n \n func isReadOnlyError(err error) bool {\n-\treturn strings.HasPrefix(err.Error(), \"READONLY \")\n+\treturn proto.IsReadOnlyError(err)\n }\n \n func isMovedSameConnAddr(err error, addr string) bool {\n-\tredisError := err.Error()\n-\tif !strings.HasPrefix(redisError, \"MOVED \") {\n-\t\treturn false\n+\tif movedErr, ok := proto.IsMovedError(err); ok {\n+\t\treturn strings.HasSuffix(movedErr.Addr(), addr)\n+\t}\n+\treturn false\n+}\n+\n+//------------------------------------------------------------------------------\n+\n+// Typed error checking functions for public use.\n+// These functions work correctly even when errors are wrapped in hooks.\n+\n+// IsLoadingError checks if an error is a Redis LOADING error, even if wrapped.\n+// LOADING errors occur when Redis is loading the dataset in memory.\n+func IsLoadingError(err error) bool {\n+\treturn proto.IsLoadingError(err)\n+}\n+\n+// IsReadOnlyError checks if an error is a Redis READONLY error, even if wrapped.\n+// READONLY errors occur when trying to write to a read-only replica.\n+func IsReadOnlyError(err error) bool {\n+\treturn proto.IsReadOnlyError(err)\n+}\n+\n+// IsClusterDownError checks if an error is a Redis CLUSTERDOWN error, even if wrapped.\n+// CLUSTERDOWN errors occur when the cluster is down.\n+func IsClusterDownError(err error) bool {\n+\treturn proto.IsClusterDownError(err)\n+}\n+\n+// IsTryAgainError checks if an error is a Redis TRYAGAIN error, even if wrapped.\n+// TRYAGAIN errors occur when a command cannot be processed and should be retried.\n+func IsTryAgainError(err error) bool {\n+\treturn proto.IsTryAgainError(err)\n+}\n+\n+// IsMasterDownError checks if an error is a Redis MASTERDOWN error, even if wrapped.\n+// MASTERDOWN errors occur when the master is down.\n+func IsMasterDownError(err error) bool {\n+\treturn proto.IsMasterDownError(err)\n+}\n+\n+// IsMaxClientsError checks if an error is a Redis max clients error, even if wrapped.\n+// This error occurs when the maximum number of clients has been reached.\n+func IsMaxClientsError(err error) bool {\n+\treturn proto.IsMaxClientsError(err)\n+}\n+\n+// IsMovedError checks if an error is a Redis MOVED error, even if wrapped.\n+// MOVED errors occur in cluster mode when a key has been moved to a different node.\n+// Returns the address of the node where the key has been moved and a boolean indicating if it's a MOVED error.\n+func IsMovedError(err error) (addr string, ok bool) {\n+\tif movedErr, isMovedErr := proto.IsMovedError(err); isMovedErr {\n+\t\treturn movedErr.Addr(), true\n \t}\n-\treturn strings.HasSuffix(redisError, \" \"+addr)\n+\treturn \"\", false\n+}\n+\n+// IsAskError checks if an error is a Redis ASK error, even if wrapped.\n+// ASK errors occur in cluster mode when a key is being migrated and the client should ask another node.\n+// Returns the address of the node to ask and a boolean indicating if it's an ASK error.\n+func IsAskError(err error) (addr string, ok bool) {\n+\tif askErr, isAskErr := proto.IsAskError(err); isAskErr {\n+\t\treturn askErr.Addr(), true\n+\t}\n+\treturn \"\", false\n+}\n+\n+// IsAuthError checks if an error is a Redis authentication error, even if wrapped.\n+// Authentication errors occur when:\n+// - NOAUTH: Redis requires authentication but none was provided\n+// - WRONGPASS: Redis authentication failed due to incorrect password\n+// - unauthenticated: Error returned when password changed\n+func IsAuthError(err error) bool {\n+\treturn proto.IsAuthError(err)\n+}\n+\n+// IsPermissionError checks if an error is a Redis permission error, even if wrapped.\n+// Permission errors (NOPERM) occur when a user does not have permission to execute a command.\n+func IsPermissionError(err error) bool {\n+\treturn proto.IsPermissionError(err)\n+}\n+\n+// IsExecAbortError checks if an error is a Redis EXECABORT error, even if wrapped.\n+// EXECABORT errors occur when a transaction is aborted.\n+func IsExecAbortError(err error) bool {\n+\treturn proto.IsExecAbortError(err)\n+}\n+\n+// IsOOMError checks if an error is a Redis OOM (Out Of Memory) error, even if wrapped.\n+// OOM errors occur when Redis is out of memory.\n+func IsOOMError(err error) bool {\n+\treturn proto.IsOOMError(err)\n }\n \n //------------------------------------------------------------------------------"
    },
    {
      "sha": "e0d49a6b78f02da58418b0471d36791f0be28077",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/gears_commands.go",
      "status": "removed",
      "additions": 0,
      "deletions": 149,
      "changes": 149,
      "blob_url": "https://github.com/umputun/remark42/blob/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fgears_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fgears_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fgears_commands.go?ref=b4511427905c02e55730d7cb27036f5796decaa9",
      "patch": "@@ -1,149 +0,0 @@\n-package redis\n-\n-import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"strings\"\n-)\n-\n-type GearsCmdable interface {\n-\tTFunctionLoad(ctx context.Context, lib string) *StatusCmd\n-\tTFunctionLoadArgs(ctx context.Context, lib string, options *TFunctionLoadOptions) *StatusCmd\n-\tTFunctionDelete(ctx context.Context, libName string) *StatusCmd\n-\tTFunctionList(ctx context.Context) *MapStringInterfaceSliceCmd\n-\tTFunctionListArgs(ctx context.Context, options *TFunctionListOptions) *MapStringInterfaceSliceCmd\n-\tTFCall(ctx context.Context, libName string, funcName string, numKeys int) *Cmd\n-\tTFCallArgs(ctx context.Context, libName string, funcName string, numKeys int, options *TFCallOptions) *Cmd\n-\tTFCallASYNC(ctx context.Context, libName string, funcName string, numKeys int) *Cmd\n-\tTFCallASYNCArgs(ctx context.Context, libName string, funcName string, numKeys int, options *TFCallOptions) *Cmd\n-}\n-\n-type TFunctionLoadOptions struct {\n-\tReplace bool\n-\tConfig  string\n-}\n-\n-type TFunctionListOptions struct {\n-\tWithcode bool\n-\tVerbose  int\n-\tLibrary  string\n-}\n-\n-type TFCallOptions struct {\n-\tKeys      []string\n-\tArguments []string\n-}\n-\n-// TFunctionLoad - load a new JavaScript library into Redis.\n-// For more information - https://redis.io/commands/tfunction-load/\n-func (c cmdable) TFunctionLoad(ctx context.Context, lib string) *StatusCmd {\n-\targs := []interface{}{\"TFUNCTION\", \"LOAD\", lib}\n-\tcmd := NewStatusCmd(ctx, args...)\n-\t_ = c(ctx, cmd)\n-\treturn cmd\n-}\n-\n-func (c cmdable) TFunctionLoadArgs(ctx context.Context, lib string, options *TFunctionLoadOptions) *StatusCmd {\n-\targs := []interface{}{\"TFUNCTION\", \"LOAD\"}\n-\tif options != nil {\n-\t\tif options.Replace {\n-\t\t\targs = append(args, \"REPLACE\")\n-\t\t}\n-\t\tif options.Config != \"\" {\n-\t\t\targs = append(args, \"CONFIG\", options.Config)\n-\t\t}\n-\t}\n-\targs = append(args, lib)\n-\tcmd := NewStatusCmd(ctx, args...)\n-\t_ = c(ctx, cmd)\n-\treturn cmd\n-}\n-\n-// TFunctionDelete - delete a JavaScript library from Redis.\n-// For more information - https://redis.io/commands/tfunction-delete/\n-func (c cmdable) TFunctionDelete(ctx context.Context, libName string) *StatusCmd {\n-\targs := []interface{}{\"TFUNCTION\", \"DELETE\", libName}\n-\tcmd := NewStatusCmd(ctx, args...)\n-\t_ = c(ctx, cmd)\n-\treturn cmd\n-}\n-\n-// TFunctionList - list the functions with additional information about each function.\n-// For more information - https://redis.io/commands/tfunction-list/\n-func (c cmdable) TFunctionList(ctx context.Context) *MapStringInterfaceSliceCmd {\n-\targs := []interface{}{\"TFUNCTION\", \"LIST\"}\n-\tcmd := NewMapStringInterfaceSliceCmd(ctx, args...)\n-\t_ = c(ctx, cmd)\n-\treturn cmd\n-}\n-\n-func (c cmdable) TFunctionListArgs(ctx context.Context, options *TFunctionListOptions) *MapStringInterfaceSliceCmd {\n-\targs := []interface{}{\"TFUNCTION\", \"LIST\"}\n-\tif options != nil {\n-\t\tif options.Withcode {\n-\t\t\targs = append(args, \"WITHCODE\")\n-\t\t}\n-\t\tif options.Verbose != 0 {\n-\t\t\tv := strings.Repeat(\"v\", options.Verbose)\n-\t\t\targs = append(args, v)\n-\t\t}\n-\t\tif options.Library != \"\" {\n-\t\t\targs = append(args, \"LIBRARY\", options.Library)\n-\t\t}\n-\t}\n-\tcmd := NewMapStringInterfaceSliceCmd(ctx, args...)\n-\t_ = c(ctx, cmd)\n-\treturn cmd\n-}\n-\n-// TFCall - invoke a function.\n-// For more information - https://redis.io/commands/tfcall/\n-func (c cmdable) TFCall(ctx context.Context, libName string, funcName string, numKeys int) *Cmd {\n-\tlf := libName + \".\" + funcName\n-\targs := []interface{}{\"TFCALL\", lf, numKeys}\n-\tcmd := NewCmd(ctx, args...)\n-\t_ = c(ctx, cmd)\n-\treturn cmd\n-}\n-\n-func (c cmdable) TFCallArgs(ctx context.Context, libName string, funcName string, numKeys int, options *TFCallOptions) *Cmd {\n-\tlf := libName + \".\" + funcName\n-\targs := []interface{}{\"TFCALL\", lf, numKeys}\n-\tif options != nil {\n-\t\tfor _, key := range options.Keys {\n-\t\t\targs = append(args, key)\n-\t\t}\n-\t\tfor _, key := range options.Arguments {\n-\t\t\targs = append(args, key)\n-\t\t}\n-\t}\n-\tcmd := NewCmd(ctx, args...)\n-\t_ = c(ctx, cmd)\n-\treturn cmd\n-}\n-\n-// TFCallASYNC - invoke an asynchronous JavaScript function (coroutine).\n-// For more information - https://redis.io/commands/TFCallASYNC/\n-func (c cmdable) TFCallASYNC(ctx context.Context, libName string, funcName string, numKeys int) *Cmd {\n-\tlf := fmt.Sprintf(\"%s.%s\", libName, funcName)\n-\targs := []interface{}{\"TFCALLASYNC\", lf, numKeys}\n-\tcmd := NewCmd(ctx, args...)\n-\t_ = c(ctx, cmd)\n-\treturn cmd\n-}\n-\n-func (c cmdable) TFCallASYNCArgs(ctx context.Context, libName string, funcName string, numKeys int, options *TFCallOptions) *Cmd {\n-\tlf := fmt.Sprintf(\"%s.%s\", libName, funcName)\n-\targs := []interface{}{\"TFCALLASYNC\", lf, numKeys}\n-\tif options != nil {\n-\t\tfor _, key := range options.Keys {\n-\t\t\targs = append(args, key)\n-\t\t}\n-\t\tfor _, key := range options.Arguments {\n-\t\t\targs = append(args, key)\n-\t\t}\n-\t}\n-\tcmd := NewCmd(ctx, args...)\n-\t_ = c(ctx, cmd)\n-\treturn cmd\n-}"
    },
    {
      "sha": "c7100222cd8d0a17b7b983d0ecc82f47f2db425c",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/generic_commands.go",
      "status": "modified",
      "additions": 8,
      "deletions": 0,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fgeneric_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fgeneric_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fgeneric_commands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -3,6 +3,8 @@ package redis\n import (\n \t\"context\"\n \t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/internal/hashtag\"\n )\n \n type GenericCmdable interface {\n@@ -363,6 +365,9 @@ func (c cmdable) Scan(ctx context.Context, cursor uint64, match string, count in\n \t\targs = append(args, \"count\", count)\n \t}\n \tcmd := NewScanCmd(ctx, c, args...)\n+\tif hashtag.Present(match) {\n+\t\tcmd.SetFirstKeyPos(3)\n+\t}\n \t_ = c(ctx, cmd)\n \treturn cmd\n }\n@@ -379,6 +384,9 @@ func (c cmdable) ScanType(ctx context.Context, cursor uint64, match string, coun\n \t\targs = append(args, \"type\", keyType)\n \t}\n \tcmd := NewScanCmd(ctx, c, args...)\n+\tif hashtag.Present(match) {\n+\t\tcmd.SetFirstKeyPos(3)\n+\t}\n \t_ = c(ctx, cmd)\n \treturn cmd\n }"
    },
    {
      "sha": "b78860a5a1c0f00f2ec1e6d52d077a7873e8b344",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/hash_commands.go",
      "status": "modified",
      "additions": 180,
      "deletions": 11,
      "changes": 191,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fhash_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fhash_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fhash_commands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -3,26 +3,34 @@ package redis\n import (\n \t\"context\"\n \t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/internal/hashtag\"\n )\n \n type HashCmdable interface {\n \tHDel(ctx context.Context, key string, fields ...string) *IntCmd\n \tHExists(ctx context.Context, key, field string) *BoolCmd\n \tHGet(ctx context.Context, key, field string) *StringCmd\n \tHGetAll(ctx context.Context, key string) *MapStringStringCmd\n+\tHGetDel(ctx context.Context, key string, fields ...string) *StringSliceCmd\n+\tHGetEX(ctx context.Context, key string, fields ...string) *StringSliceCmd\n+\tHGetEXWithArgs(ctx context.Context, key string, options *HGetEXOptions, fields ...string) *StringSliceCmd\n \tHIncrBy(ctx context.Context, key, field string, incr int64) *IntCmd\n \tHIncrByFloat(ctx context.Context, key, field string, incr float64) *FloatCmd\n \tHKeys(ctx context.Context, key string) *StringSliceCmd\n \tHLen(ctx context.Context, key string) *IntCmd\n \tHMGet(ctx context.Context, key string, fields ...string) *SliceCmd\n \tHSet(ctx context.Context, key string, values ...interface{}) *IntCmd\n \tHMSet(ctx context.Context, key string, values ...interface{}) *BoolCmd\n+\tHSetEX(ctx context.Context, key string, fieldsAndValues ...string) *IntCmd\n+\tHSetEXWithArgs(ctx context.Context, key string, options *HSetEXOptions, fieldsAndValues ...string) *IntCmd\n \tHSetNX(ctx context.Context, key, field string, value interface{}) *BoolCmd\n \tHScan(ctx context.Context, key string, cursor uint64, match string, count int64) *ScanCmd\n \tHScanNoValues(ctx context.Context, key string, cursor uint64, match string, count int64) *ScanCmd\n \tHVals(ctx context.Context, key string) *StringSliceCmd\n \tHRandField(ctx context.Context, key string, count int) *StringSliceCmd\n \tHRandFieldWithValues(ctx context.Context, key string, count int) *KeyValueSliceCmd\n+\tHStrLen(ctx context.Context, key, field string) *IntCmd\n \tHExpire(ctx context.Context, key string, expiration time.Duration, fields ...string) *IntSliceCmd\n \tHExpireWithArgs(ctx context.Context, key string, expiration time.Duration, expirationArgs HExpireArgs, fields ...string) *IntSliceCmd\n \tHPExpire(ctx context.Context, key string, expiration time.Duration, fields ...string) *IntSliceCmd\n@@ -108,16 +116,16 @@ func (c cmdable) HMGet(ctx context.Context, key string, fields ...string) *Slice\n \n // HSet accepts values in following formats:\n //\n-//   - HSet(\"myhash\", \"key1\", \"value1\", \"key2\", \"value2\")\n+//   - HSet(ctx, \"myhash\", \"key1\", \"value1\", \"key2\", \"value2\")\n //\n-//   - HSet(\"myhash\", []string{\"key1\", \"value1\", \"key2\", \"value2\"})\n+//   - HSet(ctx, \"myhash\", []string{\"key1\", \"value1\", \"key2\", \"value2\"})\n //\n-//   - HSet(\"myhash\", map[string]interface{}{\"key1\": \"value1\", \"key2\": \"value2\"})\n+//   - HSet(ctx, \"myhash\", map[string]interface{}{\"key1\": \"value1\", \"key2\": \"value2\"})\n //\n //     Playing struct With \"redis\" tag.\n //     type MyHash struct { Key1 string `redis:\"key1\"`; Key2 int `redis:\"key2\"` }\n //\n-//   - HSet(\"myhash\", MyHash{\"value1\", \"value2\"}) Warn: redis-server >= 4.0\n+//   - HSet(ctx, \"myhash\", MyHash{\"value1\", \"value2\"}) Warn: redis-server >= 4.0\n //\n //     For struct, can be a structure pointer type, we only parse the field whose tag is redis.\n //     if you don't want the field to be read, you can use the `redis:\"-\"` flag to ignore it,\n@@ -186,10 +194,18 @@ func (c cmdable) HScan(ctx context.Context, key string, cursor uint64, match str\n \t\targs = append(args, \"count\", count)\n \t}\n \tcmd := NewScanCmd(ctx, c, args...)\n+\tif hashtag.Present(match) {\n+\t\tcmd.SetFirstKeyPos(4)\n+\t}\n \t_ = c(ctx, cmd)\n \treturn cmd\n }\n \n+func (c cmdable) HStrLen(ctx context.Context, key, field string) *IntCmd {\n+\tcmd := NewIntCmd(ctx, \"hstrlen\", key, field)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n func (c cmdable) HScanNoValues(ctx context.Context, key string, cursor uint64, match string, count int64) *ScanCmd {\n \targs := []interface{}{\"hscan\", key, cursor}\n \tif match != \"\" {\n@@ -200,6 +216,9 @@ func (c cmdable) HScanNoValues(ctx context.Context, key string, cursor uint64, m\n \t}\n \targs = append(args, \"novalues\")\n \tcmd := NewScanCmd(ctx, c, args...)\n+\tif hashtag.Present(match) {\n+\t\tcmd.SetFirstKeyPos(4)\n+\t}\n \t_ = c(ctx, cmd)\n \treturn cmd\n }\n@@ -213,7 +232,10 @@ type HExpireArgs struct {\n \n // HExpire - Sets the expiration time for specified fields in a hash in seconds.\n // The command constructs an argument list starting with \"HEXPIRE\", followed by the key, duration, any conditional flags, and the specified fields.\n-// For more information - https://redis.io/commands/hexpire/\n+// Available since Redis7.4 CE.\n+// For more information refer to [HEXPIRE Documentation].\n+//\n+// [HEXPIRE Documentation]: https://redis.io/commands/hexpire/\n func (c cmdable) HExpire(ctx context.Context, key string, expiration time.Duration, fields ...string) *IntSliceCmd {\n \targs := []interface{}{\"HEXPIRE\", key, formatSec(ctx, expiration), \"FIELDS\", len(fields)}\n \n@@ -228,7 +250,10 @@ func (c cmdable) HExpire(ctx context.Context, key string, expiration time.Durati\n // HExpireWithArgs - Sets the expiration time for specified fields in a hash in seconds.\n // It requires a key, an expiration duration, a struct with boolean flags for conditional expiration settings (NX, XX, GT, LT), and a list of fields.\n // The command constructs an argument list starting with \"HEXPIRE\", followed by the key, duration, any conditional flags, and the specified fields.\n-// For more information - https://redis.io/commands/hexpire/\n+// Available since Redis7.4 CE.\n+// For more information refer to [HEXPIRE Documentation].\n+//\n+// [HEXPIRE Documentation]: https://redis.io/commands/hexpire/\n func (c cmdable) HExpireWithArgs(ctx context.Context, key string, expiration time.Duration, expirationArgs HExpireArgs, fields ...string) *IntSliceCmd {\n \targs := []interface{}{\"HEXPIRE\", key, formatSec(ctx, expiration)}\n \n@@ -257,7 +282,10 @@ func (c cmdable) HExpireWithArgs(ctx context.Context, key string, expiration tim\n // HPExpire - Sets the expiration time for specified fields in a hash in milliseconds.\n // Similar to HExpire, it accepts a key, an expiration duration in milliseconds, a struct with expiration condition flags, and a list of fields.\n // The command modifies the standard time.Duration to milliseconds for the Redis command.\n-// For more information - https://redis.io/commands/hpexpire/\n+// Available since Redis7.4 CE.\n+// For more information refer to [HPEXPIRE Documentation].\n+//\n+// [HPEXPIRE Documentation]: https://redis.io/commands/hpexpire/\n func (c cmdable) HPExpire(ctx context.Context, key string, expiration time.Duration, fields ...string) *IntSliceCmd {\n \targs := []interface{}{\"HPEXPIRE\", key, formatMs(ctx, expiration), \"FIELDS\", len(fields)}\n \n@@ -269,6 +297,13 @@ func (c cmdable) HPExpire(ctx context.Context, key string, expiration time.Durat\n \treturn cmd\n }\n \n+// HPExpireWithArgs - Sets the expiration time for specified fields in a hash in milliseconds.\n+// It requires a key, an expiration duration, a struct with boolean flags for conditional expiration settings (NX, XX, GT, LT), and a list of fields.\n+// The command constructs an argument list starting with \"HPEXPIRE\", followed by the key, duration, any conditional flags, and the specified fields.\n+// Available since Redis7.4 CE.\n+// For more information refer to [HPEXPIRE Documentation].\n+//\n+// [HPEXPIRE Documentation]: https://redis.io/commands/hpexpire/\n func (c cmdable) HPExpireWithArgs(ctx context.Context, key string, expiration time.Duration, expirationArgs HExpireArgs, fields ...string) *IntSliceCmd {\n \targs := []interface{}{\"HPEXPIRE\", key, formatMs(ctx, expiration)}\n \n@@ -297,7 +332,10 @@ func (c cmdable) HPExpireWithArgs(ctx context.Context, key string, expiration ti\n // HExpireAt - Sets the expiration time for specified fields in a hash to a UNIX timestamp in seconds.\n // Takes a key, a UNIX timestamp, a struct of conditional flags, and a list of fields.\n // The command sets absolute expiration times based on the UNIX timestamp provided.\n-// For more information - https://redis.io/commands/hexpireat/\n+// Available since Redis7.4 CE.\n+// For more information refer to [HExpireAt Documentation].\n+//\n+// [HExpireAt Documentation]: https://redis.io/commands/hexpireat/\n func (c cmdable) HExpireAt(ctx context.Context, key string, tm time.Time, fields ...string) *IntSliceCmd {\n \n \targs := []interface{}{\"HEXPIREAT\", key, tm.Unix(), \"FIELDS\", len(fields)}\n@@ -337,7 +375,10 @@ func (c cmdable) HExpireAtWithArgs(ctx context.Context, key string, tm time.Time\n \n // HPExpireAt - Sets the expiration time for specified fields in a hash to a UNIX timestamp in milliseconds.\n // Similar to HExpireAt but for timestamps in milliseconds. It accepts the same parameters and adjusts the UNIX time to milliseconds.\n-// For more information - https://redis.io/commands/hpexpireat/\n+// Available since Redis 7.4 CE.\n+// For more information refer to [HExpireAt Documentation].\n+//\n+// [HExpireAt Documentation]: https://redis.io/commands/hexpireat/\n func (c cmdable) HPExpireAt(ctx context.Context, key string, tm time.Time, fields ...string) *IntSliceCmd {\n \targs := []interface{}{\"HPEXPIREAT\", key, tm.UnixNano() / int64(time.Millisecond), \"FIELDS\", len(fields)}\n \n@@ -377,7 +418,10 @@ func (c cmdable) HPExpireAtWithArgs(ctx context.Context, key string, tm time.Tim\n // HPersist - Removes the expiration time from specified fields in a hash.\n // Accepts a key and the fields themselves.\n // This command ensures that each field specified will have its expiration removed if present.\n-// For more information - https://redis.io/commands/hpersist/\n+// Available since Redis7.4 CE.\n+// For more information refer to [HPersist Documentation].\n+//\n+// [HPersist Documentation]: https://redis.io/commands/hpersist/\n func (c cmdable) HPersist(ctx context.Context, key string, fields ...string) *IntSliceCmd {\n \targs := []interface{}{\"HPERSIST\", key, \"FIELDS\", len(fields)}\n \n@@ -392,6 +436,10 @@ func (c cmdable) HPersist(ctx context.Context, key string, fields ...string) *In\n // HExpireTime - Retrieves the expiration time for specified fields in a hash as a UNIX timestamp in seconds.\n // Requires a key and the fields themselves to fetch their expiration timestamps.\n // This command returns the expiration times for each field or error/status codes for each field as specified.\n+// Available since Redis7.4 CE.\n+// For more information refer to [HExpireTime Documentation].\n+//\n+// [HExpireTime Documentation]: https://redis.io/commands/hexpiretime/\n // For more information - https://redis.io/commands/hexpiretime/\n func (c cmdable) HExpireTime(ctx context.Context, key string, fields ...string) *IntSliceCmd {\n \targs := []interface{}{\"HEXPIRETIME\", key, \"FIELDS\", len(fields)}\n@@ -407,6 +455,10 @@ func (c cmdable) HExpireTime(ctx context.Context, key string, fields ...string)\n // HPExpireTime - Retrieves the expiration time for specified fields in a hash as a UNIX timestamp in milliseconds.\n // Similar to HExpireTime, adjusted for timestamps in milliseconds. It requires the same parameters.\n // Provides the expiration timestamp for each field in milliseconds.\n+// Available since Redis7.4 CE.\n+// For more information refer to [HExpireTime Documentation].\n+//\n+// [HExpireTime Documentation]: https://redis.io/commands/hexpiretime/\n // For more information - https://redis.io/commands/hexpiretime/\n func (c cmdable) HPExpireTime(ctx context.Context, key string, fields ...string) *IntSliceCmd {\n \targs := []interface{}{\"HPEXPIRETIME\", key, \"FIELDS\", len(fields)}\n@@ -422,7 +474,10 @@ func (c cmdable) HPExpireTime(ctx context.Context, key string, fields ...string)\n // HTTL - Retrieves the remaining time to live for specified fields in a hash in seconds.\n // Requires a key and the fields themselves. It returns the TTL for each specified field.\n // This command fetches the TTL in seconds for each field or returns error/status codes as appropriate.\n-// For more information - https://redis.io/commands/httl/\n+// Available since Redis7.4 CE.\n+// For more information refer to [HTTL Documentation].\n+//\n+// [HTTL Documentation]: https://redis.io/commands/httl/\n func (c cmdable) HTTL(ctx context.Context, key string, fields ...string) *IntSliceCmd {\n \targs := []interface{}{\"HTTL\", key, \"FIELDS\", len(fields)}\n \n@@ -437,6 +492,10 @@ func (c cmdable) HTTL(ctx context.Context, key string, fields ...string) *IntSli\n // HPTTL - Retrieves the remaining time to live for specified fields in a hash in milliseconds.\n // Similar to HTTL, but returns the TTL in milliseconds. It requires a key and the specified fields.\n // This command provides the TTL in milliseconds for each field or returns error/status codes as needed.\n+// Available since Redis7.4 CE.\n+// For more information refer to [HPTTL Documentation].\n+//\n+// [HPTTL Documentation]: https://redis.io/commands/hpttl/\n // For more information - https://redis.io/commands/hpttl/\n func (c cmdable) HPTTL(ctx context.Context, key string, fields ...string) *IntSliceCmd {\n \targs := []interface{}{\"HPTTL\", key, \"FIELDS\", len(fields)}\n@@ -448,3 +507,113 @@ func (c cmdable) HPTTL(ctx context.Context, key string, fields ...string) *IntSl\n \t_ = c(ctx, cmd)\n \treturn cmd\n }\n+\n+func (c cmdable) HGetDel(ctx context.Context, key string, fields ...string) *StringSliceCmd {\n+\targs := []interface{}{\"HGETDEL\", key, \"FIELDS\", len(fields)}\n+\tfor _, field := range fields {\n+\t\targs = append(args, field)\n+\t}\n+\tcmd := NewStringSliceCmd(ctx, args...)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) HGetEX(ctx context.Context, key string, fields ...string) *StringSliceCmd {\n+\targs := []interface{}{\"HGETEX\", key, \"FIELDS\", len(fields)}\n+\tfor _, field := range fields {\n+\t\targs = append(args, field)\n+\t}\n+\tcmd := NewStringSliceCmd(ctx, args...)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+// HGetEXExpirationType represents an expiration option for the HGETEX command.\n+type HGetEXExpirationType string\n+\n+const (\n+\tHGetEXExpirationEX      HGetEXExpirationType = \"EX\"\n+\tHGetEXExpirationPX      HGetEXExpirationType = \"PX\"\n+\tHGetEXExpirationEXAT    HGetEXExpirationType = \"EXAT\"\n+\tHGetEXExpirationPXAT    HGetEXExpirationType = \"PXAT\"\n+\tHGetEXExpirationPERSIST HGetEXExpirationType = \"PERSIST\"\n+)\n+\n+type HGetEXOptions struct {\n+\tExpirationType HGetEXExpirationType\n+\tExpirationVal  int64\n+}\n+\n+func (c cmdable) HGetEXWithArgs(ctx context.Context, key string, options *HGetEXOptions, fields ...string) *StringSliceCmd {\n+\targs := []interface{}{\"HGETEX\", key}\n+\tif options.ExpirationType != \"\" {\n+\t\targs = append(args, string(options.ExpirationType))\n+\t\tif options.ExpirationType != HGetEXExpirationPERSIST {\n+\t\t\targs = append(args, options.ExpirationVal)\n+\t\t}\n+\t}\n+\n+\targs = append(args, \"FIELDS\", len(fields))\n+\tfor _, field := range fields {\n+\t\targs = append(args, field)\n+\t}\n+\n+\tcmd := NewStringSliceCmd(ctx, args...)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+type HSetEXCondition string\n+\n+const (\n+\tHSetEXFNX HSetEXCondition = \"FNX\" // Only set the fields if none of them already exist.\n+\tHSetEXFXX HSetEXCondition = \"FXX\" // Only set the fields if all already exist.\n+)\n+\n+type HSetEXExpirationType string\n+\n+const (\n+\tHSetEXExpirationEX      HSetEXExpirationType = \"EX\"\n+\tHSetEXExpirationPX      HSetEXExpirationType = \"PX\"\n+\tHSetEXExpirationEXAT    HSetEXExpirationType = \"EXAT\"\n+\tHSetEXExpirationPXAT    HSetEXExpirationType = \"PXAT\"\n+\tHSetEXExpirationKEEPTTL HSetEXExpirationType = \"KEEPTTL\"\n+)\n+\n+type HSetEXOptions struct {\n+\tCondition      HSetEXCondition\n+\tExpirationType HSetEXExpirationType\n+\tExpirationVal  int64\n+}\n+\n+func (c cmdable) HSetEX(ctx context.Context, key string, fieldsAndValues ...string) *IntCmd {\n+\targs := []interface{}{\"HSETEX\", key, \"FIELDS\", len(fieldsAndValues) / 2}\n+\tfor _, field := range fieldsAndValues {\n+\t\targs = append(args, field)\n+\t}\n+\n+\tcmd := NewIntCmd(ctx, args...)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}\n+\n+func (c cmdable) HSetEXWithArgs(ctx context.Context, key string, options *HSetEXOptions, fieldsAndValues ...string) *IntCmd {\n+\targs := []interface{}{\"HSETEX\", key}\n+\tif options.Condition != \"\" {\n+\t\targs = append(args, string(options.Condition))\n+\t}\n+\tif options.ExpirationType != \"\" {\n+\t\targs = append(args, string(options.ExpirationType))\n+\t\tif options.ExpirationType != HSetEXExpirationKEEPTTL {\n+\t\t\targs = append(args, options.ExpirationVal)\n+\t\t}\n+\t}\n+\targs = append(args, \"FIELDS\", len(fieldsAndValues)/2)\n+\tfor _, field := range fieldsAndValues {\n+\t\targs = append(args, field)\n+\t}\n+\n+\tcmd := NewIntCmd(ctx, args...)\n+\t_ = c(ctx, cmd)\n+\treturn cmd\n+}"
    },
    {
      "sha": "22bfedf713b647bf20da26ef99cfb79795b599f7",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/auth/streaming/conn_reauth_credentials_listener.go",
      "status": "added",
      "additions": 100,
      "deletions": 0,
      "changes": 100,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fconn_reauth_credentials_listener.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fconn_reauth_credentials_listener.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fconn_reauth_credentials_listener.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,100 @@\n+package streaming\n+\n+import (\n+\t\"github.com/redis/go-redis/v9/auth\"\n+\t\"github.com/redis/go-redis/v9/internal/pool\"\n+)\n+\n+// ConnReAuthCredentialsListener is a credentials listener for a specific connection\n+// that triggers re-authentication when credentials change.\n+//\n+// This listener implements the auth.CredentialsListener interface and is subscribed\n+// to a StreamingCredentialsProvider. When new credentials are received via OnNext,\n+// it marks the connection for re-authentication through the manager.\n+//\n+// The re-authentication is always performed asynchronously to avoid blocking the\n+// credentials provider and to prevent potential deadlocks with the pool semaphore.\n+// The actual re-auth happens when the connection is returned to the pool in an idle state.\n+//\n+// Lifecycle:\n+//   - Created during connection initialization via Manager.Listener()\n+//   - Subscribed to the StreamingCredentialsProvider\n+//   - Receives credential updates via OnNext()\n+//   - Cleaned up when connection is removed from pool via Manager.RemoveListener()\n+type ConnReAuthCredentialsListener struct {\n+\t// reAuth is the function to re-authenticate the connection with new credentials\n+\treAuth func(conn *pool.Conn, credentials auth.Credentials) error\n+\n+\t// onErr is the function to call when re-authentication or acquisition fails\n+\tonErr func(conn *pool.Conn, err error)\n+\n+\t// conn is the connection this listener is associated with\n+\tconn *pool.Conn\n+\n+\t// manager is the streaming credentials manager for coordinating re-auth\n+\tmanager *Manager\n+}\n+\n+// OnNext is called when new credentials are received from the StreamingCredentialsProvider.\n+//\n+// This method marks the connection for asynchronous re-authentication. The actual\n+// re-authentication happens in the background when the connection is returned to the\n+// pool and is in an idle state.\n+//\n+// Asynchronous re-auth is used to:\n+//   - Avoid blocking the credentials provider's notification goroutine\n+//   - Prevent deadlocks with the pool's semaphore (especially with small pool sizes)\n+//   - Ensure re-auth happens when the connection is safe to use (not processing commands)\n+//\n+// The reAuthFn callback receives:\n+//   - nil if the connection was successfully acquired for re-auth\n+//   - error if acquisition timed out or failed\n+//\n+// Thread-safe: Called by the credentials provider's notification goroutine.\n+func (c *ConnReAuthCredentialsListener) OnNext(credentials auth.Credentials) {\n+\tif c.conn == nil || c.conn.IsClosed() || c.manager == nil || c.reAuth == nil {\n+\t\treturn\n+\t}\n+\n+\t// Always use async reauth to avoid complex pool semaphore issues\n+\t// The synchronous path can cause deadlocks in the pool's semaphore mechanism\n+\t// when called from the Subscribe goroutine, especially with small pool sizes.\n+\t// The connection pool hook will re-authenticate the connection when it is\n+\t// returned to the pool in a clean, idle state.\n+\tc.manager.MarkForReAuth(c.conn, func(err error) {\n+\t\t// err is from connection acquisition (timeout, etc.)\n+\t\tif err != nil {\n+\t\t\t// Log the error\n+\t\t\tc.OnError(err)\n+\t\t\treturn\n+\t\t}\n+\t\t// err is from reauth command execution\n+\t\terr = c.reAuth(c.conn, credentials)\n+\t\tif err != nil {\n+\t\t\t// Log the error\n+\t\t\tc.OnError(err)\n+\t\t\treturn\n+\t\t}\n+\t})\n+}\n+\n+// OnError is called when an error occurs during credential streaming or re-authentication.\n+//\n+// This method can be called from:\n+//   - The StreamingCredentialsProvider when there's an error in the credentials stream\n+//   - The re-auth process when connection acquisition times out\n+//   - The re-auth process when the AUTH command fails\n+//\n+// The error is delegated to the onErr callback provided during listener creation.\n+//\n+// Thread-safe: Can be called from multiple goroutines (provider, re-auth worker).\n+func (c *ConnReAuthCredentialsListener) OnError(err error) {\n+\tif c.onErr == nil {\n+\t\treturn\n+\t}\n+\n+\tc.onErr(c.conn, err)\n+}\n+\n+// Ensure ConnReAuthCredentialsListener implements the CredentialsListener interface.\n+var _ auth.CredentialsListener = (*ConnReAuthCredentialsListener)(nil)"
    },
    {
      "sha": "66e6eafdce8f56b028d4c19e665744104a65afd0",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/auth/streaming/cred_listeners.go",
      "status": "added",
      "additions": 77,
      "deletions": 0,
      "changes": 77,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fcred_listeners.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fcred_listeners.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fcred_listeners.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,77 @@\n+package streaming\n+\n+import (\n+\t\"sync\"\n+\n+\t\"github.com/redis/go-redis/v9/auth\"\n+)\n+\n+// CredentialsListeners is a thread-safe collection of credentials listeners\n+// indexed by connection ID.\n+//\n+// This collection is used by the Manager to maintain a registry of listeners\n+// for each connection in the pool. Listeners are reused when connections are\n+// reinitialized (e.g., after a handoff) to avoid creating duplicate subscriptions\n+// to the StreamingCredentialsProvider.\n+//\n+// The collection supports concurrent access from multiple goroutines during\n+// connection initialization, credential updates, and connection removal.\n+type CredentialsListeners struct {\n+\t// listeners maps connection ID to credentials listener\n+\tlisteners map[uint64]auth.CredentialsListener\n+\n+\t// lock protects concurrent access to the listeners map\n+\tlock sync.RWMutex\n+}\n+\n+// NewCredentialsListeners creates a new thread-safe credentials listeners collection.\n+func NewCredentialsListeners() *CredentialsListeners {\n+\treturn &CredentialsListeners{\n+\t\tlisteners: make(map[uint64]auth.CredentialsListener),\n+\t}\n+}\n+\n+// Add adds or updates a credentials listener for a connection.\n+//\n+// If a listener already exists for the connection ID, it is replaced.\n+// This is safe because the old listener should have been unsubscribed\n+// before the connection was reinitialized.\n+//\n+// Thread-safe: Can be called concurrently from multiple goroutines.\n+func (c *CredentialsListeners) Add(connID uint64, listener auth.CredentialsListener) {\n+\tc.lock.Lock()\n+\tdefer c.lock.Unlock()\n+\tif c.listeners == nil {\n+\t\tc.listeners = make(map[uint64]auth.CredentialsListener)\n+\t}\n+\tc.listeners[connID] = listener\n+}\n+\n+// Get retrieves the credentials listener for a connection.\n+//\n+// Returns:\n+//   - listener: The credentials listener for the connection, or nil if not found\n+//   - ok: true if a listener exists for the connection ID, false otherwise\n+//\n+// Thread-safe: Can be called concurrently from multiple goroutines.\n+func (c *CredentialsListeners) Get(connID uint64) (auth.CredentialsListener, bool) {\n+\tc.lock.RLock()\n+\tdefer c.lock.RUnlock()\n+\tif len(c.listeners) == 0 {\n+\t\treturn nil, false\n+\t}\n+\tlistener, ok := c.listeners[connID]\n+\treturn listener, ok\n+}\n+\n+// Remove removes the credentials listener for a connection.\n+//\n+// This is called when a connection is removed from the pool to prevent\n+// memory leaks. If no listener exists for the connection ID, this is a no-op.\n+//\n+// Thread-safe: Can be called concurrently from multiple goroutines.\n+func (c *CredentialsListeners) Remove(connID uint64) {\n+\tc.lock.Lock()\n+\tdefer c.lock.Unlock()\n+\tdelete(c.listeners, connID)\n+}"
    },
    {
      "sha": "f785927ee32cea3511819711bd23d76e55fcc8e4",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/auth/streaming/manager.go",
      "status": "added",
      "additions": 137,
      "deletions": 0,
      "changes": 137,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fmanager.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fmanager.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fmanager.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,137 @@\n+package streaming\n+\n+import (\n+\t\"errors\"\n+\t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/auth\"\n+\t\"github.com/redis/go-redis/v9/internal/pool\"\n+)\n+\n+// Manager coordinates streaming credentials and re-authentication for a connection pool.\n+//\n+// The manager is responsible for:\n+//   - Creating and managing per-connection credentials listeners\n+//   - Providing the pool hook for re-authentication\n+//   - Coordinating between credentials updates and pool operations\n+//\n+// When credentials change via a StreamingCredentialsProvider:\n+//  1. The credentials listener (ConnReAuthCredentialsListener) receives the update\n+//  2. It calls MarkForReAuth on the manager\n+//  3. The manager delegates to the pool hook\n+//  4. The pool hook schedules background re-authentication\n+//\n+// The manager maintains a registry of credentials listeners indexed by connection ID,\n+// allowing listener reuse when connections are reinitialized (e.g., after handoff).\n+type Manager struct {\n+\t// credentialsListeners maps connection ID to credentials listener\n+\tcredentialsListeners *CredentialsListeners\n+\n+\t// pool is the connection pool being managed\n+\tpool pool.Pooler\n+\n+\t// poolHookRef is the re-authentication pool hook\n+\tpoolHookRef *ReAuthPoolHook\n+}\n+\n+// NewManager creates a new streaming credentials manager.\n+//\n+// Parameters:\n+//   - pl: The connection pool to manage\n+//   - reAuthTimeout: Maximum time to wait for acquiring a connection for re-authentication\n+//\n+// The manager creates a ReAuthPoolHook sized to match the pool size, ensuring that\n+// re-auth operations don't exhaust the connection pool.\n+func NewManager(pl pool.Pooler, reAuthTimeout time.Duration) *Manager {\n+\tm := &Manager{\n+\t\tpool:                 pl,\n+\t\tpoolHookRef:          NewReAuthPoolHook(pl.Size(), reAuthTimeout),\n+\t\tcredentialsListeners: NewCredentialsListeners(),\n+\t}\n+\tm.poolHookRef.manager = m\n+\treturn m\n+}\n+\n+// PoolHook returns the pool hook for re-authentication.\n+//\n+// This hook should be registered with the connection pool to enable\n+// automatic re-authentication when credentials change.\n+func (m *Manager) PoolHook() pool.PoolHook {\n+\treturn m.poolHookRef\n+}\n+\n+// Listener returns or creates a credentials listener for a connection.\n+//\n+// This method is called during connection initialization to set up the\n+// credentials listener. If a listener already exists for the connection ID\n+// (e.g., after a handoff), it is reused.\n+//\n+// Parameters:\n+//   - poolCn: The connection to create/get a listener for\n+//   - reAuth: Function to re-authenticate the connection with new credentials\n+//   - onErr: Function to call when re-authentication fails\n+//\n+// Returns:\n+//   - auth.CredentialsListener: The listener to subscribe to the credentials provider\n+//   - error: Non-nil if poolCn is nil\n+//\n+// Note: The reAuth and onErr callbacks are captured once when the listener is\n+// created and reused for the connection's lifetime. They should not change.\n+//\n+// Thread-safe: Can be called concurrently during connection initialization.\n+func (m *Manager) Listener(\n+\tpoolCn *pool.Conn,\n+\treAuth func(*pool.Conn, auth.Credentials) error,\n+\tonErr func(*pool.Conn, error),\n+) (auth.CredentialsListener, error) {\n+\tif poolCn == nil {\n+\t\treturn nil, errors.New(\"poolCn cannot be nil\")\n+\t}\n+\tconnID := poolCn.GetID()\n+\t// if we reconnect the underlying network connection, the streaming credentials listener will continue to work\n+\t// so we can get the old listener from the cache and use it.\n+\t// subscribing the same (an already subscribed) listener for a StreamingCredentialsProvider SHOULD be a no-op\n+\tlistener, ok := m.credentialsListeners.Get(connID)\n+\tif !ok || listener == nil {\n+\t\t// Create new listener for this connection\n+\t\t// Note: Callbacks (reAuth, onErr) are captured once and reused for the connection's lifetime\n+\t\tnewCredListener := &ConnReAuthCredentialsListener{\n+\t\t\tconn:    poolCn,\n+\t\t\treAuth:  reAuth,\n+\t\t\tonErr:   onErr,\n+\t\t\tmanager: m,\n+\t\t}\n+\n+\t\tm.credentialsListeners.Add(connID, newCredListener)\n+\t\tlistener = newCredListener\n+\t}\n+\treturn listener, nil\n+}\n+\n+// MarkForReAuth marks a connection for re-authentication.\n+//\n+// This method is called by the credentials listener when new credentials are\n+// received. It delegates to the pool hook to schedule background re-authentication.\n+//\n+// Parameters:\n+//   - poolCn: The connection to re-authenticate\n+//   - reAuthFn: Function to call for re-authentication, receives error if acquisition fails\n+//\n+// Thread-safe: Called by credentials listeners when credentials change.\n+func (m *Manager) MarkForReAuth(poolCn *pool.Conn, reAuthFn func(error)) {\n+\tconnID := poolCn.GetID()\n+\tm.poolHookRef.MarkForReAuth(connID, reAuthFn)\n+}\n+\n+// RemoveListener removes the credentials listener for a connection.\n+//\n+// This method is called by the pool hook's OnRemove to clean up listeners\n+// when connections are removed from the pool.\n+//\n+// Parameters:\n+//   - connID: The connection ID whose listener should be removed\n+//\n+// Thread-safe: Called during connection removal.\n+func (m *Manager) RemoveListener(connID uint64) {\n+\tm.credentialsListeners.Remove(connID)\n+}"
    },
    {
      "sha": "aaf4f6099f76f949e80eabb86855d8d9296c78fc",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/auth/streaming/pool_hook.go",
      "status": "added",
      "additions": 241,
      "deletions": 0,
      "changes": 241,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fpool_hook.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fpool_hook.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fauth%2Fstreaming%2Fpool_hook.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,241 @@\n+package streaming\n+\n+import (\n+\t\"context\"\n+\t\"sync\"\n+\t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/pool\"\n+)\n+\n+// ReAuthPoolHook is a pool hook that manages background re-authentication of connections\n+// when credentials change via a streaming credentials provider.\n+//\n+// The hook uses a semaphore-based worker pool to limit concurrent re-authentication\n+// operations and prevent pool exhaustion. When credentials change, connections are\n+// marked for re-authentication and processed asynchronously in the background.\n+//\n+// The re-authentication process:\n+//  1. OnPut: When a connection is returned to the pool, check if it needs re-auth\n+//  2. If yes, schedule it for background processing (move from shouldReAuth to scheduledReAuth)\n+//  3. A worker goroutine acquires the connection (waits until it's not in use)\n+//  4. Executes the re-auth function while holding the connection\n+//  5. Releases the connection back to the pool\n+//\n+// The hook ensures that:\n+//   - Only one re-auth operation runs per connection at a time\n+//   - Connections are not used for commands during re-authentication\n+//   - Re-auth operations timeout if they can't acquire the connection\n+//   - Resources are properly cleaned up on connection removal\n+type ReAuthPoolHook struct {\n+\t// shouldReAuth maps connection ID to re-auth function\n+\t// Connections in this map need re-authentication but haven't been scheduled yet\n+\tshouldReAuth     map[uint64]func(error)\n+\tshouldReAuthLock sync.RWMutex\n+\n+\t// workers is a semaphore limiting concurrent re-auth operations\n+\t// Initialized with poolSize tokens to prevent pool exhaustion\n+\t// Uses FastSemaphore for better performance with eventual fairness\n+\tworkers *internal.FastSemaphore\n+\n+\t// reAuthTimeout is the maximum time to wait for acquiring a connection for re-auth\n+\treAuthTimeout time.Duration\n+\n+\t// scheduledReAuth maps connection ID to scheduled status\n+\t// Connections in this map have a background worker attempting re-authentication\n+\tscheduledReAuth map[uint64]bool\n+\tscheduledLock   sync.RWMutex\n+\n+\t// manager is a back-reference for cleanup operations\n+\tmanager *Manager\n+}\n+\n+// NewReAuthPoolHook creates a new re-authentication pool hook.\n+//\n+// Parameters:\n+//   - poolSize: Maximum number of concurrent re-auth operations (typically matches pool size)\n+//   - reAuthTimeout: Maximum time to wait for acquiring a connection for re-authentication\n+//\n+// The poolSize parameter is used to initialize the worker semaphore, ensuring that\n+// re-auth operations don't exhaust the connection pool.\n+func NewReAuthPoolHook(poolSize int, reAuthTimeout time.Duration) *ReAuthPoolHook {\n+\treturn &ReAuthPoolHook{\n+\t\tshouldReAuth:    make(map[uint64]func(error)),\n+\t\tscheduledReAuth: make(map[uint64]bool),\n+\t\tworkers:         internal.NewFastSemaphore(int32(poolSize)),\n+\t\treAuthTimeout:   reAuthTimeout,\n+\t}\n+}\n+\n+// MarkForReAuth marks a connection for re-authentication.\n+//\n+// This method is called when credentials change and a connection needs to be\n+// re-authenticated. The actual re-authentication happens asynchronously when\n+// the connection is returned to the pool (in OnPut).\n+//\n+// Parameters:\n+//   - connID: The connection ID to mark for re-authentication\n+//   - reAuthFn: Function to call for re-authentication, receives error if acquisition fails\n+//\n+// Thread-safe: Can be called concurrently from multiple goroutines.\n+func (r *ReAuthPoolHook) MarkForReAuth(connID uint64, reAuthFn func(error)) {\n+\tr.shouldReAuthLock.Lock()\n+\tdefer r.shouldReAuthLock.Unlock()\n+\tr.shouldReAuth[connID] = reAuthFn\n+}\n+\n+// OnGet is called when a connection is retrieved from the pool.\n+//\n+// This hook checks if the connection needs re-authentication or has a scheduled\n+// re-auth operation. If so, it rejects the connection (returns accept=false),\n+// causing the pool to try another connection.\n+//\n+// Returns:\n+//   - accept: false if connection needs re-auth, true otherwise\n+//   - err: always nil (errors are not used in this hook)\n+//\n+// Thread-safe: Called concurrently by multiple goroutines getting connections.\n+func (r *ReAuthPoolHook) OnGet(_ context.Context, conn *pool.Conn, _ bool) (accept bool, err error) {\n+\tconnID := conn.GetID()\n+\tr.shouldReAuthLock.RLock()\n+\t_, shouldReAuth := r.shouldReAuth[connID]\n+\tr.shouldReAuthLock.RUnlock()\n+\t// This connection was marked for reauth while in the pool,\n+\t// reject the connection\n+\tif shouldReAuth {\n+\t\t// simply reject the connection, it will be re-authenticated in OnPut\n+\t\treturn false, nil\n+\t}\n+\tr.scheduledLock.RLock()\n+\t_, hasScheduled := r.scheduledReAuth[connID]\n+\tr.scheduledLock.RUnlock()\n+\t// has scheduled reauth, reject the connection\n+\tif hasScheduled {\n+\t\t// simply reject the connection, it currently has a reauth scheduled\n+\t\t// and the worker is waiting for slot to execute the reauth\n+\t\treturn false, nil\n+\t}\n+\treturn true, nil\n+}\n+\n+// OnPut is called when a connection is returned to the pool.\n+//\n+// This hook checks if the connection needs re-authentication. If so, it schedules\n+// a background goroutine to perform the re-auth asynchronously. The goroutine:\n+//  1. Waits for a worker slot (semaphore)\n+//  2. Acquires the connection (waits until not in use)\n+//  3. Executes the re-auth function\n+//  4. Releases the connection and worker slot\n+//\n+// The connection is always pooled (not removed) since re-auth happens in background.\n+//\n+// Returns:\n+//   - shouldPool: always true (connection stays in pool during background re-auth)\n+//   - shouldRemove: always false\n+//   - err: always nil\n+//\n+// Thread-safe: Called concurrently by multiple goroutines returning connections.\n+func (r *ReAuthPoolHook) OnPut(_ context.Context, conn *pool.Conn) (bool, bool, error) {\n+\tif conn == nil {\n+\t\t// noop\n+\t\treturn true, false, nil\n+\t}\n+\tconnID := conn.GetID()\n+\t// Check if reauth is needed and get the function with proper locking\n+\tr.shouldReAuthLock.RLock()\n+\treAuthFn, ok := r.shouldReAuth[connID]\n+\tr.shouldReAuthLock.RUnlock()\n+\n+\tif ok {\n+\t\t// Acquire both locks to atomically move from shouldReAuth to scheduledReAuth\n+\t\t// This prevents race conditions where OnGet might miss the transition\n+\t\tr.shouldReAuthLock.Lock()\n+\t\tr.scheduledLock.Lock()\n+\t\tr.scheduledReAuth[connID] = true\n+\t\tdelete(r.shouldReAuth, connID)\n+\t\tr.scheduledLock.Unlock()\n+\t\tr.shouldReAuthLock.Unlock()\n+\t\tgo func() {\n+\t\t\tr.workers.AcquireBlocking()\n+\t\t\t// safety first\n+\t\t\tif conn == nil || (conn != nil && conn.IsClosed()) {\n+\t\t\t\tr.workers.Release()\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tdefer func() {\n+\t\t\t\tif rec := recover(); rec != nil {\n+\t\t\t\t\t// once again - safety first\n+\t\t\t\t\tinternal.Logger.Printf(context.Background(), \"panic in reauth worker: %v\", rec)\n+\t\t\t\t}\n+\t\t\t\tr.scheduledLock.Lock()\n+\t\t\t\tdelete(r.scheduledReAuth, connID)\n+\t\t\t\tr.scheduledLock.Unlock()\n+\t\t\t\tr.workers.Release()\n+\t\t\t}()\n+\n+\t\t\t// Create timeout context for connection acquisition\n+\t\t\t// This prevents indefinite waiting if the connection is stuck\n+\t\t\tctx, cancel := context.WithTimeout(context.Background(), r.reAuthTimeout)\n+\t\t\tdefer cancel()\n+\n+\t\t\t// Try to acquire the connection for re-authentication\n+\t\t\t// We need to ensure the connection is IDLE (not IN_USE) before transitioning to UNUSABLE\n+\t\t\t// This prevents re-authentication from interfering with active commands\n+\t\t\t// Use AwaitAndTransition to wait for the connection to become IDLE\n+\t\t\tstateMachine := conn.GetStateMachine()\n+\t\t\tif stateMachine == nil {\n+\t\t\t\t// No state machine - should not happen, but handle gracefully\n+\t\t\t\treAuthFn(pool.ErrConnUnusableTimeout)\n+\t\t\t\treturn\n+\t\t\t}\n+\n+\t\t\t// Use predefined slice to avoid allocation\n+\t\t\t_, err := stateMachine.AwaitAndTransition(ctx, pool.ValidFromIdle(), pool.StateUnusable)\n+\t\t\tif err != nil {\n+\t\t\t\t// Timeout or other error occurred, cannot acquire connection\n+\t\t\t\treAuthFn(err)\n+\t\t\t\treturn\n+\t\t\t}\n+\n+\t\t\t// safety first\n+\t\t\tif !conn.IsClosed() {\n+\t\t\t\t// Successfully acquired the connection, perform reauth\n+\t\t\t\treAuthFn(nil)\n+\t\t\t}\n+\n+\t\t\t// Release the connection: transition from UNUSABLE back to IDLE\n+\t\t\tstateMachine.Transition(pool.StateIdle)\n+\t\t}()\n+\t}\n+\n+\t// the reauth will happen in background, as far as the pool is concerned:\n+\t// pool the connection, don't remove it, no error\n+\treturn true, false, nil\n+}\n+\n+// OnRemove is called when a connection is removed from the pool.\n+//\n+// This hook cleans up all state associated with the connection:\n+//   - Removes from shouldReAuth map (pending re-auth)\n+//   - Removes from scheduledReAuth map (active re-auth)\n+//   - Removes credentials listener from manager\n+//\n+// This prevents memory leaks and ensures that removed connections don't have\n+// lingering re-auth operations or listeners.\n+//\n+// Thread-safe: Called when connections are removed due to errors, timeouts, or pool closure.\n+func (r *ReAuthPoolHook) OnRemove(_ context.Context, conn *pool.Conn, _ error) {\n+\tconnID := conn.GetID()\n+\tr.shouldReAuthLock.Lock()\n+\tr.scheduledLock.Lock()\n+\tdelete(r.scheduledReAuth, connID)\n+\tdelete(r.shouldReAuth, connID)\n+\tr.scheduledLock.Unlock()\n+\tr.shouldReAuthLock.Unlock()\n+\tif r.manager != nil {\n+\t\tr.manager.RemoveListener(connID)\n+\t}\n+}\n+\n+var _ pool.PoolHook = (*ReAuthPoolHook)(nil)"
    },
    {
      "sha": "ea56fd6c7ca4948075484a180778eeb8092c3547",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/hashtag/hashtag.go",
      "status": "modified",
      "additions": 12,
      "deletions": 0,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fhashtag%2Fhashtag.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fhashtag%2Fhashtag.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fhashtag%2Fhashtag.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -56,6 +56,18 @@ func Key(key string) string {\n \treturn key\n }\n \n+func Present(key string) bool {\n+\tif key == \"\" {\n+\t\treturn false\n+\t}\n+\tif s := strings.IndexByte(key, '{'); s > -1 {\n+\t\tif e := strings.IndexByte(key[s+1:], '}'); e > 0 {\n+\t\t\treturn true\n+\t\t}\n+\t}\n+\treturn false\n+}\n+\n func RandomSlot() int {\n \treturn rand.Intn(slotNumber)\n }"
    },
    {
      "sha": "17e2a1850236c58e08be3bdd4611609f5dbc9cba",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/interfaces/interfaces.go",
      "status": "added",
      "additions": 54,
      "deletions": 0,
      "changes": 54,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Finterfaces%2Finterfaces.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Finterfaces%2Finterfaces.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Finterfaces%2Finterfaces.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,54 @@\n+// Package interfaces provides shared interfaces used by both the main redis package\n+// and the maintnotifications upgrade package to avoid circular dependencies.\n+package interfaces\n+\n+import (\n+\t\"context\"\n+\t\"net\"\n+\t\"time\"\n+)\n+\n+// NotificationProcessor is (most probably) a push.NotificationProcessor\n+// forward declaration to avoid circular imports\n+type NotificationProcessor interface {\n+\tRegisterHandler(pushNotificationName string, handler interface{}, protected bool) error\n+\tUnregisterHandler(pushNotificationName string) error\n+\tGetHandler(pushNotificationName string) interface{}\n+}\n+\n+// ClientInterface defines the interface that clients must implement for maintnotifications upgrades.\n+type ClientInterface interface {\n+\t// GetOptions returns the client options.\n+\tGetOptions() OptionsInterface\n+\n+\t// GetPushProcessor returns the client's push notification processor.\n+\tGetPushProcessor() NotificationProcessor\n+}\n+\n+// OptionsInterface defines the interface for client options.\n+// Uses an adapter pattern to avoid circular dependencies.\n+type OptionsInterface interface {\n+\t// GetReadTimeout returns the read timeout.\n+\tGetReadTimeout() time.Duration\n+\n+\t// GetWriteTimeout returns the write timeout.\n+\tGetWriteTimeout() time.Duration\n+\n+\t// GetNetwork returns the network type.\n+\tGetNetwork() string\n+\n+\t// GetAddr returns the connection address.\n+\tGetAddr() string\n+\n+\t// IsTLSEnabled returns true if TLS is enabled.\n+\tIsTLSEnabled() bool\n+\n+\t// GetProtocol returns the protocol version.\n+\tGetProtocol() int\n+\n+\t// GetPoolSize returns the connection pool size.\n+\tGetPoolSize() int\n+\n+\t// NewDialer returns a new dialer function for the connection.\n+\tNewDialer() func(context.Context) (net.Conn, error)\n+}"
    },
    {
      "sha": "0bfffc311b43fd576d99bfaa00810bc71799b18d",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/log.go",
      "status": "modified",
      "additions": 57,
      "deletions": 4,
      "changes": 61,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Flog.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Flog.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Flog.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -7,20 +7,73 @@ import (\n \t\"os\"\n )\n \n+// TODO (ned): Revisit logging\n+// Add more standardized approach with log levels and configurability\n+\n type Logging interface {\n \tPrintf(ctx context.Context, format string, v ...interface{})\n }\n \n-type logger struct {\n+type DefaultLogger struct {\n \tlog *log.Logger\n }\n \n-func (l *logger) Printf(ctx context.Context, format string, v ...interface{}) {\n+func (l *DefaultLogger) Printf(ctx context.Context, format string, v ...interface{}) {\n \t_ = l.log.Output(2, fmt.Sprintf(format, v...))\n }\n \n+func NewDefaultLogger() Logging {\n+\treturn &DefaultLogger{\n+\t\tlog: log.New(os.Stderr, \"redis: \", log.LstdFlags|log.Lshortfile),\n+\t}\n+}\n+\n // Logger calls Output to print to the stderr.\n // Arguments are handled in the manner of fmt.Print.\n-var Logger Logging = &logger{\n-\tlog: log.New(os.Stderr, \"redis: \", log.LstdFlags|log.Lshortfile),\n+var Logger Logging = NewDefaultLogger()\n+\n+var LogLevel LogLevelT = LogLevelError\n+\n+// LogLevelT represents the logging level\n+type LogLevelT int\n+\n+// Log level constants for the entire go-redis library\n+const (\n+\tLogLevelError LogLevelT = iota // 0 - errors only\n+\tLogLevelWarn                   // 1 - warnings and errors\n+\tLogLevelInfo                   // 2 - info, warnings, and errors\n+\tLogLevelDebug                  // 3 - debug, info, warnings, and errors\n+)\n+\n+// String returns the string representation of the log level\n+func (l LogLevelT) String() string {\n+\tswitch l {\n+\tcase LogLevelError:\n+\t\treturn \"ERROR\"\n+\tcase LogLevelWarn:\n+\t\treturn \"WARN\"\n+\tcase LogLevelInfo:\n+\t\treturn \"INFO\"\n+\tcase LogLevelDebug:\n+\t\treturn \"DEBUG\"\n+\tdefault:\n+\t\treturn \"UNKNOWN\"\n+\t}\n+}\n+\n+// IsValid returns true if the log level is valid\n+func (l LogLevelT) IsValid() bool {\n+\treturn l >= LogLevelError && l <= LogLevelDebug\n+}\n+\n+func (l LogLevelT) WarnOrAbove() bool {\n+\treturn l >= LogLevelWarn\n+}\n+\n+func (l LogLevelT) InfoOrAbove() bool {\n+\treturn l >= LogLevelInfo\n+}\n+\n+func (l LogLevelT) DebugOrAbove() bool {\n+\treturn l >= LogLevelDebug\n }"
    },
    {
      "sha": "34cb1692d9b71daefb2e5bab14c26c524d8ca398",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/maintnotifications/logs/log_messages.go",
      "status": "added",
      "additions": 625,
      "deletions": 0,
      "changes": 625,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fmaintnotifications%2Flogs%2Flog_messages.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fmaintnotifications%2Flogs%2Flog_messages.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fmaintnotifications%2Flogs%2Flog_messages.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,625 @@\n+package logs\n+\n+import (\n+\t\"encoding/json\"\n+\t\"fmt\"\n+\t\"regexp\"\n+\n+\t\"github.com/redis/go-redis/v9/internal\"\n+)\n+\n+// appendJSONIfDebug appends JSON data to a message only if the global log level is Debug\n+func appendJSONIfDebug(message string, data map[string]interface{}) string {\n+\tif internal.LogLevel.DebugOrAbove() {\n+\t\tjsonData, _ := json.Marshal(data)\n+\t\treturn fmt.Sprintf(\"%s %s\", message, string(jsonData))\n+\t}\n+\treturn message\n+}\n+\n+const (\n+\t// ========================================\n+\t// CIRCUIT_BREAKER.GO - Circuit breaker management\n+\t// ========================================\n+\tCircuitBreakerTransitioningToHalfOpenMessage = \"circuit breaker transitioning to half-open\"\n+\tCircuitBreakerOpenedMessage                  = \"circuit breaker opened\"\n+\tCircuitBreakerReopenedMessage                = \"circuit breaker reopened\"\n+\tCircuitBreakerClosedMessage                  = \"circuit breaker closed\"\n+\tCircuitBreakerCleanupMessage                 = \"circuit breaker cleanup\"\n+\tCircuitBreakerOpenMessage                    = \"circuit breaker is open, failing fast\"\n+\n+\t// ========================================\n+\t// CONFIG.GO - Configuration and debug\n+\t// ========================================\n+\tDebugLoggingEnabledMessage = \"debug logging enabled\"\n+\tConfigDebugMessage         = \"config debug\"\n+\n+\t// ========================================\n+\t// ERRORS.GO - Error message constants\n+\t// ========================================\n+\tInvalidRelaxedTimeoutErrorMessage                 = \"relaxed timeout must be greater than 0\"\n+\tInvalidHandoffTimeoutErrorMessage                 = \"handoff timeout must be greater than 0\"\n+\tInvalidHandoffWorkersErrorMessage                 = \"MaxWorkers must be greater than or equal to 0\"\n+\tInvalidHandoffQueueSizeErrorMessage               = \"handoff queue size must be greater than 0\"\n+\tInvalidPostHandoffRelaxedDurationErrorMessage     = \"post-handoff relaxed duration must be greater than or equal to 0\"\n+\tInvalidEndpointTypeErrorMessage                   = \"invalid endpoint type\"\n+\tInvalidMaintNotificationsErrorMessage             = \"invalid maintenance notifications setting (must be 'disabled', 'enabled', or 'auto')\"\n+\tInvalidHandoffRetriesErrorMessage                 = \"MaxHandoffRetries must be between 1 and 10\"\n+\tInvalidClientErrorMessage                         = \"invalid client type\"\n+\tInvalidNotificationErrorMessage                   = \"invalid notification format\"\n+\tMaxHandoffRetriesReachedErrorMessage              = \"max handoff retries reached\"\n+\tHandoffQueueFullErrorMessage                      = \"handoff queue is full, cannot queue new handoff requests - consider increasing HandoffQueueSize or MaxWorkers in configuration\"\n+\tInvalidCircuitBreakerFailureThresholdErrorMessage = \"circuit breaker failure threshold must be >= 1\"\n+\tInvalidCircuitBreakerResetTimeoutErrorMessage     = \"circuit breaker reset timeout must be >= 0\"\n+\tInvalidCircuitBreakerMaxRequestsErrorMessage      = \"circuit breaker max requests must be >= 1\"\n+\tConnectionMarkedForHandoffErrorMessage            = \"connection marked for handoff\"\n+\tConnectionInvalidHandoffStateErrorMessage         = \"connection is in invalid state for handoff\"\n+\tShutdownErrorMessage                              = \"shutdown\"\n+\tCircuitBreakerOpenErrorMessage                    = \"circuit breaker is open, failing fast\"\n+\n+\t// ========================================\n+\t// EXAMPLE_HOOKS.GO - Example metrics hooks\n+\t// ========================================\n+\tMetricsHookProcessingNotificationMessage = \"metrics hook processing\"\n+\tMetricsHookRecordedErrorMessage          = \"metrics hook recorded error\"\n+\n+\t// ========================================\n+\t// HANDOFF_WORKER.GO - Connection handoff processing\n+\t// ========================================\n+\tHandoffStartedMessage                            = \"handoff started\"\n+\tHandoffFailedMessage                             = \"handoff failed\"\n+\tConnectionNotMarkedForHandoffMessage             = \"is not marked for handoff and has no retries\"\n+\tConnectionNotMarkedForHandoffErrorMessage        = \"is not marked for handoff\"\n+\tHandoffRetryAttemptMessage                       = \"Performing handoff\"\n+\tCannotQueueHandoffForRetryMessage                = \"can't queue handoff for retry\"\n+\tHandoffQueueFullMessage                          = \"handoff queue is full\"\n+\tFailedToDialNewEndpointMessage                   = \"failed to dial new endpoint\"\n+\tApplyingRelaxedTimeoutDueToPostHandoffMessage    = \"applying relaxed timeout due to post-handoff\"\n+\tHandoffSuccessMessage                            = \"handoff succeeded\"\n+\tRemovingConnectionFromPoolMessage                = \"removing connection from pool\"\n+\tNoPoolProvidedMessageCannotRemoveMessage         = \"no pool provided, cannot remove connection, closing it\"\n+\tWorkerExitingDueToShutdownMessage                = \"worker exiting due to shutdown\"\n+\tWorkerExitingDueToShutdownWhileProcessingMessage = \"worker exiting due to shutdown while processing request\"\n+\tWorkerPanicRecoveredMessage                      = \"worker panic recovered\"\n+\tWorkerExitingDueToInactivityTimeoutMessage       = \"worker exiting due to inactivity timeout\"\n+\tReachedMaxHandoffRetriesMessage                  = \"reached max handoff retries\"\n+\n+\t// ========================================\n+\t// MANAGER.GO - Moving operation tracking and handler registration\n+\t// ========================================\n+\tDuplicateMovingOperationMessage  = \"duplicate MOVING operation ignored\"\n+\tTrackingMovingOperationMessage   = \"tracking MOVING operation\"\n+\tUntrackingMovingOperationMessage = \"untracking MOVING operation\"\n+\tOperationNotTrackedMessage       = \"operation not tracked\"\n+\tFailedToRegisterHandlerMessage   = \"failed to register handler\"\n+\n+\t// ========================================\n+\t// HOOKS.GO - Notification processing hooks\n+\t// ========================================\n+\tProcessingNotificationMessage          = \"processing notification started\"\n+\tProcessingNotificationFailedMessage    = \"proccessing notification failed\"\n+\tProcessingNotificationSucceededMessage = \"processing notification succeeded\"\n+\n+\t// ========================================\n+\t// POOL_HOOK.GO - Pool connection management\n+\t// ========================================\n+\tFailedToQueueHandoffMessage = \"failed to queue handoff\"\n+\tMarkedForHandoffMessage     = \"connection marked for handoff\"\n+\n+\t// ========================================\n+\t// PUSH_NOTIFICATION_HANDLER.GO - Push notification validation and processing\n+\t// ========================================\n+\tInvalidNotificationFormatMessage              = \"invalid notification format\"\n+\tInvalidNotificationTypeFormatMessage          = \"invalid notification type format\"\n+\tInvalidSeqIDInMovingNotificationMessage       = \"invalid seqID in MOVING notification\"\n+\tInvalidTimeSInMovingNotificationMessage       = \"invalid timeS in MOVING notification\"\n+\tInvalidNewEndpointInMovingNotificationMessage = \"invalid newEndpoint in MOVING notification\"\n+\tNoConnectionInHandlerContextMessage           = \"no connection in handler context\"\n+\tInvalidConnectionTypeInHandlerContextMessage  = \"invalid connection type in handler context\"\n+\tSchedulingHandoffToCurrentEndpointMessage     = \"scheduling handoff to current endpoint\"\n+\tRelaxedTimeoutDueToNotificationMessage        = \"applying relaxed timeout due to notification\"\n+\tUnrelaxedTimeoutMessage                       = \"clearing relaxed timeout\"\n+\tManagerNotInitializedMessage                  = \"manager not initialized\"\n+\tFailedToMarkForHandoffMessage                 = \"failed to mark connection for handoff\"\n+\n+\t// ========================================\n+\t// used in pool/conn\n+\t// ========================================\n+\tUnrelaxedTimeoutAfterDeadlineMessage = \"clearing relaxed timeout after deadline\"\n+)\n+\n+func HandoffStarted(connID uint64, newEndpoint string) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s to %s\", connID, HandoffStartedMessage, newEndpoint)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":   connID,\n+\t\t\"endpoint\": newEndpoint,\n+\t})\n+}\n+\n+func HandoffFailed(connID uint64, newEndpoint string, attempt int, maxAttempts int, err error) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s to %s (attempt %d/%d): %v\", connID, HandoffFailedMessage, newEndpoint, attempt, maxAttempts, err)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":      connID,\n+\t\t\"endpoint\":    newEndpoint,\n+\t\t\"attempt\":     attempt,\n+\t\t\"maxAttempts\": maxAttempts,\n+\t\t\"error\":       err.Error(),\n+\t})\n+}\n+\n+func HandoffSucceeded(connID uint64, newEndpoint string) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s to %s\", connID, HandoffSuccessMessage, newEndpoint)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":   connID,\n+\t\t\"endpoint\": newEndpoint,\n+\t})\n+}\n+\n+// Timeout-related log functions\n+func RelaxedTimeoutDueToNotification(connID uint64, notificationType string, timeout interface{}) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s %s (%v)\", connID, RelaxedTimeoutDueToNotificationMessage, notificationType, timeout)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":           connID,\n+\t\t\"notificationType\": notificationType,\n+\t\t\"timeout\":          fmt.Sprintf(\"%v\", timeout),\n+\t})\n+}\n+\n+func UnrelaxedTimeout(connID uint64) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s\", connID, UnrelaxedTimeoutMessage)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\": connID,\n+\t})\n+}\n+\n+func UnrelaxedTimeoutAfterDeadline(connID uint64) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s\", connID, UnrelaxedTimeoutAfterDeadlineMessage)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\": connID,\n+\t})\n+}\n+\n+// Handoff queue and marking functions\n+func HandoffQueueFull(queueLen, queueCap int) string {\n+\tmessage := fmt.Sprintf(\"%s (%d/%d), cannot queue new handoff requests - consider increasing HandoffQueueSize or MaxWorkers in configuration\", HandoffQueueFullMessage, queueLen, queueCap)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"queueLen\": queueLen,\n+\t\t\"queueCap\": queueCap,\n+\t})\n+}\n+\n+func FailedToQueueHandoff(connID uint64, err error) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s: %v\", connID, FailedToQueueHandoffMessage, err)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\": connID,\n+\t\t\"error\":  err.Error(),\n+\t})\n+}\n+\n+func FailedToMarkForHandoff(connID uint64, err error) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s: %v\", connID, FailedToMarkForHandoffMessage, err)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\": connID,\n+\t\t\"error\":  err.Error(),\n+\t})\n+}\n+\n+func FailedToDialNewEndpoint(connID uint64, endpoint string, err error) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s %s: %v\", connID, FailedToDialNewEndpointMessage, endpoint, err)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":   connID,\n+\t\t\"endpoint\": endpoint,\n+\t\t\"error\":    err.Error(),\n+\t})\n+}\n+\n+func ReachedMaxHandoffRetries(connID uint64, endpoint string, maxRetries int) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s to %s (max retries: %d)\", connID, ReachedMaxHandoffRetriesMessage, endpoint, maxRetries)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":     connID,\n+\t\t\"endpoint\":   endpoint,\n+\t\t\"maxRetries\": maxRetries,\n+\t})\n+}\n+\n+// Notification processing functions\n+func ProcessingNotification(connID uint64, seqID int64, notificationType string, notification interface{}) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] seqID[%d] %s %s: %v\", connID, seqID, ProcessingNotificationMessage, notificationType, notification)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":           connID,\n+\t\t\"seqID\":            seqID,\n+\t\t\"notificationType\": notificationType,\n+\t\t\"notification\":     fmt.Sprintf(\"%v\", notification),\n+\t})\n+}\n+\n+func ProcessingNotificationFailed(connID uint64, notificationType string, err error, notification interface{}) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s %s: %v - %v\", connID, ProcessingNotificationFailedMessage, notificationType, err, notification)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":           connID,\n+\t\t\"notificationType\": notificationType,\n+\t\t\"error\":            err.Error(),\n+\t\t\"notification\":     fmt.Sprintf(\"%v\", notification),\n+\t})\n+}\n+\n+func ProcessingNotificationSucceeded(connID uint64, notificationType string) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s %s\", connID, ProcessingNotificationSucceededMessage, notificationType)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":           connID,\n+\t\t\"notificationType\": notificationType,\n+\t})\n+}\n+\n+// Moving operation tracking functions\n+func DuplicateMovingOperation(connID uint64, endpoint string, seqID int64) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s for %s seqID[%d]\", connID, DuplicateMovingOperationMessage, endpoint, seqID)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":   connID,\n+\t\t\"endpoint\": endpoint,\n+\t\t\"seqID\":    seqID,\n+\t})\n+}\n+\n+func TrackingMovingOperation(connID uint64, endpoint string, seqID int64) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s for %s seqID[%d]\", connID, TrackingMovingOperationMessage, endpoint, seqID)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":   connID,\n+\t\t\"endpoint\": endpoint,\n+\t\t\"seqID\":    seqID,\n+\t})\n+}\n+\n+func UntrackingMovingOperation(connID uint64, seqID int64) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s seqID[%d]\", connID, UntrackingMovingOperationMessage, seqID)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\": connID,\n+\t\t\"seqID\":  seqID,\n+\t})\n+}\n+\n+func OperationNotTracked(connID uint64, seqID int64) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s seqID[%d]\", connID, OperationNotTrackedMessage, seqID)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\": connID,\n+\t\t\"seqID\":  seqID,\n+\t})\n+}\n+\n+// Connection pool functions\n+func RemovingConnectionFromPool(connID uint64, reason error) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s due to: %v\", connID, RemovingConnectionFromPoolMessage, reason)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\": connID,\n+\t\t\"reason\": reason.Error(),\n+\t})\n+}\n+\n+func NoPoolProvidedCannotRemove(connID uint64, reason error) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s due to: %v\", connID, NoPoolProvidedMessageCannotRemoveMessage, reason)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\": connID,\n+\t\t\"reason\": reason.Error(),\n+\t})\n+}\n+\n+// Circuit breaker functions\n+func CircuitBreakerOpen(connID uint64, endpoint string) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s for %s\", connID, CircuitBreakerOpenMessage, endpoint)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":   connID,\n+\t\t\"endpoint\": endpoint,\n+\t})\n+}\n+\n+// Additional handoff functions for specific cases\n+func ConnectionNotMarkedForHandoff(connID uint64) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s\", connID, ConnectionNotMarkedForHandoffMessage)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\": connID,\n+\t})\n+}\n+\n+func ConnectionNotMarkedForHandoffError(connID uint64) string {\n+\treturn fmt.Sprintf(\"conn[%d] %s\", connID, ConnectionNotMarkedForHandoffErrorMessage)\n+}\n+\n+func HandoffRetryAttempt(connID uint64, retries int, newEndpoint string, oldEndpoint string) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] Retry %d: %s to %s(was %s)\", connID, retries, HandoffRetryAttemptMessage, newEndpoint, oldEndpoint)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":      connID,\n+\t\t\"retries\":     retries,\n+\t\t\"newEndpoint\": newEndpoint,\n+\t\t\"oldEndpoint\": oldEndpoint,\n+\t})\n+}\n+\n+func CannotQueueHandoffForRetry(err error) string {\n+\tmessage := fmt.Sprintf(\"%s: %v\", CannotQueueHandoffForRetryMessage, err)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"error\": err.Error(),\n+\t})\n+}\n+\n+// Validation and error functions\n+func InvalidNotificationFormat(notification interface{}) string {\n+\tmessage := fmt.Sprintf(\"%s: %v\", InvalidNotificationFormatMessage, notification)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"notification\": fmt.Sprintf(\"%v\", notification),\n+\t})\n+}\n+\n+func InvalidNotificationTypeFormat(notificationType interface{}) string {\n+\tmessage := fmt.Sprintf(\"%s: %v\", InvalidNotificationTypeFormatMessage, notificationType)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"notificationType\": fmt.Sprintf(\"%v\", notificationType),\n+\t})\n+}\n+\n+// InvalidNotification creates a log message for invalid notifications of any type\n+func InvalidNotification(notificationType string, notification interface{}) string {\n+\tmessage := fmt.Sprintf(\"invalid %s notification: %v\", notificationType, notification)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"notificationType\": notificationType,\n+\t\t\"notification\":     fmt.Sprintf(\"%v\", notification),\n+\t})\n+}\n+\n+func InvalidSeqIDInMovingNotification(seqID interface{}) string {\n+\tmessage := fmt.Sprintf(\"%s: %v\", InvalidSeqIDInMovingNotificationMessage, seqID)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"seqID\": fmt.Sprintf(\"%v\", seqID),\n+\t})\n+}\n+\n+func InvalidTimeSInMovingNotification(timeS interface{}) string {\n+\tmessage := fmt.Sprintf(\"%s: %v\", InvalidTimeSInMovingNotificationMessage, timeS)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"timeS\": fmt.Sprintf(\"%v\", timeS),\n+\t})\n+}\n+\n+func InvalidNewEndpointInMovingNotification(newEndpoint interface{}) string {\n+\tmessage := fmt.Sprintf(\"%s: %v\", InvalidNewEndpointInMovingNotificationMessage, newEndpoint)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"newEndpoint\": fmt.Sprintf(\"%v\", newEndpoint),\n+\t})\n+}\n+\n+func NoConnectionInHandlerContext(notificationType string) string {\n+\tmessage := fmt.Sprintf(\"%s for %s notification\", NoConnectionInHandlerContextMessage, notificationType)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"notificationType\": notificationType,\n+\t})\n+}\n+\n+func InvalidConnectionTypeInHandlerContext(notificationType string, conn interface{}, handlerCtx interface{}) string {\n+\tmessage := fmt.Sprintf(\"%s for %s notification - %T %#v\", InvalidConnectionTypeInHandlerContextMessage, notificationType, conn, handlerCtx)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"notificationType\": notificationType,\n+\t\t\"connType\":         fmt.Sprintf(\"%T\", conn),\n+\t})\n+}\n+\n+func SchedulingHandoffToCurrentEndpoint(connID uint64, seconds float64) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s in %v seconds\", connID, SchedulingHandoffToCurrentEndpointMessage, seconds)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":  connID,\n+\t\t\"seconds\": seconds,\n+\t})\n+}\n+\n+func ManagerNotInitialized() string {\n+\treturn appendJSONIfDebug(ManagerNotInitializedMessage, map[string]interface{}{})\n+}\n+\n+func FailedToRegisterHandler(notificationType string, err error) string {\n+\tmessage := fmt.Sprintf(\"%s for %s: %v\", FailedToRegisterHandlerMessage, notificationType, err)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"notificationType\": notificationType,\n+\t\t\"error\":            err.Error(),\n+\t})\n+}\n+\n+func ShutdownError() string {\n+\treturn appendJSONIfDebug(ShutdownErrorMessage, map[string]interface{}{})\n+}\n+\n+// Configuration validation error functions\n+func InvalidRelaxedTimeoutError() string {\n+\treturn appendJSONIfDebug(InvalidRelaxedTimeoutErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidHandoffTimeoutError() string {\n+\treturn appendJSONIfDebug(InvalidHandoffTimeoutErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidHandoffWorkersError() string {\n+\treturn appendJSONIfDebug(InvalidHandoffWorkersErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidHandoffQueueSizeError() string {\n+\treturn appendJSONIfDebug(InvalidHandoffQueueSizeErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidPostHandoffRelaxedDurationError() string {\n+\treturn appendJSONIfDebug(InvalidPostHandoffRelaxedDurationErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidEndpointTypeError() string {\n+\treturn appendJSONIfDebug(InvalidEndpointTypeErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidMaintNotificationsError() string {\n+\treturn appendJSONIfDebug(InvalidMaintNotificationsErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidHandoffRetriesError() string {\n+\treturn appendJSONIfDebug(InvalidHandoffRetriesErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidClientError() string {\n+\treturn appendJSONIfDebug(InvalidClientErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidNotificationError() string {\n+\treturn appendJSONIfDebug(InvalidNotificationErrorMessage, map[string]interface{}{})\n+}\n+\n+func MaxHandoffRetriesReachedError() string {\n+\treturn appendJSONIfDebug(MaxHandoffRetriesReachedErrorMessage, map[string]interface{}{})\n+}\n+\n+func HandoffQueueFullError() string {\n+\treturn appendJSONIfDebug(HandoffQueueFullErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidCircuitBreakerFailureThresholdError() string {\n+\treturn appendJSONIfDebug(InvalidCircuitBreakerFailureThresholdErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidCircuitBreakerResetTimeoutError() string {\n+\treturn appendJSONIfDebug(InvalidCircuitBreakerResetTimeoutErrorMessage, map[string]interface{}{})\n+}\n+\n+func InvalidCircuitBreakerMaxRequestsError() string {\n+\treturn appendJSONIfDebug(InvalidCircuitBreakerMaxRequestsErrorMessage, map[string]interface{}{})\n+}\n+\n+// Configuration and debug functions\n+func DebugLoggingEnabled() string {\n+\treturn appendJSONIfDebug(DebugLoggingEnabledMessage, map[string]interface{}{})\n+}\n+\n+func ConfigDebug(config interface{}) string {\n+\tmessage := fmt.Sprintf(\"%s: %+v\", ConfigDebugMessage, config)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"config\": fmt.Sprintf(\"%+v\", config),\n+\t})\n+}\n+\n+// Handoff worker functions\n+func WorkerExitingDueToShutdown() string {\n+\treturn appendJSONIfDebug(WorkerExitingDueToShutdownMessage, map[string]interface{}{})\n+}\n+\n+func WorkerExitingDueToShutdownWhileProcessing() string {\n+\treturn appendJSONIfDebug(WorkerExitingDueToShutdownWhileProcessingMessage, map[string]interface{}{})\n+}\n+\n+func WorkerPanicRecovered(panicValue interface{}) string {\n+\tmessage := fmt.Sprintf(\"%s: %v\", WorkerPanicRecoveredMessage, panicValue)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"panic\": fmt.Sprintf(\"%v\", panicValue),\n+\t})\n+}\n+\n+func WorkerExitingDueToInactivityTimeout(timeout interface{}) string {\n+\tmessage := fmt.Sprintf(\"%s (%v)\", WorkerExitingDueToInactivityTimeoutMessage, timeout)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"timeout\": fmt.Sprintf(\"%v\", timeout),\n+\t})\n+}\n+\n+func ApplyingRelaxedTimeoutDueToPostHandoff(connID uint64, timeout interface{}, until string) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s (%v) until %s\", connID, ApplyingRelaxedTimeoutDueToPostHandoffMessage, timeout, until)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\":  connID,\n+\t\t\"timeout\": fmt.Sprintf(\"%v\", timeout),\n+\t\t\"until\":   until,\n+\t})\n+}\n+\n+// Example hooks functions\n+func MetricsHookProcessingNotification(notificationType string, connID uint64) string {\n+\tmessage := fmt.Sprintf(\"%s %s notification on conn[%d]\", MetricsHookProcessingNotificationMessage, notificationType, connID)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"notificationType\": notificationType,\n+\t\t\"connID\":           connID,\n+\t})\n+}\n+\n+func MetricsHookRecordedError(notificationType string, connID uint64, err error) string {\n+\tmessage := fmt.Sprintf(\"%s for %s notification on conn[%d]: %v\", MetricsHookRecordedErrorMessage, notificationType, connID, err)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"notificationType\": notificationType,\n+\t\t\"connID\":           connID,\n+\t\t\"error\":            err.Error(),\n+\t})\n+}\n+\n+// Pool hook functions\n+func MarkedForHandoff(connID uint64) string {\n+\tmessage := fmt.Sprintf(\"conn[%d] %s\", connID, MarkedForHandoffMessage)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"connID\": connID,\n+\t})\n+}\n+\n+// Circuit breaker additional functions\n+func CircuitBreakerTransitioningToHalfOpen(endpoint string) string {\n+\tmessage := fmt.Sprintf(\"%s for %s\", CircuitBreakerTransitioningToHalfOpenMessage, endpoint)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"endpoint\": endpoint,\n+\t})\n+}\n+\n+func CircuitBreakerOpened(endpoint string, failures int64) string {\n+\tmessage := fmt.Sprintf(\"%s for endpoint %s after %d failures\", CircuitBreakerOpenedMessage, endpoint, failures)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"endpoint\": endpoint,\n+\t\t\"failures\": failures,\n+\t})\n+}\n+\n+func CircuitBreakerReopened(endpoint string) string {\n+\tmessage := fmt.Sprintf(\"%s for endpoint %s due to failure in half-open state\", CircuitBreakerReopenedMessage, endpoint)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"endpoint\": endpoint,\n+\t})\n+}\n+\n+func CircuitBreakerClosed(endpoint string, successes int64) string {\n+\tmessage := fmt.Sprintf(\"%s for endpoint %s after %d successful requests\", CircuitBreakerClosedMessage, endpoint, successes)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"endpoint\":  endpoint,\n+\t\t\"successes\": successes,\n+\t})\n+}\n+\n+func CircuitBreakerCleanup(removed int, total int) string {\n+\tmessage := fmt.Sprintf(\"%s removed %d/%d entries\", CircuitBreakerCleanupMessage, removed, total)\n+\treturn appendJSONIfDebug(message, map[string]interface{}{\n+\t\t\"removed\": removed,\n+\t\t\"total\":   total,\n+\t})\n+}\n+\n+// ExtractDataFromLogMessage extracts structured data from maintnotifications log messages\n+// Returns a map containing the parsed key-value pairs from the structured data section\n+// Example: \"conn[123] handoff started to localhost:6379 {\"connID\":123,\"endpoint\":\"localhost:6379\"}\"\n+// Returns: map[string]interface{}{\"connID\": 123, \"endpoint\": \"localhost:6379\"}\n+func ExtractDataFromLogMessage(logMessage string) map[string]interface{} {\n+\tresult := make(map[string]interface{})\n+\n+\t// Find the JSON data section at the end of the message\n+\tre := regexp.MustCompile(`(\\{.*\\})$`)\n+\tmatches := re.FindStringSubmatch(logMessage)\n+\tif len(matches) < 2 {\n+\t\treturn result\n+\t}\n+\n+\tjsonStr := matches[1]\n+\tif jsonStr == \"\" {\n+\t\treturn result\n+\t}\n+\n+\t// Parse the JSON directly\n+\tvar jsonResult map[string]interface{}\n+\tif err := json.Unmarshal([]byte(jsonStr), &jsonResult); err == nil {\n+\t\treturn jsonResult\n+\t}\n+\n+\t// If JSON parsing fails, return empty map\n+\treturn result\n+}"
    },
    {
      "sha": "95d83bfde48b176573f4c1c567a6920bff388dad",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/pool/conn.go",
      "status": "modified",
      "additions": 817,
      "deletions": 23,
      "changes": 840,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -1,64 +1,803 @@\n+// Package pool implements the pool management\n package pool\n \n import (\n \t\"bufio\"\n \t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n \t\"net\"\n+\t\"sync\"\n \t\"sync/atomic\"\n \t\"time\"\n \n+\t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/maintnotifications/logs\"\n \t\"github.com/redis/go-redis/v9/internal/proto\"\n )\n \n var noDeadline = time.Time{}\n \n+// Preallocated errors for hot paths to avoid allocations\n+var (\n+\terrAlreadyMarkedForHandoff  = errors.New(\"connection is already marked for handoff\")\n+\terrNotMarkedForHandoff      = errors.New(\"connection was not marked for handoff\")\n+\terrHandoffStateChanged      = errors.New(\"handoff state changed during marking\")\n+\terrConnectionNotAvailable   = errors.New(\"redis: connection not available\")\n+\terrConnNotAvailableForWrite = errors.New(\"redis: connection not available for write operation\")\n+)\n+\n+// getCachedTimeNs returns the current time in nanoseconds.\n+// This function previously used a global cache updated by a background goroutine,\n+// but that caused unnecessary CPU usage when the client was idle (ticker waking up\n+// the scheduler every 50ms). We now use time.Now() directly, which is fast enough\n+// on modern systems (vDSO on Linux) and only adds ~1-2% overhead in extreme\n+// high-concurrency benchmarks while eliminating idle CPU usage.\n+func getCachedTimeNs() int64 {\n+\treturn time.Now().UnixNano()\n+}\n+\n+// GetCachedTimeNs returns the current time in nanoseconds.\n+// Exported for use by other packages that need fast time access.\n+func GetCachedTimeNs() int64 {\n+\treturn getCachedTimeNs()\n+}\n+\n+// Global atomic counter for connection IDs\n+var connIDCounter uint64\n+\n+// HandoffState represents the atomic state for connection handoffs\n+// This struct is stored atomically to prevent race conditions between\n+// checking handoff status and reading handoff parameters\n+type HandoffState struct {\n+\tShouldHandoff bool   // Whether connection should be handed off\n+\tEndpoint      string // New endpoint for handoff\n+\tSeqID         int64  // Sequence ID from MOVING notification\n+}\n+\n+// atomicNetConn is a wrapper to ensure consistent typing in atomic.Value\n+type atomicNetConn struct {\n+\tconn net.Conn\n+}\n+\n+// generateConnID generates a fast unique identifier for a connection with zero allocations\n+func generateConnID() uint64 {\n+\treturn atomic.AddUint64(&connIDCounter, 1)\n+}\n+\n type Conn struct {\n-\tusedAt  int64 // atomic\n-\tnetConn net.Conn\n+\t// Connection identifier for unique tracking\n+\tid uint64\n+\n+\tusedAt    atomic.Int64\n+\tlastPutAt atomic.Int64\n+\n+\t// Lock-free netConn access using atomic.Value\n+\t// Contains *atomicNetConn wrapper, accessed atomically for better performance\n+\tnetConnAtomic atomic.Value // stores *atomicNetConn\n \n \trd *proto.Reader\n \tbw *bufio.Writer\n \twr *proto.Writer\n \n-\tInited    bool\n+\t// Lightweight mutex to protect reader operations during handoff\n+\t// Only used for the brief period during SetNetConn and HasBufferedData/PeekReplyTypeSafe\n+\treaderMu sync.RWMutex\n+\n+\t// State machine for connection state management\n+\t// Replaces: usable, Inited, used\n+\t// Provides thread-safe state transitions with FIFO waiting queue\n+\t// States: CREATED  INITIALIZING  IDLE  IN_USE\n+\t//                                    \n+\t//                                UNUSABLE (handoff/reauth)\n+\t//                                    \n+\t//                                IDLE/CLOSED\n+\tstateMachine *ConnStateMachine\n+\n+\t// Handoff metadata - managed separately from state machine\n+\t// These are atomic for lock-free access during handoff operations\n+\thandoffStateAtomic   atomic.Value  // stores *HandoffState\n+\thandoffRetriesAtomic atomic.Uint32 // retry counter\n+\n \tpooled    bool\n+\tpubsub    bool\n+\tclosed    atomic.Bool\n \tcreatedAt time.Time\n+\texpiresAt time.Time\n+\n+\t// maintenanceNotifications upgrade support: relaxed timeouts during migrations/failovers\n+\n+\t// Using atomic operations for lock-free access to avoid mutex contention\n+\trelaxedReadTimeoutNs  atomic.Int64 // time.Duration as nanoseconds\n+\trelaxedWriteTimeoutNs atomic.Int64 // time.Duration as nanoseconds\n+\trelaxedDeadlineNs     atomic.Int64 // time.Time as nanoseconds since epoch\n+\n+\t// Counter to track multiple relaxed timeout setters if we have nested calls\n+\t// will be decremented when ClearRelaxedTimeout is called or deadline is reached\n+\t// if counter reaches 0, we clear the relaxed timeouts\n+\trelaxedCounter atomic.Int32\n+\n+\t// Connection initialization function for reconnections\n+\tinitConnFunc func(context.Context, *Conn) error\n+\n+\tonClose func() error\n }\n \n func NewConn(netConn net.Conn) *Conn {\n+\treturn NewConnWithBufferSize(netConn, proto.DefaultBufferSize, proto.DefaultBufferSize)\n+}\n+\n+func NewConnWithBufferSize(netConn net.Conn, readBufSize, writeBufSize int) *Conn {\n+\tnow := time.Now()\n \tcn := &Conn{\n-\t\tnetConn:   netConn,\n-\t\tcreatedAt: time.Now(),\n+\t\tcreatedAt:    now,\n+\t\tid:           generateConnID(), // Generate unique ID for this connection\n+\t\tstateMachine: NewConnStateMachine(),\n+\t}\n+\n+\t// Use specified buffer sizes, or fall back to 32KiB defaults if 0\n+\tif readBufSize > 0 {\n+\t\tcn.rd = proto.NewReaderSize(netConn, readBufSize)\n+\t} else {\n+\t\tcn.rd = proto.NewReader(netConn) // Uses 32KiB default\n+\t}\n+\n+\tif writeBufSize > 0 {\n+\t\tcn.bw = bufio.NewWriterSize(netConn, writeBufSize)\n+\t} else {\n+\t\tcn.bw = bufio.NewWriterSize(netConn, proto.DefaultBufferSize)\n \t}\n-\tcn.rd = proto.NewReader(netConn)\n-\tcn.bw = bufio.NewWriter(netConn)\n+\n+\t// Store netConn atomically for lock-free access using wrapper\n+\tcn.netConnAtomic.Store(&atomicNetConn{conn: netConn})\n+\n \tcn.wr = proto.NewWriter(cn.bw)\n-\tcn.SetUsedAt(time.Now())\n+\tcn.SetUsedAt(now)\n+\t// Initialize handoff state atomically\n+\tinitialHandoffState := &HandoffState{\n+\t\tShouldHandoff: false,\n+\t\tEndpoint:      \"\",\n+\t\tSeqID:         0,\n+\t}\n+\tcn.handoffStateAtomic.Store(initialHandoffState)\n \treturn cn\n }\n \n func (cn *Conn) UsedAt() time.Time {\n-\tunix := atomic.LoadInt64(&cn.usedAt)\n-\treturn time.Unix(unix, 0)\n+\treturn time.Unix(0, cn.usedAt.Load())\n }\n-\n func (cn *Conn) SetUsedAt(tm time.Time) {\n-\tatomic.StoreInt64(&cn.usedAt, tm.Unix())\n+\tcn.usedAt.Store(tm.UnixNano())\n+}\n+\n+func (cn *Conn) UsedAtNs() int64 {\n+\treturn cn.usedAt.Load()\n+}\n+func (cn *Conn) SetUsedAtNs(ns int64) {\n+\tcn.usedAt.Store(ns)\n+}\n+\n+func (cn *Conn) LastPutAtNs() int64 {\n+\treturn cn.lastPutAt.Load()\n+}\n+func (cn *Conn) SetLastPutAtNs(ns int64) {\n+\tcn.lastPutAt.Store(ns)\n+}\n+\n+// Backward-compatible wrapper methods for state machine\n+// These maintain the existing API while using the new state machine internally\n+\n+// CompareAndSwapUsable atomically compares and swaps the usable flag (lock-free).\n+//\n+// This is used by background operations (handoff, re-auth) to acquire exclusive\n+// access to a connection. The operation sets usable to false, preventing the pool\n+// from returning the connection to clients.\n+//\n+// Returns true if the swap was successful (old value matched), false otherwise.\n+//\n+// Implementation note: This is a compatibility wrapper around the state machine.\n+// It checks if the current state is \"usable\" (IDLE or IN_USE) and transitions accordingly.\n+// Deprecated: Use GetStateMachine().TryTransition() directly for better state management.\n+func (cn *Conn) CompareAndSwapUsable(old, new bool) bool {\n+\tcurrentState := cn.stateMachine.GetState()\n+\n+\t// Check if current state matches the \"old\" usable value\n+\tcurrentUsable := (currentState == StateIdle || currentState == StateInUse)\n+\tif currentUsable != old {\n+\t\treturn false\n+\t}\n+\n+\t// If we're trying to set to the same value, succeed immediately\n+\tif old == new {\n+\t\treturn true\n+\t}\n+\n+\t// Transition based on new value\n+\tif new {\n+\t\t// Trying to make usable - transition from UNUSABLE to IDLE\n+\t\t// This should only work from UNUSABLE or INITIALIZING states\n+\t\t// Use predefined slice to avoid allocation\n+\t\t_, err := cn.stateMachine.TryTransition(\n+\t\t\tvalidFromInitializingOrUnusable,\n+\t\t\tStateIdle,\n+\t\t)\n+\t\treturn err == nil\n+\t}\n+\t// Trying to make unusable - transition from IDLE to UNUSABLE\n+\t// This is typically for acquiring the connection for background operations\n+\t// Use predefined slice to avoid allocation\n+\t_, err := cn.stateMachine.TryTransition(\n+\t\tvalidFromIdle,\n+\t\tStateUnusable,\n+\t)\n+\treturn err == nil\n+}\n+\n+// IsUsable returns true if the connection is safe to use for new commands (lock-free).\n+//\n+// A connection is \"usable\" when it's in a stable state and can be returned to clients.\n+// It becomes unusable during:\n+//   - Handoff operations (network connection replacement)\n+//   - Re-authentication (credential updates)\n+//   - Other background operations that need exclusive access\n+//\n+// Note: CREATED state is considered usable because new connections need to pass OnGet() hook\n+// before initialization. The initialization happens after OnGet() in the client code.\n+func (cn *Conn) IsUsable() bool {\n+\tstate := cn.stateMachine.GetState()\n+\t// CREATED, IDLE, and IN_USE states are considered usable\n+\t// CREATED: new connection, not yet initialized (will be initialized by client)\n+\t// IDLE: initialized and ready to be acquired\n+\t// IN_USE: usable but currently acquired by someone\n+\treturn state == StateCreated || state == StateIdle || state == StateInUse\n+}\n+\n+// SetUsable sets the usable flag for the connection (lock-free).\n+//\n+// Deprecated: Use GetStateMachine().Transition() directly for better state management.\n+// This method is kept for backwards compatibility.\n+//\n+// This should be called to mark a connection as usable after initialization or\n+// to release it after a background operation completes.\n+//\n+// Prefer CompareAndSwapUsable() when acquiring exclusive access to avoid race conditions.\n+// Deprecated: Use GetStateMachine().Transition() directly for better state management.\n+func (cn *Conn) SetUsable(usable bool) {\n+\tif usable {\n+\t\t// Transition to IDLE state (ready to be acquired)\n+\t\tcn.stateMachine.Transition(StateIdle)\n+\t} else {\n+\t\t// Transition to UNUSABLE state (for background operations)\n+\t\tcn.stateMachine.Transition(StateUnusable)\n+\t}\n+}\n+\n+// IsInited returns true if the connection has been initialized.\n+// This is a backward-compatible wrapper around the state machine.\n+func (cn *Conn) IsInited() bool {\n+\tstate := cn.stateMachine.GetState()\n+\t// Connection is initialized if it's in IDLE or any post-initialization state\n+\treturn state != StateCreated && state != StateInitializing && state != StateClosed\n+}\n+\n+// Used - State machine based implementation\n+\n+// CompareAndSwapUsed atomically compares and swaps the used flag (lock-free).\n+// This method is kept for backwards compatibility.\n+//\n+// This is the preferred method for acquiring a connection from the pool, as it\n+// ensures that only one goroutine marks the connection as used.\n+//\n+// Implementation: Uses state machine transitions IDLE  IN_USE\n+//\n+// Returns true if the swap was successful (old value matched), false otherwise.\n+// Deprecated: Use GetStateMachine().TryTransition() directly for better state management.\n+func (cn *Conn) CompareAndSwapUsed(old, new bool) bool {\n+\tif old == new {\n+\t\t// No change needed\n+\t\tcurrentState := cn.stateMachine.GetState()\n+\t\tcurrentUsed := (currentState == StateInUse)\n+\t\treturn currentUsed == old\n+\t}\n+\n+\tif !old && new {\n+\t\t// Acquiring: IDLE  IN_USE\n+\t\t// Use predefined slice to avoid allocation\n+\t\t_, err := cn.stateMachine.TryTransition(validFromCreatedOrIdle, StateInUse)\n+\t\treturn err == nil\n+\t} else {\n+\t\t// Releasing: IN_USE  IDLE\n+\t\t// Use predefined slice to avoid allocation\n+\t\t_, err := cn.stateMachine.TryTransition(validFromInUse, StateIdle)\n+\t\treturn err == nil\n+\t}\n+}\n+\n+// IsUsed returns true if the connection is currently in use (lock-free).\n+//\n+// Deprecated: Use GetStateMachine().GetState() == StateInUse directly for better clarity.\n+// This method is kept for backwards compatibility.\n+//\n+// A connection is \"used\" when it has been retrieved from the pool and is\n+// actively processing a command. Background operations (like re-auth) should\n+// wait until the connection is not used before executing commands.\n+func (cn *Conn) IsUsed() bool {\n+\treturn cn.stateMachine.GetState() == StateInUse\n+}\n+\n+// SetUsed sets the used flag for the connection (lock-free).\n+//\n+// This should be called when returning a connection to the pool (set to false)\n+// or when a single-connection pool retrieves its connection (set to true).\n+//\n+// Prefer CompareAndSwapUsed() when acquiring from a multi-connection pool to\n+// avoid race conditions.\n+// Deprecated: Use GetStateMachine().Transition() directly for better state management.\n+func (cn *Conn) SetUsed(val bool) {\n+\tif val {\n+\t\tcn.stateMachine.Transition(StateInUse)\n+\t} else {\n+\t\tcn.stateMachine.Transition(StateIdle)\n+\t}\n+}\n+\n+// getNetConn returns the current network connection using atomic load (lock-free).\n+// This is the fast path for accessing netConn without mutex overhead.\n+func (cn *Conn) getNetConn() net.Conn {\n+\tif v := cn.netConnAtomic.Load(); v != nil {\n+\t\tif wrapper, ok := v.(*atomicNetConn); ok {\n+\t\t\treturn wrapper.conn\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// setNetConn stores the network connection atomically (lock-free).\n+// This is used for the fast path of connection replacement.\n+func (cn *Conn) setNetConn(netConn net.Conn) {\n+\tcn.netConnAtomic.Store(&atomicNetConn{conn: netConn})\n+}\n+\n+// Handoff state management - atomic access to handoff metadata\n+\n+// ShouldHandoff returns true if connection needs handoff (lock-free).\n+func (cn *Conn) ShouldHandoff() bool {\n+\tif v := cn.handoffStateAtomic.Load(); v != nil {\n+\t\treturn v.(*HandoffState).ShouldHandoff\n+\t}\n+\treturn false\n+}\n+\n+// GetHandoffEndpoint returns the new endpoint for handoff (lock-free).\n+func (cn *Conn) GetHandoffEndpoint() string {\n+\tif v := cn.handoffStateAtomic.Load(); v != nil {\n+\t\treturn v.(*HandoffState).Endpoint\n+\t}\n+\treturn \"\"\n+}\n+\n+// GetMovingSeqID returns the sequence ID from the MOVING notification (lock-free).\n+func (cn *Conn) GetMovingSeqID() int64 {\n+\tif v := cn.handoffStateAtomic.Load(); v != nil {\n+\t\treturn v.(*HandoffState).SeqID\n+\t}\n+\treturn 0\n+}\n+\n+// GetHandoffInfo returns all handoff information atomically (lock-free).\n+// This method prevents race conditions by returning all handoff state in a single atomic operation.\n+// Returns (shouldHandoff, endpoint, seqID).\n+func (cn *Conn) GetHandoffInfo() (bool, string, int64) {\n+\tif v := cn.handoffStateAtomic.Load(); v != nil {\n+\t\tstate := v.(*HandoffState)\n+\t\treturn state.ShouldHandoff, state.Endpoint, state.SeqID\n+\t}\n+\treturn false, \"\", 0\n+}\n+\n+// HandoffRetries returns the current handoff retry count (lock-free).\n+func (cn *Conn) HandoffRetries() int {\n+\treturn int(cn.handoffRetriesAtomic.Load())\n+}\n+\n+// IncrementAndGetHandoffRetries atomically increments and returns handoff retries (lock-free).\n+func (cn *Conn) IncrementAndGetHandoffRetries(n int) int {\n+\treturn int(cn.handoffRetriesAtomic.Add(uint32(n)))\n+}\n+\n+// IsPooled returns true if the connection is managed by a pool and will be pooled on Put.\n+func (cn *Conn) IsPooled() bool {\n+\treturn cn.pooled\n+}\n+\n+// IsPubSub returns true if the connection is used for PubSub.\n+func (cn *Conn) IsPubSub() bool {\n+\treturn cn.pubsub\n+}\n+\n+// SetRelaxedTimeout sets relaxed timeouts for this connection during maintenanceNotifications upgrades.\n+// These timeouts will be used for all subsequent commands until the deadline expires.\n+// Uses atomic operations for lock-free access.\n+func (cn *Conn) SetRelaxedTimeout(readTimeout, writeTimeout time.Duration) {\n+\tcn.relaxedCounter.Add(1)\n+\tcn.relaxedReadTimeoutNs.Store(int64(readTimeout))\n+\tcn.relaxedWriteTimeoutNs.Store(int64(writeTimeout))\n+}\n+\n+// SetRelaxedTimeoutWithDeadline sets relaxed timeouts with an expiration deadline.\n+// After the deadline, timeouts automatically revert to normal values.\n+// Uses atomic operations for lock-free access.\n+func (cn *Conn) SetRelaxedTimeoutWithDeadline(readTimeout, writeTimeout time.Duration, deadline time.Time) {\n+\tcn.SetRelaxedTimeout(readTimeout, writeTimeout)\n+\tcn.relaxedDeadlineNs.Store(deadline.UnixNano())\n+}\n+\n+// ClearRelaxedTimeout removes relaxed timeouts, returning to normal timeout behavior.\n+// Uses atomic operations for lock-free access.\n+func (cn *Conn) ClearRelaxedTimeout() {\n+\t// Atomically decrement counter and check if we should clear\n+\tnewCount := cn.relaxedCounter.Add(-1)\n+\tdeadlineNs := cn.relaxedDeadlineNs.Load()\n+\tif newCount <= 0 && (deadlineNs == 0 || time.Now().UnixNano() >= deadlineNs) {\n+\t\t// Use atomic load to get current value for CAS to avoid stale value race\n+\t\tcurrent := cn.relaxedCounter.Load()\n+\t\tif current <= 0 && cn.relaxedCounter.CompareAndSwap(current, 0) {\n+\t\t\tcn.clearRelaxedTimeout()\n+\t\t}\n+\t}\n+}\n+\n+func (cn *Conn) clearRelaxedTimeout() {\n+\tcn.relaxedReadTimeoutNs.Store(0)\n+\tcn.relaxedWriteTimeoutNs.Store(0)\n+\tcn.relaxedDeadlineNs.Store(0)\n+\tcn.relaxedCounter.Store(0)\n+}\n+\n+// HasRelaxedTimeout returns true if relaxed timeouts are currently active on this connection.\n+// This checks both the timeout values and the deadline (if set).\n+// Uses atomic operations for lock-free access.\n+func (cn *Conn) HasRelaxedTimeout() bool {\n+\t// Fast path: no relaxed timeouts are set\n+\tif cn.relaxedCounter.Load() <= 0 {\n+\t\treturn false\n+\t}\n+\n+\treadTimeoutNs := cn.relaxedReadTimeoutNs.Load()\n+\twriteTimeoutNs := cn.relaxedWriteTimeoutNs.Load()\n+\n+\t// If no relaxed timeouts are set, return false\n+\tif readTimeoutNs <= 0 && writeTimeoutNs <= 0 {\n+\t\treturn false\n+\t}\n+\n+\tdeadlineNs := cn.relaxedDeadlineNs.Load()\n+\t// If no deadline is set, relaxed timeouts are active\n+\tif deadlineNs == 0 {\n+\t\treturn true\n+\t}\n+\n+\t// If deadline is set, check if it's still in the future\n+\treturn time.Now().UnixNano() < deadlineNs\n+}\n+\n+// getEffectiveReadTimeout returns the timeout to use for read operations.\n+// If relaxed timeout is set and not expired, it takes precedence over the provided timeout.\n+// This method automatically clears expired relaxed timeouts using atomic operations.\n+func (cn *Conn) getEffectiveReadTimeout(normalTimeout time.Duration) time.Duration {\n+\treadTimeoutNs := cn.relaxedReadTimeoutNs.Load()\n+\n+\t// Fast path: no relaxed timeout set\n+\tif readTimeoutNs <= 0 {\n+\t\treturn normalTimeout\n+\t}\n+\n+\tdeadlineNs := cn.relaxedDeadlineNs.Load()\n+\t// If no deadline is set, use relaxed timeout\n+\tif deadlineNs == 0 {\n+\t\treturn time.Duration(readTimeoutNs)\n+\t}\n+\n+\t// Use cached time to avoid expensive syscall (max 50ms staleness is acceptable for timeout checks)\n+\tnowNs := getCachedTimeNs()\n+\t// Check if deadline has passed\n+\tif nowNs < deadlineNs {\n+\t\t// Deadline is in the future, use relaxed timeout\n+\t\treturn time.Duration(readTimeoutNs)\n+\t} else {\n+\t\t// Deadline has passed, clear relaxed timeouts atomically and use normal timeout\n+\t\tnewCount := cn.relaxedCounter.Add(-1)\n+\t\tif newCount <= 0 {\n+\t\t\tinternal.Logger.Printf(context.Background(), logs.UnrelaxedTimeoutAfterDeadline(cn.GetID()))\n+\t\t\tcn.clearRelaxedTimeout()\n+\t\t}\n+\t\treturn normalTimeout\n+\t}\n+}\n+\n+// getEffectiveWriteTimeout returns the timeout to use for write operations.\n+// If relaxed timeout is set and not expired, it takes precedence over the provided timeout.\n+// This method automatically clears expired relaxed timeouts using atomic operations.\n+func (cn *Conn) getEffectiveWriteTimeout(normalTimeout time.Duration) time.Duration {\n+\twriteTimeoutNs := cn.relaxedWriteTimeoutNs.Load()\n+\n+\t// Fast path: no relaxed timeout set\n+\tif writeTimeoutNs <= 0 {\n+\t\treturn normalTimeout\n+\t}\n+\n+\tdeadlineNs := cn.relaxedDeadlineNs.Load()\n+\t// If no deadline is set, use relaxed timeout\n+\tif deadlineNs == 0 {\n+\t\treturn time.Duration(writeTimeoutNs)\n+\t}\n+\n+\t// Use cached time to avoid expensive syscall (max 50ms staleness is acceptable for timeout checks)\n+\tnowNs := getCachedTimeNs()\n+\t// Check if deadline has passed\n+\tif nowNs < deadlineNs {\n+\t\t// Deadline is in the future, use relaxed timeout\n+\t\treturn time.Duration(writeTimeoutNs)\n+\t} else {\n+\t\t// Deadline has passed, clear relaxed timeouts atomically and use normal timeout\n+\t\tnewCount := cn.relaxedCounter.Add(-1)\n+\t\tif newCount <= 0 {\n+\t\t\tinternal.Logger.Printf(context.Background(), logs.UnrelaxedTimeoutAfterDeadline(cn.GetID()))\n+\t\t\tcn.clearRelaxedTimeout()\n+\t\t}\n+\t\treturn normalTimeout\n+\t}\n+}\n+\n+func (cn *Conn) SetOnClose(fn func() error) {\n+\tcn.onClose = fn\n+}\n+\n+// SetInitConnFunc sets the connection initialization function to be called on reconnections.\n+func (cn *Conn) SetInitConnFunc(fn func(context.Context, *Conn) error) {\n+\tcn.initConnFunc = fn\n+}\n+\n+// ExecuteInitConn runs the stored connection initialization function if available.\n+func (cn *Conn) ExecuteInitConn(ctx context.Context) error {\n+\tif cn.initConnFunc != nil {\n+\t\treturn cn.initConnFunc(ctx, cn)\n+\t}\n+\treturn fmt.Errorf(\"redis: no initConnFunc set for conn[%d]\", cn.GetID())\n }\n \n func (cn *Conn) SetNetConn(netConn net.Conn) {\n-\tcn.netConn = netConn\n+\t// Store the new connection atomically first (lock-free)\n+\tcn.setNetConn(netConn)\n+\t// Protect reader reset operations to avoid data races\n+\t// Use write lock since we're modifying the reader state\n+\tcn.readerMu.Lock()\n \tcn.rd.Reset(netConn)\n+\tcn.readerMu.Unlock()\n+\n \tcn.bw.Reset(netConn)\n }\n \n+// GetNetConn safely returns the current network connection using atomic load (lock-free).\n+// This method is used by the pool for health checks and provides better performance.\n+func (cn *Conn) GetNetConn() net.Conn {\n+\treturn cn.getNetConn()\n+}\n+\n+// SetNetConnAndInitConn replaces the underlying connection and executes the initialization.\n+// This method ensures only one initialization can happen at a time by using atomic state transitions.\n+// If another goroutine is currently initializing, this will wait for it to complete.\n+func (cn *Conn) SetNetConnAndInitConn(ctx context.Context, netConn net.Conn) error {\n+\t// Wait for and transition to INITIALIZING state - this prevents concurrent initializations\n+\t// Valid from states: CREATED (first init), IDLE (reconnect), UNUSABLE (handoff/reauth)\n+\t// If another goroutine is initializing, we'll wait for it to finish\n+\t// if the context has a deadline, use that, otherwise use the connection read (relaxed) timeout\n+\t// which should be set during handoff. If it is not set, use a 5 second default\n+\tdeadline, ok := ctx.Deadline()\n+\tif !ok {\n+\t\tdeadline = time.Now().Add(cn.getEffectiveReadTimeout(5 * time.Second))\n+\t}\n+\twaitCtx, cancel := context.WithDeadline(ctx, deadline)\n+\tdefer cancel()\n+\t// Use predefined slice to avoid allocation\n+\tfinalState, err := cn.stateMachine.AwaitAndTransition(\n+\t\twaitCtx,\n+\t\tvalidFromCreatedIdleOrUnusable,\n+\t\tStateInitializing,\n+\t)\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"cannot initialize connection from state %s: %w\", finalState, err)\n+\t}\n+\n+\t// Replace the underlying connection\n+\tcn.SetNetConn(netConn)\n+\n+\t// Execute initialization\n+\t// NOTE: ExecuteInitConn (via baseClient.initConn) will transition to IDLE on success\n+\t// or CLOSED on failure. We don't need to do it here.\n+\t// NOTE: Initconn returns conn in IDLE state\n+\tinitErr := cn.ExecuteInitConn(ctx)\n+\tif initErr != nil {\n+\t\t// ExecuteInitConn already transitioned to CLOSED, just return the error\n+\t\treturn initErr\n+\t}\n+\n+\t// ExecuteInitConn already transitioned to IDLE\n+\treturn nil\n+}\n+\n+// MarkForHandoff marks the connection for handoff due to MOVING notification.\n+// Returns an error if the connection is already marked for handoff.\n+// Note: This only sets metadata - the connection state is not changed until OnPut.\n+// This allows the current user to finish using the connection before handoff.\n+func (cn *Conn) MarkForHandoff(newEndpoint string, seqID int64) error {\n+\t// Check if already marked for handoff\n+\tif cn.ShouldHandoff() {\n+\t\treturn errAlreadyMarkedForHandoff\n+\t}\n+\n+\t// Set handoff metadata atomically\n+\tcn.handoffStateAtomic.Store(&HandoffState{\n+\t\tShouldHandoff: true,\n+\t\tEndpoint:      newEndpoint,\n+\t\tSeqID:         seqID,\n+\t})\n+\treturn nil\n+}\n+\n+// MarkQueuedForHandoff marks the connection as queued for handoff processing.\n+// This makes the connection unusable until handoff completes.\n+// This is called from OnPut hook, where the connection is typically in IN_USE state.\n+// The pool will preserve the UNUSABLE state and not overwrite it with IDLE.\n+func (cn *Conn) MarkQueuedForHandoff() error {\n+\t// Get current handoff state\n+\tcurrentState := cn.handoffStateAtomic.Load()\n+\tif currentState == nil {\n+\t\treturn errNotMarkedForHandoff\n+\t}\n+\n+\tstate := currentState.(*HandoffState)\n+\tif !state.ShouldHandoff {\n+\t\treturn errNotMarkedForHandoff\n+\t}\n+\n+\t// Create new state with ShouldHandoff=false but preserve endpoint and seqID\n+\t// This prevents the connection from being queued multiple times while still\n+\t// allowing the worker to access the handoff metadata\n+\tnewState := &HandoffState{\n+\t\tShouldHandoff: false,\n+\t\tEndpoint:      state.Endpoint, // Preserve endpoint for handoff processing\n+\t\tSeqID:         state.SeqID,    // Preserve seqID for handoff processing\n+\t}\n+\n+\t// Atomic compare-and-swap to update state\n+\tif !cn.handoffStateAtomic.CompareAndSwap(currentState, newState) {\n+\t\t// State changed between load and CAS - retry or return error\n+\t\treturn errHandoffStateChanged\n+\t}\n+\n+\t// Transition to UNUSABLE from IN_USE (normal flow), IDLE (edge cases), or CREATED (tests/uninitialized)\n+\t// The connection is typically in IN_USE state when OnPut is called (normal Put flow)\n+\t// But in some edge cases or tests, it might be in IDLE or CREATED state\n+\t// The pool will detect this state change and preserve it (not overwrite with IDLE)\n+\t// Use predefined slice to avoid allocation\n+\tfinalState, err := cn.stateMachine.TryTransition(validFromCreatedInUseOrIdle, StateUnusable)\n+\tif err != nil {\n+\t\t// Check if already in UNUSABLE state (race condition or retry)\n+\t\t// ShouldHandoff should be false now, but check just in case\n+\t\tif finalState == StateUnusable && !cn.ShouldHandoff() {\n+\t\t\t// Already unusable - this is fine, keep the new handoff state\n+\t\t\treturn nil\n+\t\t}\n+\t\t// Restore the original state if transition fails for other reasons\n+\t\tcn.handoffStateAtomic.Store(currentState)\n+\t\treturn fmt.Errorf(\"failed to mark connection as unusable: %w\", err)\n+\t}\n+\treturn nil\n+}\n+\n+// GetID returns the unique identifier for this connection.\n+func (cn *Conn) GetID() uint64 {\n+\treturn cn.id\n+}\n+\n+// GetStateMachine returns the connection's state machine for advanced state management.\n+// This is primarily used by internal packages like maintnotifications for handoff processing.\n+func (cn *Conn) GetStateMachine() *ConnStateMachine {\n+\treturn cn.stateMachine\n+}\n+\n+// TryAcquire attempts to acquire the connection for use.\n+// This is an optimized inline method for the hot path (Get operation).\n+//\n+// It tries to transition from IDLE -> IN_USE or CREATED -> CREATED.\n+// Returns true if the connection was successfully acquired, false otherwise.\n+// The CREATED->CREATED is done so we can keep the state correct for later\n+// initialization of the connection in initConn.\n+//\n+// Performance: This is faster than calling GetStateMachine() + TryTransitionFast()\n+//\n+// NOTE: We directly access cn.stateMachine.state here instead of using the state machine's\n+// methods. This breaks encapsulation but is necessary for performance.\n+// The IDLE->IN_USE and CREATED->CREATED transitions don't need\n+// waiter notification, and benchmarks show 1-3% improvement. If the state machine ever\n+// needs to notify waiters on these transitions, update this to use TryTransitionFast().\n+func (cn *Conn) TryAcquire() bool {\n+\t// The || operator short-circuits, so only 1 CAS in the common case\n+\treturn cn.stateMachine.state.CompareAndSwap(uint32(StateIdle), uint32(StateInUse)) ||\n+\t\tcn.stateMachine.state.CompareAndSwap(uint32(StateCreated), uint32(StateCreated))\n+}\n+\n+// Release releases the connection back to the pool.\n+// This is an optimized inline method for the hot path (Put operation).\n+//\n+// It tries to transition from IN_USE -> IDLE.\n+// Returns true if the connection was successfully released, false otherwise.\n+//\n+// Performance: This is faster than calling GetStateMachine() + TryTransitionFast().\n+//\n+// NOTE: We directly access cn.stateMachine.state here instead of using the state machine's\n+// methods. This breaks encapsulation but is necessary for performance.\n+// If the state machine ever needs to notify waiters\n+// on this transition, update this to use TryTransitionFast().\n+func (cn *Conn) Release() bool {\n+\t// Inline the hot path - single CAS operation\n+\treturn cn.stateMachine.state.CompareAndSwap(uint32(StateInUse), uint32(StateIdle))\n+}\n+\n+// ClearHandoffState clears the handoff state after successful handoff.\n+// Makes the connection usable again.\n+func (cn *Conn) ClearHandoffState() {\n+\t// Clear handoff metadata\n+\tcn.handoffStateAtomic.Store(&HandoffState{\n+\t\tShouldHandoff: false,\n+\t\tEndpoint:      \"\",\n+\t\tSeqID:         0,\n+\t})\n+\n+\t// Reset retry counter\n+\tcn.handoffRetriesAtomic.Store(0)\n+\n+\t// Mark connection as usable again\n+\t// Use state machine directly instead of deprecated SetUsable\n+\t// probably done by initConn\n+\tcn.stateMachine.Transition(StateIdle)\n+}\n+\n+// HasBufferedData safely checks if the connection has buffered data.\n+// This method is used to avoid data races when checking for push notifications.\n+func (cn *Conn) HasBufferedData() bool {\n+\t// Use read lock for concurrent access to reader state\n+\tcn.readerMu.RLock()\n+\tdefer cn.readerMu.RUnlock()\n+\treturn cn.rd.Buffered() > 0\n+}\n+\n+// PeekReplyTypeSafe safely peeks at the reply type.\n+// This method is used to avoid data races when checking for push notifications.\n+func (cn *Conn) PeekReplyTypeSafe() (byte, error) {\n+\t// Use read lock for concurrent access to reader state\n+\tcn.readerMu.RLock()\n+\tdefer cn.readerMu.RUnlock()\n+\n+\tif cn.rd.Buffered() <= 0 {\n+\t\treturn 0, fmt.Errorf(\"redis: can't peek reply type, no data available\")\n+\t}\n+\treturn cn.rd.PeekReplyType()\n+}\n+\n func (cn *Conn) Write(b []byte) (int, error) {\n-\treturn cn.netConn.Write(b)\n+\t// Lock-free netConn access for better performance\n+\tif netConn := cn.getNetConn(); netConn != nil {\n+\t\treturn netConn.Write(b)\n+\t}\n+\treturn 0, net.ErrClosed\n }\n \n func (cn *Conn) RemoteAddr() net.Addr {\n-\tif cn.netConn != nil {\n-\t\treturn cn.netConn.RemoteAddr()\n+\t// Lock-free netConn access for better performance\n+\tif netConn := cn.getNetConn(); netConn != nil {\n+\t\treturn netConn.RemoteAddr()\n \t}\n \treturn nil\n }\n@@ -67,7 +806,16 @@ func (cn *Conn) WithReader(\n \tctx context.Context, timeout time.Duration, fn func(rd *proto.Reader) error,\n ) error {\n \tif timeout >= 0 {\n-\t\tif err := cn.netConn.SetReadDeadline(cn.deadline(ctx, timeout)); err != nil {\n+\t\t// Use relaxed timeout if set, otherwise use provided timeout\n+\t\teffectiveTimeout := cn.getEffectiveReadTimeout(timeout)\n+\n+\t\t// Get the connection directly from atomic storage\n+\t\tnetConn := cn.getNetConn()\n+\t\tif netConn == nil {\n+\t\t\treturn errConnectionNotAvailable\n+\t\t}\n+\n+\t\tif err := netConn.SetReadDeadline(cn.deadline(ctx, effectiveTimeout)); err != nil {\n \t\t\treturn err\n \t\t}\n \t}\n@@ -78,13 +826,25 @@ func (cn *Conn) WithWriter(\n \tctx context.Context, timeout time.Duration, fn func(wr *proto.Writer) error,\n ) error {\n \tif timeout >= 0 {\n-\t\tif err := cn.netConn.SetWriteDeadline(cn.deadline(ctx, timeout)); err != nil {\n-\t\t\treturn err\n+\t\t// Use relaxed timeout if set, otherwise use provided timeout\n+\t\teffectiveTimeout := cn.getEffectiveWriteTimeout(timeout)\n+\n+\t\t// Set write deadline on the connection\n+\t\tif netConn := cn.getNetConn(); netConn != nil {\n+\t\t\tif err := netConn.SetWriteDeadline(cn.deadline(ctx, effectiveTimeout)); err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// Connection is not available - return preallocated error\n+\t\t\treturn errConnNotAvailableForWrite\n \t\t}\n \t}\n \n+\t// Reset the buffered writer if needed, should not happen\n \tif cn.bw.Buffered() > 0 {\n-\t\tcn.bw.Reset(cn.netConn)\n+\t\tif netConn := cn.getNetConn(); netConn != nil {\n+\t\t\tcn.bw.Reset(netConn)\n+\t\t}\n \t}\n \n \tif err := fn(cn.wr); err != nil {\n@@ -94,13 +854,47 @@ func (cn *Conn) WithWriter(\n \treturn cn.bw.Flush()\n }\n \n+func (cn *Conn) IsClosed() bool {\n+\treturn cn.closed.Load() || cn.stateMachine.GetState() == StateClosed\n+}\n+\n func (cn *Conn) Close() error {\n-\treturn cn.netConn.Close()\n+\tcn.closed.Store(true)\n+\n+\t// Transition to CLOSED state\n+\tcn.stateMachine.Transition(StateClosed)\n+\n+\tif cn.onClose != nil {\n+\t\t// ignore error\n+\t\t_ = cn.onClose()\n+\t}\n+\n+\t// Lock-free netConn access for better performance\n+\tif netConn := cn.getNetConn(); netConn != nil {\n+\t\treturn netConn.Close()\n+\t}\n+\treturn nil\n+}\n+\n+// MaybeHasData tries to peek at the next byte in the socket without consuming it\n+// This is used to check if there are push notifications available\n+// Important: This will work on Linux, but not on Windows\n+func (cn *Conn) MaybeHasData() bool {\n+\t// Lock-free netConn access for better performance\n+\tif netConn := cn.getNetConn(); netConn != nil {\n+\t\treturn maybeHasData(netConn)\n+\t}\n+\treturn false\n }\n \n+// deadline computes the effective deadline time based on context and timeout.\n+// It updates the usedAt timestamp to now.\n+// Uses cached time to avoid expensive syscall (max 50ms staleness is acceptable for deadline calculation).\n func (cn *Conn) deadline(ctx context.Context, timeout time.Duration) time.Time {\n-\ttm := time.Now()\n-\tcn.SetUsedAt(tm)\n+\t// Use cached time for deadline calculation (called 2x per command: read + write)\n+\tnowNs := getCachedTimeNs()\n+\tcn.SetUsedAtNs(nowNs)\n+\ttm := time.Unix(0, nowNs)\n \n \tif timeout > 0 {\n \t\ttm = tm.Add(timeout)"
    },
    {
      "sha": "9e83dd833e53bfb0955744c8f8f40bf9ac8fa100",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/pool/conn_check.go",
      "status": "modified",
      "additions": 11,
      "deletions": 1,
      "changes": 12,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn_check.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn_check.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn_check.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -12,6 +12,9 @@ import (\n \n var errUnexpectedRead = errors.New(\"unexpected read from socket\")\n \n+// connCheck checks if the connection is still alive and if there is data in the socket\n+// it will try to peek at the next byte without consuming it since we may want to work with it\n+// later on (e.g. push notifications)\n func connCheck(conn net.Conn) error {\n \t// Reset previous timeout.\n \t_ = conn.SetDeadline(time.Time{})\n@@ -29,7 +32,9 @@ func connCheck(conn net.Conn) error {\n \n \tif err := rawConn.Read(func(fd uintptr) bool {\n \t\tvar buf [1]byte\n-\t\tn, err := syscall.Read(int(fd), buf[:])\n+\t\t// Use MSG_PEEK to peek at data without consuming it\n+\t\tn, _, err := syscall.Recvfrom(int(fd), buf[:], syscall.MSG_PEEK|syscall.MSG_DONTWAIT)\n+\n \t\tswitch {\n \t\tcase n == 0 && err == nil:\n \t\t\tsysErr = io.EOF\n@@ -47,3 +52,8 @@ func connCheck(conn net.Conn) error {\n \n \treturn sysErr\n }\n+\n+// maybeHasData checks if there is data in the socket without consuming it\n+func maybeHasData(conn net.Conn) bool {\n+\treturn connCheck(conn) == errUnexpectedRead\n+}"
    },
    {
      "sha": "f971d94c4728420c4d391ef218ee528f66893df4",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/pool/conn_check_dummy.go",
      "status": "modified",
      "additions": 13,
      "deletions": 2,
      "changes": 15,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn_check_dummy.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn_check_dummy.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn_check_dummy.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -2,8 +2,19 @@\n \n package pool\n \n-import \"net\"\n+import (\n+\t\"errors\"\n+\t\"net\"\n+)\n \n-func connCheck(conn net.Conn) error {\n+// errUnexpectedRead is placeholder error variable for non-unix build constraints\n+var errUnexpectedRead = errors.New(\"unexpected read from socket\")\n+\n+func connCheck(_ net.Conn) error {\n \treturn nil\n }\n+\n+// since we can't check for data on the socket, we just assume there is some\n+func maybeHasData(_ net.Conn) bool {\n+\treturn true\n+}"
    },
    {
      "sha": "2050a742b8dfe951c8fd6b8aa8b744841b95f084",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/pool/conn_state.go",
      "status": "added",
      "additions": 343,
      "deletions": 0,
      "changes": 343,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn_state.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn_state.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fconn_state.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,343 @@\n+package pool\n+\n+import (\n+\t\"container/list\"\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"sync\"\n+\t\"sync/atomic\"\n+)\n+\n+// ConnState represents the connection state in the state machine.\n+// States are designed to be lightweight and fast to check.\n+//\n+// State Transitions:\n+//   CREATED  INITIALIZING  IDLE  IN_USE\n+//                              \n+//                          UNUSABLE (handoff/reauth)\n+//                              \n+//                           IDLE/CLOSED\n+type ConnState uint32\n+\n+const (\n+\t// StateCreated - Connection just created, not yet initialized\n+\tStateCreated ConnState = iota\n+\n+\t// StateInitializing - Connection initialization in progress\n+\tStateInitializing\n+\n+\t// StateIdle - Connection initialized and idle in pool, ready to be acquired\n+\tStateIdle\n+\n+\t// StateInUse - Connection actively processing a command (retrieved from pool)\n+\tStateInUse\n+\n+\t// StateUnusable - Connection temporarily unusable due to background operation\n+\t// (handoff, reauth, etc.). Cannot be acquired from pool.\n+\tStateUnusable\n+\n+\t// StateClosed - Connection closed\n+\tStateClosed\n+)\n+\n+// Predefined state slices to avoid allocations in hot paths\n+var (\n+\tvalidFromInUse              = []ConnState{StateInUse}\n+\tvalidFromCreatedOrIdle      = []ConnState{StateCreated, StateIdle}\n+\tvalidFromCreatedInUseOrIdle = []ConnState{StateCreated, StateInUse, StateIdle}\n+\t// For AwaitAndTransition calls\n+\tvalidFromCreatedIdleOrUnusable = []ConnState{StateCreated, StateIdle, StateUnusable}\n+\tvalidFromIdle                  = []ConnState{StateIdle}\n+\t// For CompareAndSwapUsable\n+\tvalidFromInitializingOrUnusable = []ConnState{StateInitializing, StateUnusable}\n+)\n+\n+// Accessor functions for predefined slices to avoid allocations in external packages\n+// These return the same slice instance, so they're zero-allocation\n+\n+// ValidFromIdle returns a predefined slice containing only StateIdle.\n+// Use this to avoid allocations when calling AwaitAndTransition or TryTransition.\n+func ValidFromIdle() []ConnState {\n+\treturn validFromIdle\n+}\n+\n+// ValidFromCreatedIdleOrUnusable returns a predefined slice for initialization transitions.\n+// Use this to avoid allocations when calling AwaitAndTransition or TryTransition.\n+func ValidFromCreatedIdleOrUnusable() []ConnState {\n+\treturn validFromCreatedIdleOrUnusable\n+}\n+\n+// String returns a human-readable string representation of the state.\n+func (s ConnState) String() string {\n+\tswitch s {\n+\tcase StateCreated:\n+\t\treturn \"CREATED\"\n+\tcase StateInitializing:\n+\t\treturn \"INITIALIZING\"\n+\tcase StateIdle:\n+\t\treturn \"IDLE\"\n+\tcase StateInUse:\n+\t\treturn \"IN_USE\"\n+\tcase StateUnusable:\n+\t\treturn \"UNUSABLE\"\n+\tcase StateClosed:\n+\t\treturn \"CLOSED\"\n+\tdefault:\n+\t\treturn fmt.Sprintf(\"UNKNOWN(%d)\", s)\n+\t}\n+}\n+\n+var (\n+\t// ErrInvalidStateTransition is returned when a state transition is not allowed\n+\tErrInvalidStateTransition = errors.New(\"invalid state transition\")\n+\n+\t// ErrStateMachineClosed is returned when operating on a closed state machine\n+\tErrStateMachineClosed = errors.New(\"state machine is closed\")\n+\n+\t// ErrTimeout is returned when a state transition times out\n+\tErrTimeout = errors.New(\"state transition timeout\")\n+)\n+\n+// waiter represents a goroutine waiting for a state transition.\n+// Designed for minimal allocations and fast processing.\n+type waiter struct {\n+\tvalidStates map[ConnState]struct{} // States we're waiting for\n+\ttargetState ConnState              // State to transition to\n+\tdone        chan error             // Signaled when transition completes or times out\n+}\n+\n+// ConnStateMachine manages connection state transitions with FIFO waiting queue.\n+// Optimized for:\n+// - Lock-free reads (hot path)\n+// - Minimal allocations\n+// - Fast state transitions\n+// - FIFO fairness for waiters\n+// Note: Handoff metadata (endpoint, seqID, retries) is managed separately in the Conn struct.\n+type ConnStateMachine struct {\n+\t// Current state - atomic for lock-free reads\n+\tstate atomic.Uint32\n+\n+\t// FIFO queue for waiters - only locked during waiter add/remove/notify\n+\tmu          sync.Mutex\n+\twaiters     *list.List // List of *waiter\n+\twaiterCount atomic.Int32 // Fast lock-free check for waiters (avoids mutex in hot path)\n+}\n+\n+// NewConnStateMachine creates a new connection state machine.\n+// Initial state is StateCreated.\n+func NewConnStateMachine() *ConnStateMachine {\n+\tsm := &ConnStateMachine{\n+\t\twaiters: list.New(),\n+\t}\n+\tsm.state.Store(uint32(StateCreated))\n+\treturn sm\n+}\n+\n+// GetState returns the current state (lock-free read).\n+// This is the hot path - optimized for zero allocations and minimal overhead.\n+// Note: Zero allocations applies to state reads; converting the returned state to a string\n+// (via String()) may allocate if the state is unknown.\n+func (sm *ConnStateMachine) GetState() ConnState {\n+\treturn ConnState(sm.state.Load())\n+}\n+\n+// TryTransitionFast is an optimized version for the hot path (Get/Put operations).\n+// It only handles simple state transitions without waiter notification.\n+// This is safe because:\n+// 1. Get/Put don't need to wait for state changes\n+// 2. Background operations (handoff/reauth) use UNUSABLE state, which this won't match\n+// 3. If a background operation is in progress (state is UNUSABLE), this fails fast\n+//\n+// Returns true if transition succeeded, false otherwise.\n+// Use this for performance-critical paths where you don't need error details.\n+//\n+// Performance: Single CAS operation - as fast as the old atomic bool!\n+// For multiple from states, use: sm.TryTransitionFast(State1, Target) || sm.TryTransitionFast(State2, Target)\n+// The || operator short-circuits, so only 1 CAS is executed in the common case.\n+func (sm *ConnStateMachine) TryTransitionFast(fromState, targetState ConnState) bool {\n+\treturn sm.state.CompareAndSwap(uint32(fromState), uint32(targetState))\n+}\n+\n+// TryTransition attempts an immediate state transition without waiting.\n+// Returns the current state after the transition attempt and an error if the transition failed.\n+// The returned state is the CURRENT state (after the attempt), not the previous state.\n+// This is faster than AwaitAndTransition when you don't need to wait.\n+// Uses compare-and-swap to atomically transition, preventing concurrent transitions.\n+// This method does NOT wait - it fails immediately if the transition cannot be performed.\n+//\n+// Performance: Zero allocations on success path (hot path).\n+func (sm *ConnStateMachine) TryTransition(validFromStates []ConnState, targetState ConnState) (ConnState, error) {\n+\t// Try each valid from state with CAS\n+\t// This ensures only ONE goroutine can successfully transition at a time\n+\tfor _, fromState := range validFromStates {\n+\t\t// Try to atomically swap from fromState to targetState\n+\t\t// If successful, we won the race and can proceed\n+\t\tif sm.state.CompareAndSwap(uint32(fromState), uint32(targetState)) {\n+\t\t\t// Success! We transitioned atomically\n+\t\t\t// Hot path optimization: only check for waiters if transition succeeded\n+\t\t\t// This avoids atomic load on every Get/Put when no waiters exist\n+\t\t\tif sm.waiterCount.Load() > 0 {\n+\t\t\t\tsm.notifyWaiters()\n+\t\t\t}\n+\t\t\treturn targetState, nil\n+\t\t}\n+\t}\n+\n+\t// All CAS attempts failed - state is not valid for this transition\n+\t// Return the current state so caller can decide what to do\n+\t// Note: This error path allocates, but it's the exceptional case\n+\tcurrentState := sm.GetState()\n+\treturn currentState, fmt.Errorf(\"%w: cannot transition from %s to %s (valid from: %v)\",\n+\t\tErrInvalidStateTransition, currentState, targetState, validFromStates)\n+}\n+\n+// Transition unconditionally transitions to the target state.\n+// Use with caution - prefer AwaitAndTransition or TryTransition for safety.\n+// This is useful for error paths or when you know the transition is valid.\n+func (sm *ConnStateMachine) Transition(targetState ConnState) {\n+\tsm.state.Store(uint32(targetState))\n+\tsm.notifyWaiters()\n+}\n+\n+// AwaitAndTransition waits for the connection to reach one of the valid states,\n+// then atomically transitions to the target state.\n+// Returns the current state after the transition attempt and an error if the operation failed.\n+// The returned state is the CURRENT state (after the attempt), not the previous state.\n+// Returns error if timeout expires or context is cancelled.\n+//\n+// This method implements FIFO fairness - the first caller to wait gets priority\n+// when the state becomes available.\n+//\n+// Performance notes:\n+// - If already in a valid state, this is very fast (no allocation, no waiting)\n+// - If waiting is required, allocates one waiter struct and one channel\n+func (sm *ConnStateMachine) AwaitAndTransition(\n+\tctx context.Context,\n+\tvalidFromStates []ConnState,\n+\ttargetState ConnState,\n+) (ConnState, error) {\n+\t// Fast path: try immediate transition with CAS to prevent race conditions\n+\t// BUT: only if there are no waiters in the queue (to maintain FIFO ordering)\n+\tif sm.waiterCount.Load() == 0 {\n+\t\tfor _, fromState := range validFromStates {\n+\t\t\t// Check if we're already in target state\n+\t\t\tif fromState == targetState && sm.GetState() == targetState {\n+\t\t\t\treturn targetState, nil\n+\t\t\t}\n+\n+\t\t\t// Try to atomically swap from fromState to targetState\n+\t\t\tif sm.state.CompareAndSwap(uint32(fromState), uint32(targetState)) {\n+\t\t\t\t// Success! We transitioned atomically\n+\t\t\t\tsm.notifyWaiters()\n+\t\t\t\treturn targetState, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Fast path failed - check if we should wait or fail\n+\tcurrentState := sm.GetState()\n+\n+\t// Check if closed\n+\tif currentState == StateClosed {\n+\t\treturn currentState, ErrStateMachineClosed\n+\t}\n+\n+\t// Slow path: need to wait for state change\n+\t// Create waiter with valid states map for fast lookup\n+\tvalidStatesMap := make(map[ConnState]struct{}, len(validFromStates))\n+\tfor _, s := range validFromStates {\n+\t\tvalidStatesMap[s] = struct{}{}\n+\t}\n+\n+\tw := &waiter{\n+\t\tvalidStates: validStatesMap,\n+\t\ttargetState: targetState,\n+\t\tdone:        make(chan error, 1), // Buffered to avoid goroutine leak\n+\t}\n+\n+\t// Add to FIFO queue\n+\tsm.mu.Lock()\n+\telem := sm.waiters.PushBack(w)\n+\tsm.waiterCount.Add(1)\n+\tsm.mu.Unlock()\n+\n+\t// Wait for state change or timeout\n+\tselect {\n+\tcase <-ctx.Done():\n+\t\t// Timeout or cancellation - remove from queue\n+\t\tsm.mu.Lock()\n+\t\tsm.waiters.Remove(elem)\n+\t\tsm.waiterCount.Add(-1)\n+\t\tsm.mu.Unlock()\n+\t\treturn sm.GetState(), ctx.Err()\n+\tcase err := <-w.done:\n+\t\t// Transition completed (or failed)\n+\t\t// Note: waiterCount is decremented either in notifyWaiters (when the waiter is notified and removed)\n+\t\t// or here (on timeout/cancellation).\n+\t\treturn sm.GetState(), err\n+\t}\n+}\n+\n+// notifyWaiters checks if any waiters can proceed and notifies them in FIFO order.\n+// This is called after every state transition.\n+func (sm *ConnStateMachine) notifyWaiters() {\n+\t// Fast path: check atomic counter without acquiring lock\n+\t// This eliminates mutex overhead in the common case (no waiters)\n+\tif sm.waiterCount.Load() == 0 {\n+\t\treturn\n+\t}\n+\n+\tsm.mu.Lock()\n+\tdefer sm.mu.Unlock()\n+\n+\t// Double-check after acquiring lock (waiters might have been processed)\n+\tif sm.waiters.Len() == 0 {\n+\t\treturn\n+\t}\n+\n+\t// Process waiters in FIFO order until no more can be processed\n+\t// We loop instead of recursing to avoid stack overflow and mutex issues\n+\tfor {\n+\t\tprocessed := false\n+\n+\t\t// Find the first waiter that can proceed\n+\t\tfor elem := sm.waiters.Front(); elem != nil; elem = elem.Next() {\n+\t\t\tw := elem.Value.(*waiter)\n+\n+\t\t\t// Read current state inside the loop to get the latest value\n+\t\t\tcurrentState := sm.GetState()\n+\n+\t\t\t// Check if current state is valid for this waiter\n+\t\t\tif _, valid := w.validStates[currentState]; valid {\n+\t\t\t\t// Remove from queue first\n+\t\t\t\tsm.waiters.Remove(elem)\n+\t\t\t\tsm.waiterCount.Add(-1)\n+\n+\t\t\t\t// Use CAS to ensure state hasn't changed since we checked\n+\t\t\t\t// This prevents race condition where another thread changes state\n+\t\t\t\t// between our check and our transition\n+\t\t\t\tif sm.state.CompareAndSwap(uint32(currentState), uint32(w.targetState)) {\n+\t\t\t\t\t// Successfully transitioned - notify waiter\n+\t\t\t\t\tw.done <- nil\n+\t\t\t\t\tprocessed = true\n+\t\t\t\t\tbreak\n+\t\t\t\t} else {\n+\t\t\t\t\t// State changed - re-add waiter to front of queue to maintain FIFO ordering\n+\t\t\t\t\t// This waiter was first in line and should retain priority\n+\t\t\t\t\tsm.waiters.PushFront(w)\n+\t\t\t\t\tsm.waiterCount.Add(1)\n+\t\t\t\t\t// Continue to next iteration to re-read state\n+\t\t\t\t\tprocessed = true\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// If we didn't process any waiter, we're done\n+\t\tif !processed {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+}\n+"
    },
    {
      "sha": "a26e1976d53b5b74f2b1e4fc4484000c7c0bdb4e",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/pool/hooks.go",
      "status": "added",
      "additions": 165,
      "deletions": 0,
      "changes": 165,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fhooks.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fhooks.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fhooks.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,165 @@\n+package pool\n+\n+import (\n+\t\"context\"\n+\t\"sync\"\n+)\n+\n+// PoolHook defines the interface for connection lifecycle hooks.\n+type PoolHook interface {\n+\t// OnGet is called when a connection is retrieved from the pool.\n+\t// It can modify the connection or return an error to prevent its use.\n+\t// The accept flag can be used to prevent the connection from being used.\n+\t// On Accept = false the connection is rejected and returned to the pool.\n+\t// The error can be used to prevent the connection from being used and returned to the pool.\n+\t// On Errors, the connection is removed from the pool.\n+\t// It has isNewConn flag to indicate if this is a new connection (rather than idle from the pool)\n+\t// The flag can be used for gathering metrics on pool hit/miss ratio.\n+\tOnGet(ctx context.Context, conn *Conn, isNewConn bool) (accept bool, err error)\n+\n+\t// OnPut is called when a connection is returned to the pool.\n+\t// It returns whether the connection should be pooled and whether it should be removed.\n+\tOnPut(ctx context.Context, conn *Conn) (shouldPool bool, shouldRemove bool, err error)\n+\n+\t// OnRemove is called when a connection is removed from the pool.\n+\t// This happens when:\n+\t// - Connection fails health check\n+\t// - Connection exceeds max lifetime\n+\t// - Pool is being closed\n+\t// - Connection encounters an error\n+\t// Implementations should clean up any per-connection state.\n+\t// The reason parameter indicates why the connection was removed.\n+\tOnRemove(ctx context.Context, conn *Conn, reason error)\n+}\n+\n+// PoolHookManager manages multiple pool hooks.\n+type PoolHookManager struct {\n+\thooks   []PoolHook\n+\thooksMu sync.RWMutex\n+}\n+\n+// NewPoolHookManager creates a new pool hook manager.\n+func NewPoolHookManager() *PoolHookManager {\n+\treturn &PoolHookManager{\n+\t\thooks: make([]PoolHook, 0),\n+\t}\n+}\n+\n+// AddHook adds a pool hook to the manager.\n+// Hooks are called in the order they were added.\n+func (phm *PoolHookManager) AddHook(hook PoolHook) {\n+\tphm.hooksMu.Lock()\n+\tdefer phm.hooksMu.Unlock()\n+\tphm.hooks = append(phm.hooks, hook)\n+}\n+\n+// RemoveHook removes a pool hook from the manager.\n+func (phm *PoolHookManager) RemoveHook(hook PoolHook) {\n+\tphm.hooksMu.Lock()\n+\tdefer phm.hooksMu.Unlock()\n+\n+\tfor i, h := range phm.hooks {\n+\t\tif h == hook {\n+\t\t\t// Remove hook by swapping with last element and truncating\n+\t\t\tphm.hooks[i] = phm.hooks[len(phm.hooks)-1]\n+\t\t\tphm.hooks = phm.hooks[:len(phm.hooks)-1]\n+\t\t\tbreak\n+\t\t}\n+\t}\n+}\n+\n+// ProcessOnGet calls all OnGet hooks in order.\n+// If any hook returns an error, processing stops and the error is returned.\n+func (phm *PoolHookManager) ProcessOnGet(ctx context.Context, conn *Conn, isNewConn bool) (acceptConn bool, err error) {\n+\t// Copy slice reference while holding lock (fast)\n+\tphm.hooksMu.RLock()\n+\thooks := phm.hooks\n+\tphm.hooksMu.RUnlock()\n+\n+\t// Call hooks without holding lock (slow operations)\n+\tfor _, hook := range hooks {\n+\t\tacceptConn, err := hook.OnGet(ctx, conn, isNewConn)\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\n+\t\tif !acceptConn {\n+\t\t\treturn false, nil\n+\t\t}\n+\t}\n+\treturn true, nil\n+}\n+\n+// ProcessOnPut calls all OnPut hooks in order.\n+// The first hook that returns shouldRemove=true or shouldPool=false will stop processing.\n+func (phm *PoolHookManager) ProcessOnPut(ctx context.Context, conn *Conn) (shouldPool bool, shouldRemove bool, err error) {\n+\t// Copy slice reference while holding lock (fast)\n+\tphm.hooksMu.RLock()\n+\thooks := phm.hooks\n+\tphm.hooksMu.RUnlock()\n+\n+\tshouldPool = true // Default to pooling the connection\n+\n+\t// Call hooks without holding lock (slow operations)\n+\tfor _, hook := range hooks {\n+\t\thookShouldPool, hookShouldRemove, hookErr := hook.OnPut(ctx, conn)\n+\n+\t\tif hookErr != nil {\n+\t\t\treturn false, true, hookErr\n+\t\t}\n+\n+\t\t// If any hook says to remove or not pool, respect that decision\n+\t\tif hookShouldRemove {\n+\t\t\treturn false, true, nil\n+\t\t}\n+\n+\t\tif !hookShouldPool {\n+\t\t\tshouldPool = false\n+\t\t}\n+\t}\n+\n+\treturn shouldPool, false, nil\n+}\n+\n+// ProcessOnRemove calls all OnRemove hooks in order.\n+func (phm *PoolHookManager) ProcessOnRemove(ctx context.Context, conn *Conn, reason error) {\n+\t// Copy slice reference while holding lock (fast)\n+\tphm.hooksMu.RLock()\n+\thooks := phm.hooks\n+\tphm.hooksMu.RUnlock()\n+\n+\t// Call hooks without holding lock (slow operations)\n+\tfor _, hook := range hooks {\n+\t\thook.OnRemove(ctx, conn, reason)\n+\t}\n+}\n+\n+// GetHookCount returns the number of registered hooks (for testing).\n+func (phm *PoolHookManager) GetHookCount() int {\n+\tphm.hooksMu.RLock()\n+\tdefer phm.hooksMu.RUnlock()\n+\treturn len(phm.hooks)\n+}\n+\n+// GetHooks returns a copy of all registered hooks.\n+func (phm *PoolHookManager) GetHooks() []PoolHook {\n+\tphm.hooksMu.RLock()\n+\tdefer phm.hooksMu.RUnlock()\n+\n+\thooks := make([]PoolHook, len(phm.hooks))\n+\tcopy(hooks, phm.hooks)\n+\treturn hooks\n+}\n+\n+// Clone creates a copy of the hook manager with the same hooks.\n+// This is used for lock-free atomic updates of the hook manager.\n+func (phm *PoolHookManager) Clone() *PoolHookManager {\n+\tphm.hooksMu.RLock()\n+\tdefer phm.hooksMu.RUnlock()\n+\n+\tnewManager := &PoolHookManager{\n+\t\thooks: make([]PoolHook, len(phm.hooks)),\n+\t}\n+\tcopy(newManager.hooks, phm.hooks)\n+\treturn newManager\n+}"
    },
    {
      "sha": "d757d1f4fa2c6542917543b51b7120f7a6951813",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/pool/pool.go",
      "status": "modified",
      "additions": 678,
      "deletions": 141,
      "changes": 819,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpool.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpool.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpool.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -9,6 +9,8 @@ import (\n \t\"time\"\n \n \t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/proto\"\n+\t\"github.com/redis/go-redis/v9/internal/util\"\n )\n \n var (\n@@ -21,25 +23,48 @@ var (\n \n \t// ErrPoolTimeout timed out waiting to get a connection from the connection pool.\n \tErrPoolTimeout = errors.New(\"redis: connection pool timeout\")\n-)\n \n-var timers = sync.Pool{\n-\tNew: func() interface{} {\n-\t\tt := time.NewTimer(time.Hour)\n-\t\tt.Stop()\n-\t\treturn t\n-\t},\n-}\n+\t// ErrConnUnusableTimeout is returned when a connection is not usable and we timed out trying to mark it as unusable.\n+\tErrConnUnusableTimeout = errors.New(\"redis: timed out trying to mark connection as unusable\")\n+\n+\t// errHookRequestedRemoval is returned when a hook requests connection removal.\n+\terrHookRequestedRemoval = errors.New(\"hook requested removal\")\n+\n+\t// errConnNotPooled is returned when trying to return a non-pooled connection to the pool.\n+\terrConnNotPooled = errors.New(\"connection not pooled\")\n+\n+\t// popAttempts is the maximum number of attempts to find a usable connection\n+\t// when popping from the idle connection pool. This handles cases where connections\n+\t// are temporarily marked as unusable (e.g., during maintenanceNotifications upgrades or network issues).\n+\t// Value of 50 provides sufficient resilience without excessive overhead.\n+\t// This is capped by the idle connection count, so we won't loop excessively.\n+\tpopAttempts = 50\n+\n+\t// getAttempts is the maximum number of attempts to get a connection that passes\n+\t// hook validation (e.g., maintenanceNotifications upgrade hooks). This protects against race conditions\n+\t// where hooks might temporarily reject connections during cluster transitions.\n+\t// Value of 3 balances resilience with performance - most hook rejections resolve quickly.\n+\tgetAttempts = 3\n+\n+\tminTime      = time.Unix(-2208988800, 0) // Jan 1, 1900\n+\tmaxTime      = minTime.Add(1<<63 - 1)\n+\tnoExpiration = maxTime\n+)\n \n // Stats contains pool state information and accumulated stats.\n type Stats struct {\n-\tHits     uint32 // number of times free connection was found in the pool\n-\tMisses   uint32 // number of times free connection was NOT found in the pool\n-\tTimeouts uint32 // number of times a wait timeout occurred\n+\tHits           uint32 // number of times free connection was found in the pool\n+\tMisses         uint32 // number of times free connection was NOT found in the pool\n+\tTimeouts       uint32 // number of times a wait timeout occurred\n+\tWaitCount      uint32 // number of times a connection was waited\n+\tUnusable       uint32 // number of times a connection was found to be unusable\n+\tWaitDurationNs int64  // total time spent for waiting a connection in nanoseconds\n \n \tTotalConns uint32 // number of total connections in the pool\n \tIdleConns  uint32 // number of idle connections in the pool\n \tStaleConns uint32 // number of stale connections removed from the pool\n+\n+\tPubSubStats PubSubStats\n }\n \n type Pooler interface {\n@@ -54,20 +79,46 @@ type Pooler interface {\n \tIdleLen() int\n \tStats() *Stats\n \n+\t// Size returns the maximum pool size (capacity).\n+\t// This is used by the streaming credentials manager to size the re-auth worker pool.\n+\tSize() int\n+\n+\tAddPoolHook(hook PoolHook)\n+\tRemovePoolHook(hook PoolHook)\n+\n+\t// RemoveWithoutTurn removes a connection from the pool without freeing a turn.\n+\t// This should be used when removing a connection from a context that didn't acquire\n+\t// a turn via Get() (e.g., background workers, cleanup tasks).\n+\t// For normal removal after Get(), use Remove() instead.\n+\tRemoveWithoutTurn(context.Context, *Conn, error)\n+\n \tClose() error\n }\n \n type Options struct {\n-\tDialer func(context.Context) (net.Conn, error)\n-\n-\tPoolFIFO        bool\n-\tPoolSize        int\n-\tPoolTimeout     time.Duration\n-\tMinIdleConns    int\n-\tMaxIdleConns    int\n-\tMaxActiveConns  int\n-\tConnMaxIdleTime time.Duration\n-\tConnMaxLifetime time.Duration\n+\tDialer          func(context.Context) (net.Conn, error)\n+\tReadBufferSize  int\n+\tWriteBufferSize int\n+\n+\tPoolFIFO                 bool\n+\tPoolSize                 int32\n+\tMaxConcurrentDials       int\n+\tDialTimeout              time.Duration\n+\tPoolTimeout              time.Duration\n+\tMinIdleConns             int32\n+\tMaxIdleConns             int32\n+\tMaxActiveConns           int32\n+\tConnMaxIdleTime          time.Duration\n+\tConnMaxLifetime          time.Duration\n+\tPushNotificationsEnabled bool\n+\n+\t// DialerRetries is the maximum number of retry attempts when dialing fails.\n+\t// Default: 5\n+\tDialerRetries int\n+\n+\t// DialerRetryTimeout is the backoff duration between retry attempts.\n+\t// Default: 100ms\n+\tDialerRetryTimeout time.Duration\n }\n \n type lastDialErrorWrap struct {\n@@ -80,71 +131,165 @@ type ConnPool struct {\n \tdialErrorsNum uint32 // atomic\n \tlastDialError atomic.Value\n \n-\tqueue chan struct{}\n+\tqueue           chan struct{}\n+\tdialsInProgress chan struct{}\n+\tdialsQueue      *wantConnQueue\n+\t// Fast semaphore for connection limiting with eventual fairness\n+\t// Uses fast path optimization to avoid timer allocation when tokens are available\n+\tsemaphore *internal.FastSemaphore\n \n \tconnsMu   sync.Mutex\n-\tconns     []*Conn\n+\tconns     map[uint64]*Conn\n \tidleConns []*Conn\n \n-\tpoolSize     int\n-\tidleConnsLen int\n+\tpoolSize            atomic.Int32\n+\tidleConnsLen        atomic.Int32\n+\tidleCheckInProgress atomic.Bool\n+\tidleCheckNeeded     atomic.Bool\n \n-\tstats Stats\n+\tstats          Stats\n+\twaitDurationNs atomic.Int64\n \n \t_closed uint32 // atomic\n+\n+\t// Pool hooks manager for flexible connection processing\n+\t// Using atomic.Pointer for lock-free reads in hot paths (Get/Put)\n+\thookManager atomic.Pointer[PoolHookManager]\n }\n \n var _ Pooler = (*ConnPool)(nil)\n \n func NewConnPool(opt *Options) *ConnPool {\n \tp := &ConnPool{\n-\t\tcfg: opt,\n-\n-\t\tqueue:     make(chan struct{}, opt.PoolSize),\n-\t\tconns:     make([]*Conn, 0, opt.PoolSize),\n-\t\tidleConns: make([]*Conn, 0, opt.PoolSize),\n+\t\tcfg:             opt,\n+\t\tsemaphore:       internal.NewFastSemaphore(opt.PoolSize),\n+\t\tqueue:           make(chan struct{}, opt.PoolSize),\n+\t\tconns:           make(map[uint64]*Conn),\n+\t\tdialsInProgress: make(chan struct{}, opt.MaxConcurrentDials),\n+\t\tdialsQueue:      newWantConnQueue(),\n+\t\tidleConns:       make([]*Conn, 0, opt.PoolSize),\n \t}\n \n-\tp.connsMu.Lock()\n-\tp.checkMinIdleConns()\n-\tp.connsMu.Unlock()\n+\t// Only create MinIdleConns if explicitly requested (> 0)\n+\t// This avoids creating connections during pool initialization for tests\n+\tif opt.MinIdleConns > 0 {\n+\t\tp.connsMu.Lock()\n+\t\tp.checkMinIdleConns()\n+\t\tp.connsMu.Unlock()\n+\t}\n \n \treturn p\n }\n \n+// initializeHooks sets up the pool hooks system.\n+func (p *ConnPool) initializeHooks() {\n+\tmanager := NewPoolHookManager()\n+\tp.hookManager.Store(manager)\n+}\n+\n+// AddPoolHook adds a pool hook to the pool.\n+func (p *ConnPool) AddPoolHook(hook PoolHook) {\n+\t// Lock-free read of current manager\n+\tmanager := p.hookManager.Load()\n+\tif manager == nil {\n+\t\tp.initializeHooks()\n+\t\tmanager = p.hookManager.Load()\n+\t}\n+\n+\t// Create new manager with added hook\n+\tnewManager := manager.Clone()\n+\tnewManager.AddHook(hook)\n+\n+\t// Atomically swap to new manager\n+\tp.hookManager.Store(newManager)\n+}\n+\n+// RemovePoolHook removes a pool hook from the pool.\n+func (p *ConnPool) RemovePoolHook(hook PoolHook) {\n+\tmanager := p.hookManager.Load()\n+\tif manager != nil {\n+\t\t// Create new manager with removed hook\n+\t\tnewManager := manager.Clone()\n+\t\tnewManager.RemoveHook(hook)\n+\n+\t\t// Atomically swap to new manager\n+\t\tp.hookManager.Store(newManager)\n+\t}\n+}\n+\n func (p *ConnPool) checkMinIdleConns() {\n+\t// If a check is already in progress, mark that we need another check and return\n+\tif !p.idleCheckInProgress.CompareAndSwap(false, true) {\n+\t\tp.idleCheckNeeded.Store(true)\n+\t\treturn\n+\t}\n+\n \tif p.cfg.MinIdleConns == 0 {\n+\t\tp.idleCheckInProgress.Store(false)\n \t\treturn\n \t}\n-\tfor p.poolSize < p.cfg.PoolSize && p.idleConnsLen < p.cfg.MinIdleConns {\n-\t\tselect {\n-\t\tcase p.queue <- struct{}{}:\n-\t\t\tp.poolSize++\n-\t\t\tp.idleConnsLen++\n \n+\t// Keep checking until no more checks are needed\n+\t// This handles the case where multiple Remove() calls happen concurrently\n+\tfor {\n+\t\t// Clear the \"check needed\" flag before we start\n+\t\tp.idleCheckNeeded.Store(false)\n+\n+\t\t// Only create idle connections if we haven't reached the total pool size limit\n+\t\t// MinIdleConns should be a subset of PoolSize, not additional connections\n+\t\tfor p.poolSize.Load() < p.cfg.PoolSize && p.idleConnsLen.Load() < p.cfg.MinIdleConns {\n+\t\t\t// Try to acquire a semaphore token\n+\t\t\tif !p.semaphore.TryAcquire() {\n+\t\t\t\t// Semaphore is full, can't create more connections\n+\t\t\t\tp.idleCheckInProgress.Store(false)\n+\t\t\t\treturn\n+\t\t\t}\n+\n+\t\t\tp.poolSize.Add(1)\n+\t\t\tp.idleConnsLen.Add(1)\n \t\t\tgo func() {\n+\t\t\t\tdefer func() {\n+\t\t\t\t\tif err := recover(); err != nil {\n+\t\t\t\t\t\tp.poolSize.Add(-1)\n+\t\t\t\t\t\tp.idleConnsLen.Add(-1)\n+\n+\t\t\t\t\t\tp.freeTurn()\n+\t\t\t\t\t\tinternal.Logger.Printf(context.Background(), \"addIdleConn panic: %+v\", err)\n+\t\t\t\t\t}\n+\t\t\t\t}()\n+\n \t\t\t\terr := p.addIdleConn()\n \t\t\t\tif err != nil && err != ErrClosed {\n-\t\t\t\t\tp.connsMu.Lock()\n-\t\t\t\t\tp.poolSize--\n-\t\t\t\t\tp.idleConnsLen--\n-\t\t\t\t\tp.connsMu.Unlock()\n+\t\t\t\t\tp.poolSize.Add(-1)\n+\t\t\t\t\tp.idleConnsLen.Add(-1)\n \t\t\t\t}\n-\n \t\t\t\tp.freeTurn()\n \t\t\t}()\n-\t\tdefault:\n+\t\t}\n+\n+\t\t// If no one requested another check while we were working, we're done\n+\t\tif !p.idleCheckNeeded.Load() {\n+\t\t\tp.idleCheckInProgress.Store(false)\n \t\t\treturn\n \t\t}\n+\n+\t\t// Otherwise, loop again to handle the new requests\n \t}\n }\n \n func (p *ConnPool) addIdleConn() error {\n-\tcn, err := p.dialConn(context.TODO(), true)\n+\tctx, cancel := context.WithTimeout(context.Background(), p.cfg.DialTimeout)\n+\tdefer cancel()\n+\n+\tcn, err := p.dialConn(ctx, true)\n \tif err != nil {\n \t\treturn err\n \t}\n \n+\t// NOTE: Connection is in CREATED state and will be initialized by redis.go:initConn()\n+\t// when first acquired from the pool. Do NOT transition to IDLE here - that happens\n+\t// after initialization completes.\n+\n \tp.connsMu.Lock()\n \tdefer p.connsMu.Unlock()\n \n@@ -154,11 +299,15 @@ func (p *ConnPool) addIdleConn() error {\n \t\treturn ErrClosed\n \t}\n \n-\tp.conns = append(p.conns, cn)\n+\tp.conns[cn.GetID()] = cn\n \tp.idleConns = append(p.idleConns, cn)\n \treturn nil\n }\n \n+// NewConn creates a new connection and returns it to the user.\n+// This will still obey MaxActiveConns but will not include it in the pool and won't increase the pool size.\n+//\n+// NOTE: If you directly get a connection from the pool, it won't be pooled and won't support maintnotifications upgrades.\n func (p *ConnPool) NewConn(ctx context.Context) (*Conn, error) {\n \treturn p.newConn(ctx, false)\n }\n@@ -168,33 +317,45 @@ func (p *ConnPool) newConn(ctx context.Context, pooled bool) (*Conn, error) {\n \t\treturn nil, ErrClosed\n \t}\n \n-\tp.connsMu.Lock()\n-\tif p.cfg.MaxActiveConns > 0 && p.poolSize >= p.cfg.MaxActiveConns {\n-\t\tp.connsMu.Unlock()\n+\tif p.cfg.MaxActiveConns > 0 && p.poolSize.Load() >= p.cfg.MaxActiveConns {\n \t\treturn nil, ErrPoolExhausted\n \t}\n-\tp.connsMu.Unlock()\n \n-\tcn, err := p.dialConn(ctx, pooled)\n+\tdialCtx, cancel := context.WithTimeout(ctx, p.cfg.DialTimeout)\n+\tdefer cancel()\n+\tcn, err := p.dialConn(dialCtx, pooled)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n \n-\tp.connsMu.Lock()\n-\tdefer p.connsMu.Unlock()\n+\t// NOTE: Connection is in CREATED state and will be initialized by redis.go:initConn()\n+\t// when first used. Do NOT transition to IDLE here - that happens after initialization completes.\n+\t// The state machine flow is: CREATED  INITIALIZING (in initConn)  IDLE (after init success)\n \n-\tif p.cfg.MaxActiveConns > 0 && p.poolSize >= p.cfg.MaxActiveConns {\n+\tif p.cfg.MaxActiveConns > 0 && p.poolSize.Load() > p.cfg.MaxActiveConns {\n \t\t_ = cn.Close()\n \t\treturn nil, ErrPoolExhausted\n \t}\n \n-\tp.conns = append(p.conns, cn)\n+\tp.connsMu.Lock()\n+\tdefer p.connsMu.Unlock()\n+\tif p.closed() {\n+\t\t_ = cn.Close()\n+\t\treturn nil, ErrClosed\n+\t}\n+\t// Check if pool was closed while we were waiting for the lock\n+\tif p.conns == nil {\n+\t\tp.conns = make(map[uint64]*Conn)\n+\t}\n+\tp.conns[cn.GetID()] = cn\n+\n \tif pooled {\n \t\t// If pool is full remove the cn on next Put.\n-\t\tif p.poolSize >= p.cfg.PoolSize {\n+\t\tcurrentPoolSize := p.poolSize.Load()\n+\t\tif currentPoolSize >= p.cfg.PoolSize {\n \t\t\tcn.pooled = false\n \t\t} else {\n-\t\t\tp.poolSize++\n+\t\t\tp.poolSize.Add(1)\n \t\t}\n \t}\n \n@@ -210,18 +371,58 @@ func (p *ConnPool) dialConn(ctx context.Context, pooled bool) (*Conn, error) {\n \t\treturn nil, p.getLastDialError()\n \t}\n \n-\tnetConn, err := p.cfg.Dialer(ctx)\n-\tif err != nil {\n-\t\tp.setLastDialError(err)\n-\t\tif atomic.AddUint32(&p.dialErrorsNum, 1) == uint32(p.cfg.PoolSize) {\n-\t\t\tgo p.tryDial()\n+\t// Retry dialing with backoff\n+\t// the context timeout is already handled by the context passed in\n+\t// so we may never reach the max retries, higher values don't hurt\n+\tmaxRetries := p.cfg.DialerRetries\n+\tif maxRetries <= 0 {\n+\t\tmaxRetries = 5 // Default value\n+\t}\n+\tbackoffDuration := p.cfg.DialerRetryTimeout\n+\tif backoffDuration <= 0 {\n+\t\tbackoffDuration = 100 * time.Millisecond // Default value\n+\t}\n+\n+\tvar lastErr error\n+\tshouldLoop := true\n+\t// when the timeout is reached, we should stop retrying\n+\t// but keep the lastErr to return to the caller\n+\t// instead of a generic context deadline exceeded error\n+\tattempt := 0\n+\tfor attempt = 0; (attempt < maxRetries) && shouldLoop; attempt++ {\n+\t\tnetConn, err := p.cfg.Dialer(ctx)\n+\t\tif err != nil {\n+\t\t\tlastErr = err\n+\t\t\t// Add backoff delay for retry attempts\n+\t\t\t// (not for the first attempt, do at least one)\n+\t\t\tselect {\n+\t\t\tcase <-ctx.Done():\n+\t\t\t\tshouldLoop = false\n+\t\t\tcase <-time.After(backoffDuration):\n+\t\t\t\t// Continue with retry\n+\t\t\t}\n+\t\t\tcontinue\n \t\t}\n-\t\treturn nil, err\n+\n+\t\t// Success - create connection\n+\t\tcn := NewConnWithBufferSize(netConn, p.cfg.ReadBufferSize, p.cfg.WriteBufferSize)\n+\t\tcn.pooled = pooled\n+\t\tif p.cfg.ConnMaxLifetime > 0 {\n+\t\t\tcn.expiresAt = time.Now().Add(p.cfg.ConnMaxLifetime)\n+\t\t} else {\n+\t\t\tcn.expiresAt = noExpiration\n+\t\t}\n+\n+\t\treturn cn, nil\n \t}\n \n-\tcn := NewConn(netConn)\n-\tcn.pooled = pooled\n-\treturn cn, nil\n+\tinternal.Logger.Printf(ctx, \"redis: connection pool: failed to dial after %d attempts: %v\", attempt, lastErr)\n+\t// All retries failed - handle error tracking\n+\tp.setLastDialError(lastErr)\n+\tif atomic.AddUint32(&p.dialErrorsNum, 1) == uint32(p.cfg.PoolSize) {\n+\t\tgo p.tryDial()\n+\t}\n+\treturn nil, lastErr\n }\n \n func (p *ConnPool) tryDial() {\n@@ -230,15 +431,19 @@ func (p *ConnPool) tryDial() {\n \t\t\treturn\n \t\t}\n \n-\t\tconn, err := p.cfg.Dialer(context.Background())\n+\t\tctx, cancel := context.WithTimeout(context.Background(), p.cfg.DialTimeout)\n+\n+\t\tconn, err := p.cfg.Dialer(ctx)\n \t\tif err != nil {\n \t\t\tp.setLastDialError(err)\n \t\t\ttime.Sleep(time.Second)\n+\t\t\tcancel()\n \t\t\tcontinue\n \t\t}\n \n \t\tatomic.StoreUint32(&p.dialErrorsNum, 0)\n \t\t_ = conn.Close()\n+\t\tcancel()\n \t\treturn\n \t}\n }\n@@ -257,6 +462,14 @@ func (p *ConnPool) getLastDialError() error {\n \n // Get returns existed connection from the pool or creates a new one.\n func (p *ConnPool) Get(ctx context.Context) (*Conn, error) {\n+\treturn p.getConn(ctx)\n+}\n+\n+// getConn returns a connection from the pool.\n+func (p *ConnPool) getConn(ctx context.Context) (*Conn, error) {\n+\tvar cn *Conn\n+\tvar err error\n+\n \tif p.closed() {\n \t\treturn nil, ErrClosed\n \t}\n@@ -265,9 +478,16 @@ func (p *ConnPool) Get(ctx context.Context) (*Conn, error) {\n \t\treturn nil, err\n \t}\n \n-\tfor {\n+\t// Use cached time for health checks (max 50ms staleness is acceptable)\n+\tnowNs := getCachedTimeNs()\n+\n+\t// Lock-free atomic read - no mutex overhead!\n+\thookManager := p.hookManager.Load()\n+\n+\tfor attempts := 0; attempts < getAttempts; attempts++ {\n+\n \t\tp.connsMu.Lock()\n-\t\tcn, err := p.popIdle()\n+\t\tcn, err = p.popIdle()\n \t\tp.connsMu.Unlock()\n \n \t\tif err != nil {\n@@ -279,127 +499,394 @@ func (p *ConnPool) Get(ctx context.Context) (*Conn, error) {\n \t\t\tbreak\n \t\t}\n \n-\t\tif !p.isHealthyConn(cn) {\n+\t\tif !p.isHealthyConn(cn, nowNs) {\n \t\t\t_ = p.CloseConn(cn)\n \t\t\tcontinue\n \t\t}\n \n+\t\t// Process connection using the hooks system\n+\t\t// Combine error and rejection checks to reduce branches\n+\t\tif hookManager != nil {\n+\t\t\tacceptConn, err := hookManager.ProcessOnGet(ctx, cn, false)\n+\t\t\tif err != nil || !acceptConn {\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tinternal.Logger.Printf(ctx, \"redis: connection pool: failed to process idle connection by hook: %v\", err)\n+\t\t\t\t\t_ = p.CloseConn(cn)\n+\t\t\t\t} else {\n+\t\t\t\t\tinternal.Logger.Printf(ctx, \"redis: connection pool: conn[%d] rejected by hook, returning to pool\", cn.GetID())\n+\t\t\t\t\t// Return connection to pool without freeing the turn that this Get() call holds.\n+\t\t\t\t\t// We use putConnWithoutTurn() to run all the Put hooks and logic without freeing a turn.\n+\t\t\t\t\tp.putConnWithoutTurn(ctx, cn)\n+\t\t\t\t\tcn = nil\n+\t\t\t\t}\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t}\n+\n \t\tatomic.AddUint32(&p.stats.Hits, 1)\n \t\treturn cn, nil\n \t}\n \n \tatomic.AddUint32(&p.stats.Misses, 1)\n \n-\tnewcn, err := p.newConn(ctx, true)\n+\tnewcn, err := p.queuedNewConn(ctx)\n \tif err != nil {\n-\t\tp.freeTurn()\n \t\treturn nil, err\n \t}\n \n+\t// Process connection using the hooks system\n+\tif hookManager != nil {\n+\t\tacceptConn, err := hookManager.ProcessOnGet(ctx, newcn, true)\n+\t\t// both errors and accept=false mean a hook rejected the connection\n+\t\t// this should not happen with a new connection, but we handle it gracefully\n+\t\tif err != nil || !acceptConn {\n+\t\t\t// Failed to process connection, discard it\n+\t\t\tinternal.Logger.Printf(ctx, \"redis: connection pool: failed to process new connection conn[%d] by hook: accept=%v, err=%v\", newcn.GetID(), acceptConn, err)\n+\t\t\t_ = p.CloseConn(newcn)\n+\t\t\treturn nil, err\n+\t\t}\n+\t}\n \treturn newcn, nil\n }\n \n-func (p *ConnPool) waitTurn(ctx context.Context) error {\n+func (p *ConnPool) queuedNewConn(ctx context.Context) (*Conn, error) {\n \tselect {\n+\tcase p.dialsInProgress <- struct{}{}:\n+\t\t// Got permission, proceed to create connection\n \tcase <-ctx.Done():\n-\t\treturn ctx.Err()\n-\tdefault:\n+\t\tp.freeTurn()\n+\t\treturn nil, ctx.Err()\n \t}\n \n-\tselect {\n-\tcase p.queue <- struct{}{}:\n-\t\treturn nil\n-\tdefault:\n+\tdialCtx, cancel := context.WithTimeout(context.Background(), p.cfg.DialTimeout)\n+\n+\tw := &wantConn{\n+\t\tctx:       dialCtx,\n+\t\tcancelCtx: cancel,\n+\t\tresult:    make(chan wantConnResult, 1),\n \t}\n+\tvar err error\n+\tdefer func() {\n+\t\tif err != nil {\n+\t\t\tif cn := w.cancel(); cn != nil && p.putIdleConn(ctx, cn) {\n+\t\t\t\tp.freeTurn()\n+\t\t\t}\n+\t\t}\n+\t}()\n+\n+\tp.dialsQueue.enqueue(w)\n+\n+\tgo func(w *wantConn) {\n+\t\tvar freeTurnCalled bool\n+\t\tdefer func() {\n+\t\t\tif err := recover(); err != nil {\n+\t\t\t\tif !freeTurnCalled {\n+\t\t\t\t\tp.freeTurn()\n+\t\t\t\t}\n+\t\t\t\tinternal.Logger.Printf(context.Background(), \"queuedNewConn panic: %+v\", err)\n+\t\t\t}\n+\t\t}()\n+\n+\t\tdefer w.cancelCtx()\n+\t\tdefer func() { <-p.dialsInProgress }() // Release connection creation permission\n \n-\ttimer := timers.Get().(*time.Timer)\n-\ttimer.Reset(p.cfg.PoolTimeout)\n+\t\tdialCtx := w.getCtxForDial()\n+\t\tcn, cnErr := p.newConn(dialCtx, true)\n+\t\tif cnErr != nil {\n+\t\t\tw.tryDeliver(nil, cnErr) // deliver error to caller, notify connection creation failed\n+\t\t\tp.freeTurn()\n+\t\t\tfreeTurnCalled = true\n+\t\t\treturn\n+\t\t}\n+\n+\t\tdelivered := w.tryDeliver(cn, cnErr)\n+\t\tif !delivered && p.putIdleConn(dialCtx, cn) {\n+\t\t\tp.freeTurn()\n+\t\t\tfreeTurnCalled = true\n+\t\t}\n+\t}(w)\n \n \tselect {\n \tcase <-ctx.Done():\n-\t\tif !timer.Stop() {\n-\t\t\t<-timer.C\n+\t\terr = ctx.Err()\n+\t\treturn nil, err\n+\tcase result := <-w.result:\n+\t\terr = result.err\n+\t\treturn result.cn, err\n+\t}\n+}\n+\n+// putIdleConn puts a connection back to the pool or passes it to the next waiting request.\n+//\n+// It returns true if the connection was put back to the pool,\n+// which means the turn needs to be freed directly by the caller,\n+// or false if the connection was passed to the next waiting request,\n+// which means the turn will be freed by the waiting goroutine after it returns.\n+func (p *ConnPool) putIdleConn(ctx context.Context, cn *Conn) bool {\n+\tfor {\n+\t\tw, ok := p.dialsQueue.dequeue()\n+\t\tif !ok {\n+\t\t\tbreak\n \t\t}\n-\t\ttimers.Put(timer)\n-\t\treturn ctx.Err()\n-\tcase p.queue <- struct{}{}:\n-\t\tif !timer.Stop() {\n-\t\t\t<-timer.C\n+\t\tif w.tryDeliver(cn, nil) {\n+\t\t\treturn false\n \t\t}\n-\t\ttimers.Put(timer)\n+\t}\n+\n+\tp.connsMu.Lock()\n+\tdefer p.connsMu.Unlock()\n+\n+\tif p.closed() {\n+\t\t_ = cn.Close()\n+\t\treturn true\n+\t}\n+\n+\t// poolSize is increased in newConn\n+\tp.idleConns = append(p.idleConns, cn)\n+\tp.idleConnsLen.Add(1)\n+\n+\treturn true\n+}\n+\n+func (p *ConnPool) waitTurn(ctx context.Context) error {\n+\t// Fast path: check context first\n+\tselect {\n+\tcase <-ctx.Done():\n+\t\treturn ctx.Err()\n+\tdefault:\n+\t}\n+\n+\t// Fast path: try to acquire without blocking\n+\tif p.semaphore.TryAcquire() {\n \t\treturn nil\n-\tcase <-timer.C:\n-\t\ttimers.Put(timer)\n+\t}\n+\n+\t// Slow path: need to wait\n+\tstart := time.Now()\n+\terr := p.semaphore.Acquire(ctx, p.cfg.PoolTimeout, ErrPoolTimeout)\n+\n+\tswitch err {\n+\tcase nil:\n+\t\t// Successfully acquired after waiting\n+\t\tp.waitDurationNs.Add(time.Now().UnixNano() - start.UnixNano())\n+\t\tatomic.AddUint32(&p.stats.WaitCount, 1)\n+\tcase ErrPoolTimeout:\n \t\tatomic.AddUint32(&p.stats.Timeouts, 1)\n-\t\treturn ErrPoolTimeout\n \t}\n+\n+\treturn err\n }\n \n func (p *ConnPool) freeTurn() {\n-\t<-p.queue\n+\tp.semaphore.Release()\n }\n \n func (p *ConnPool) popIdle() (*Conn, error) {\n \tif p.closed() {\n \t\treturn nil, ErrClosed\n \t}\n+\tdefer p.checkMinIdleConns()\n+\n \tn := len(p.idleConns)\n \tif n == 0 {\n \t\treturn nil, nil\n \t}\n \n \tvar cn *Conn\n-\tif p.cfg.PoolFIFO {\n-\t\tcn = p.idleConns[0]\n-\t\tcopy(p.idleConns, p.idleConns[1:])\n-\t\tp.idleConns = p.idleConns[:n-1]\n-\t} else {\n-\t\tidx := n - 1\n-\t\tcn = p.idleConns[idx]\n-\t\tp.idleConns = p.idleConns[:idx]\n+\tattempts := 0\n+\n+\tmaxAttempts := util.Min(popAttempts, n)\n+\tfor attempts < maxAttempts {\n+\t\tif len(p.idleConns) == 0 {\n+\t\t\treturn nil, nil\n+\t\t}\n+\n+\t\tif p.cfg.PoolFIFO {\n+\t\t\tcn = p.idleConns[0]\n+\t\t\tcopy(p.idleConns, p.idleConns[1:])\n+\t\t\tp.idleConns = p.idleConns[:len(p.idleConns)-1]\n+\t\t} else {\n+\t\t\tidx := len(p.idleConns) - 1\n+\t\t\tcn = p.idleConns[idx]\n+\t\t\tp.idleConns = p.idleConns[:idx]\n+\t\t}\n+\t\tattempts++\n+\n+\t\t// Hot path optimization: try IDLE  IN_USE or CREATED  IN_USE transition\n+\t\t// Using inline TryAcquire() method for better performance (avoids pointer dereference)\n+\t\tif cn.TryAcquire() {\n+\t\t\t// Successfully acquired the connection\n+\t\t\tp.idleConnsLen.Add(-1)\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// Connection is in UNUSABLE, INITIALIZING, or other state - skip it\n+\n+\t\t// Connection is not in a valid state (might be UNUSABLE for handoff/re-auth, INITIALIZING, etc.)\n+\t\t// Put it back in the pool and try the next one\n+\t\tif p.cfg.PoolFIFO {\n+\t\t\t// FIFO: put at end (will be picked up last since we pop from front)\n+\t\t\tp.idleConns = append(p.idleConns, cn)\n+\t\t} else {\n+\t\t\t// LIFO: put at beginning (will be picked up last since we pop from end)\n+\t\t\tp.idleConns = append([]*Conn{cn}, p.idleConns...)\n+\t\t}\n+\t\tcn = nil\n \t}\n-\tp.idleConnsLen--\n-\tp.checkMinIdleConns()\n+\n+\t// If we exhausted all attempts without finding a usable connection, return nil\n+\tif attempts > 1 && attempts >= maxAttempts && int32(attempts) >= p.poolSize.Load() {\n+\t\tinternal.Logger.Printf(context.Background(), \"redis: connection pool: failed to get a usable connection after %d attempts\", attempts)\n+\t\treturn nil, nil\n+\t}\n+\n \treturn cn, nil\n }\n \n func (p *ConnPool) Put(ctx context.Context, cn *Conn) {\n-\tif cn.rd.Buffered() > 0 {\n-\t\tinternal.Logger.Printf(ctx, \"Conn has unread data\")\n-\t\tp.Remove(ctx, cn, BadConnError{})\n+\tp.putConn(ctx, cn, true)\n+}\n+\n+// putConnWithoutTurn is an internal method that puts a connection back to the pool\n+// without freeing a turn. This is used when returning a rejected connection from\n+// within Get(), where the turn is still held by the Get() call.\n+func (p *ConnPool) putConnWithoutTurn(ctx context.Context, cn *Conn) {\n+\tp.putConn(ctx, cn, false)\n+}\n+\n+// putConn is the internal implementation of Put that optionally frees a turn.\n+func (p *ConnPool) putConn(ctx context.Context, cn *Conn, freeTurn bool) {\n+\t// Process connection using the hooks system\n+\tshouldPool := true\n+\tshouldRemove := false\n+\tvar err error\n+\n+\tif cn.HasBufferedData() {\n+\t\t// Peek at the reply type to check if it's a push notification\n+\t\tif replyType, err := cn.PeekReplyTypeSafe(); err != nil || replyType != proto.RespPush {\n+\t\t\t// Not a push notification or error peeking, remove connection\n+\t\t\tinternal.Logger.Printf(ctx, \"Conn has unread data (not push notification), removing it\")\n+\t\t\tp.removeConnInternal(ctx, cn, err, freeTurn)\n+\t\t\treturn\n+\t\t}\n+\t\t// It's a push notification, allow pooling (client will handle it)\n+\t}\n+\n+\t// Lock-free atomic read - no mutex overhead!\n+\thookManager := p.hookManager.Load()\n+\n+\tif hookManager != nil {\n+\t\tshouldPool, shouldRemove, err = hookManager.ProcessOnPut(ctx, cn)\n+\t\tif err != nil {\n+\t\t\tinternal.Logger.Printf(ctx, \"Connection hook error: %v\", err)\n+\t\t\tp.removeConnInternal(ctx, cn, err, freeTurn)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// Combine all removal checks into one - reduces branches\n+\tif shouldRemove || !shouldPool {\n+\t\tp.removeConnInternal(ctx, cn, errHookRequestedRemoval, freeTurn)\n \t\treturn\n \t}\n \n \tif !cn.pooled {\n-\t\tp.Remove(ctx, cn, nil)\n+\t\tp.removeConnInternal(ctx, cn, errConnNotPooled, freeTurn)\n \t\treturn\n \t}\n \n \tvar shouldCloseConn bool\n \n-\tp.connsMu.Lock()\n+\tif p.cfg.MaxIdleConns == 0 || p.idleConnsLen.Load() < p.cfg.MaxIdleConns {\n+\t\t// Hot path optimization: try fast IN_USE  IDLE transition\n+\t\t// Using inline Release() method for better performance (avoids pointer dereference)\n+\t\ttransitionedToIdle := cn.Release()\n+\n+\t\t// Handle unexpected state changes\n+\t\tif !transitionedToIdle {\n+\t\t\t// Fast path failed - hook might have changed state (e.g., to UNUSABLE for handoff)\n+\t\t\t// Keep the state set by the hook and pool the connection anyway\n+\t\t\tcurrentState := cn.GetStateMachine().GetState()\n+\t\t\tswitch currentState {\n+\t\t\tcase StateUnusable:\n+\t\t\t\t// expected state, don't log it\n+\t\t\tcase StateClosed:\n+\t\t\t\tinternal.Logger.Printf(ctx, \"Unexpected conn[%d] state changed by hook to %v, closing it\", cn.GetID(), currentState)\n+\t\t\t\tshouldCloseConn = true\n+\t\t\t\tp.removeConnWithLock(cn)\n+\t\t\tdefault:\n+\t\t\t\t// Pool as-is\n+\t\t\t\tinternal.Logger.Printf(ctx, \"Unexpected conn[%d] state changed by hook to %v, pooling as-is\", cn.GetID(), currentState)\n+\t\t\t}\n+\t\t}\n \n-\tif p.cfg.MaxIdleConns == 0 || p.idleConnsLen < p.cfg.MaxIdleConns {\n-\t\tp.idleConns = append(p.idleConns, cn)\n-\t\tp.idleConnsLen++\n+\t\t// unusable conns are expected to become usable at some point (background process is reconnecting them)\n+\t\t// put them at the opposite end of the queue\n+\t\t// Optimization: if we just transitioned to IDLE, we know it's usable - skip the check\n+\t\tif !transitionedToIdle && !cn.IsUsable() {\n+\t\t\tif p.cfg.PoolFIFO {\n+\t\t\t\tp.connsMu.Lock()\n+\t\t\t\tp.idleConns = append(p.idleConns, cn)\n+\t\t\t\tp.connsMu.Unlock()\n+\t\t\t} else {\n+\t\t\t\tp.connsMu.Lock()\n+\t\t\t\tp.idleConns = append([]*Conn{cn}, p.idleConns...)\n+\t\t\t\tp.connsMu.Unlock()\n+\t\t\t}\n+\t\t\tp.idleConnsLen.Add(1)\n+\t\t} else if !shouldCloseConn {\n+\t\t\tp.connsMu.Lock()\n+\t\t\tp.idleConns = append(p.idleConns, cn)\n+\t\t\tp.connsMu.Unlock()\n+\t\t\tp.idleConnsLen.Add(1)\n+\t\t}\n \t} else {\n-\t\tp.removeConn(cn)\n \t\tshouldCloseConn = true\n+\t\tp.removeConnWithLock(cn)\n \t}\n \n-\tp.connsMu.Unlock()\n-\n-\tp.freeTurn()\n+\tif freeTurn {\n+\t\tp.freeTurn()\n+\t}\n \n \tif shouldCloseConn {\n \t\t_ = p.closeConn(cn)\n \t}\n+\n+\tcn.SetLastPutAtNs(getCachedTimeNs())\n }\n \n-func (p *ConnPool) Remove(_ context.Context, cn *Conn, reason error) {\n+func (p *ConnPool) Remove(ctx context.Context, cn *Conn, reason error) {\n+\tp.removeConnInternal(ctx, cn, reason, true)\n+}\n+\n+// RemoveWithoutTurn removes a connection from the pool without freeing a turn.\n+// This should be used when removing a connection from a context that didn't acquire\n+// a turn via Get() (e.g., background workers, cleanup tasks).\n+// For normal removal after Get(), use Remove() instead.\n+func (p *ConnPool) RemoveWithoutTurn(ctx context.Context, cn *Conn, reason error) {\n+\tp.removeConnInternal(ctx, cn, reason, false)\n+}\n+\n+// removeConnInternal is the internal implementation of Remove that optionally frees a turn.\n+func (p *ConnPool) removeConnInternal(ctx context.Context, cn *Conn, reason error, freeTurn bool) {\n+\t// Lock-free atomic read - no mutex overhead!\n+\thookManager := p.hookManager.Load()\n+\n+\tif hookManager != nil {\n+\t\thookManager.ProcessOnRemove(ctx, cn, reason)\n+\t}\n+\n \tp.removeConnWithLock(cn)\n-\tp.freeTurn()\n+\n+\tif freeTurn {\n+\t\tp.freeTurn()\n+\t}\n+\n \t_ = p.closeConn(cn)\n+\n+\t// Check if we need to create new idle connections to maintain MinIdleConns\n+\tp.checkMinIdleConns()\n }\n \n func (p *ConnPool) CloseConn(cn *Conn) error {\n@@ -414,17 +901,22 @@ func (p *ConnPool) removeConnWithLock(cn *Conn) {\n }\n \n func (p *ConnPool) removeConn(cn *Conn) {\n-\tfor i, c := range p.conns {\n-\t\tif c == cn {\n-\t\t\tp.conns = append(p.conns[:i], p.conns[i+1:]...)\n-\t\t\tif cn.pooled {\n-\t\t\t\tp.poolSize--\n-\t\t\t\tp.checkMinIdleConns()\n+\tcid := cn.GetID()\n+\tdelete(p.conns, cid)\n+\tatomic.AddUint32(&p.stats.StaleConns, 1)\n+\n+\t// Decrement pool size counter when removing a connection\n+\tif cn.pooled {\n+\t\tp.poolSize.Add(-1)\n+\t\t// this can be idle conn\n+\t\tfor idx, ic := range p.idleConns {\n+\t\t\tif ic == cn {\n+\t\t\t\tp.idleConns = append(p.idleConns[:idx], p.idleConns[idx+1:]...)\n+\t\t\t\tp.idleConnsLen.Add(-1)\n+\t\t\t\tbreak\n \t\t\t}\n-\t\t\tbreak\n \t\t}\n \t}\n-\tatomic.AddUint32(&p.stats.StaleConns, 1)\n }\n \n func (p *ConnPool) closeConn(cn *Conn) error {\n@@ -442,16 +934,27 @@ func (p *ConnPool) Len() int {\n // IdleLen returns number of idle connections.\n func (p *ConnPool) IdleLen() int {\n \tp.connsMu.Lock()\n-\tn := p.idleConnsLen\n+\tn := p.idleConnsLen.Load()\n \tp.connsMu.Unlock()\n-\treturn n\n+\treturn int(n)\n+}\n+\n+// Size returns the maximum pool size (capacity).\n+//\n+// This is used by the streaming credentials manager to size the re-auth worker pool,\n+// ensuring that re-auth operations don't exhaust the connection pool.\n+func (p *ConnPool) Size() int {\n+\treturn int(p.cfg.PoolSize)\n }\n \n func (p *ConnPool) Stats() *Stats {\n \treturn &Stats{\n-\t\tHits:     atomic.LoadUint32(&p.stats.Hits),\n-\t\tMisses:   atomic.LoadUint32(&p.stats.Misses),\n-\t\tTimeouts: atomic.LoadUint32(&p.stats.Timeouts),\n+\t\tHits:           atomic.LoadUint32(&p.stats.Hits),\n+\t\tMisses:         atomic.LoadUint32(&p.stats.Misses),\n+\t\tTimeouts:       atomic.LoadUint32(&p.stats.Timeouts),\n+\t\tWaitCount:      atomic.LoadUint32(&p.stats.WaitCount),\n+\t\tUnusable:       atomic.LoadUint32(&p.stats.Unusable),\n+\t\tWaitDurationNs: p.waitDurationNs.Load(),\n \n \t\tTotalConns: uint32(p.Len()),\n \t\tIdleConns:  uint32(p.IdleLen()),\n@@ -491,28 +994,62 @@ func (p *ConnPool) Close() error {\n \t\t}\n \t}\n \tp.conns = nil\n-\tp.poolSize = 0\n+\tp.poolSize.Store(0)\n \tp.idleConns = nil\n-\tp.idleConnsLen = 0\n+\tp.idleConnsLen.Store(0)\n \tp.connsMu.Unlock()\n \n \treturn firstErr\n }\n \n-func (p *ConnPool) isHealthyConn(cn *Conn) bool {\n-\tnow := time.Now()\n+func (p *ConnPool) isHealthyConn(cn *Conn, nowNs int64) bool {\n+\t// Performance optimization: check conditions from cheapest to most expensive,\n+\t// and from most likely to fail to least likely to fail.\n \n-\tif p.cfg.ConnMaxLifetime > 0 && now.Sub(cn.createdAt) >= p.cfg.ConnMaxLifetime {\n-\t\treturn false\n+\t// Only fails if ConnMaxLifetime is set AND connection is old.\n+\t// Most pools don't set ConnMaxLifetime, so this rarely fails.\n+\tif p.cfg.ConnMaxLifetime > 0 {\n+\t\tif cn.expiresAt.UnixNano() < nowNs {\n+\t\t\treturn false // Connection has exceeded max lifetime\n+\t\t}\n \t}\n-\tif p.cfg.ConnMaxIdleTime > 0 && now.Sub(cn.UsedAt()) >= p.cfg.ConnMaxIdleTime {\n-\t\treturn false\n+\n+\t// Most pools set ConnMaxIdleTime, and idle connections are common.\n+\t// Checking this first allows us to fail fast without expensive syscalls.\n+\tif p.cfg.ConnMaxIdleTime > 0 {\n+\t\tif nowNs-cn.UsedAtNs() >= int64(p.cfg.ConnMaxIdleTime) {\n+\t\t\treturn false // Connection has been idle too long\n+\t\t}\n \t}\n \n-\tif connCheck(cn.netConn) != nil {\n+\t// Only run this if the cheap checks passed.\n+\tif err := connCheck(cn.getNetConn()); err != nil {\n+\t\t// If there's unexpected data, it might be push notifications (RESP3)\n+\t\tif p.cfg.PushNotificationsEnabled && err == errUnexpectedRead {\n+\t\t\t// Peek at the reply type to check if it's a push notification\n+\t\t\tif replyType, err := cn.rd.PeekReplyType(); err == nil && replyType == proto.RespPush {\n+\t\t\t\t// For RESP3 connections with push notifications, we allow some buffered data\n+\t\t\t\t// The client will process these notifications before using the connection\n+\t\t\t\tinternal.Logger.Printf(\n+\t\t\t\t\tcontext.Background(),\n+\t\t\t\t\t\"push: conn[%d] has buffered data, likely push notifications - will be processed by client\",\n+\t\t\t\t\tcn.GetID(),\n+\t\t\t\t)\n+\n+\t\t\t\t// Update timestamp for healthy connection\n+\t\t\t\tcn.SetUsedAtNs(nowNs)\n+\n+\t\t\t\t// Connection is healthy, client will handle notifications\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t\t// Not a push notification - treat as unhealthy\n+\t\t\treturn false\n+\t\t}\n+\t\t// Connection failed health check\n \t\treturn false\n \t}\n \n-\tcn.SetUsedAt(now)\n+\t// Only update UsedAt if connection is healthy (avoids unnecessary atomic store)\n+\tcn.SetUsedAtNs(nowNs)\n \treturn true\n }"
    },
    {
      "sha": "365219a578b4f7f35f9a32f66f5ef014ee9e1011",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/pool/pool_single.go",
      "status": "modified",
      "additions": 50,
      "deletions": 4,
      "changes": 54,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpool_single.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpool_single.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpool_single.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -1,7 +1,13 @@\n package pool\n \n-import \"context\"\n+import (\n+\t\"context\"\n+\t\"time\"\n+)\n \n+// SingleConnPool is a pool that always returns the same connection.\n+// Note: This pool is not thread-safe.\n+// It is intended to be used by clients that need a single connection.\n type SingleConnPool struct {\n \tpool      Pooler\n \tcn        *Conn\n@@ -10,6 +16,12 @@ type SingleConnPool struct {\n \n var _ Pooler = (*SingleConnPool)(nil)\n \n+// NewSingleConnPool creates a new single connection pool.\n+// The pool will always return the same connection.\n+// The pool will not:\n+// - Close the connection\n+// - Reconnect the connection\n+// - Track the connection in any way\n func NewSingleConnPool(pool Pooler, cn *Conn) *SingleConnPool {\n \treturn &SingleConnPool{\n \t\tpool: pool,\n@@ -25,20 +37,47 @@ func (p *SingleConnPool) CloseConn(cn *Conn) error {\n \treturn p.pool.CloseConn(cn)\n }\n \n-func (p *SingleConnPool) Get(ctx context.Context) (*Conn, error) {\n+func (p *SingleConnPool) Get(_ context.Context) (*Conn, error) {\n \tif p.stickyErr != nil {\n \t\treturn nil, p.stickyErr\n \t}\n+\tif p.cn == nil {\n+\t\treturn nil, ErrClosed\n+\t}\n+\n+\t// NOTE: SingleConnPool is NOT thread-safe by design and is used in special scenarios:\n+\t// - During initialization (connection is in INITIALIZING state)\n+\t// - During re-authentication (connection is in UNUSABLE state)\n+\t// - For transactions (connection might be in various states)\n+\t// We use SetUsed() which forces the transition, rather than TryTransition() which\n+\t// would fail if the connection is not in IDLE/CREATED state.\n+\tp.cn.SetUsed(true)\n+\tp.cn.SetUsedAt(time.Now())\n \treturn p.cn, nil\n }\n \n-func (p *SingleConnPool) Put(ctx context.Context, cn *Conn) {}\n+func (p *SingleConnPool) Put(_ context.Context, cn *Conn) {\n+\tif p.cn == nil {\n+\t\treturn\n+\t}\n+\tif p.cn != cn {\n+\t\treturn\n+\t}\n+\tp.cn.SetUsed(false)\n+}\n \n-func (p *SingleConnPool) Remove(ctx context.Context, cn *Conn, reason error) {\n+func (p *SingleConnPool) Remove(_ context.Context, cn *Conn, reason error) {\n+\tcn.SetUsed(false)\n \tp.cn = nil\n \tp.stickyErr = reason\n }\n \n+// RemoveWithoutTurn has the same behavior as Remove for SingleConnPool\n+// since SingleConnPool doesn't use a turn-based queue system.\n+func (p *SingleConnPool) RemoveWithoutTurn(ctx context.Context, cn *Conn, reason error) {\n+\tp.Remove(ctx, cn, reason)\n+}\n+\n func (p *SingleConnPool) Close() error {\n \tp.cn = nil\n \tp.stickyErr = ErrClosed\n@@ -53,6 +92,13 @@ func (p *SingleConnPool) IdleLen() int {\n \treturn 0\n }\n \n+// Size returns the maximum pool size, which is always 1 for SingleConnPool.\n+func (p *SingleConnPool) Size() int { return 1 }\n+\n func (p *SingleConnPool) Stats() *Stats {\n \treturn &Stats{}\n }\n+\n+func (p *SingleConnPool) AddPoolHook(_ PoolHook) {}\n+\n+func (p *SingleConnPool) RemovePoolHook(_ PoolHook) {}"
    },
    {
      "sha": "be869b5693907b3b621cdc9f40a260b89b78277d",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/pool/pool_sticky.go",
      "status": "modified",
      "additions": 13,
      "deletions": 0,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpool_sticky.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpool_sticky.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpool_sticky.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -123,6 +123,12 @@ func (p *StickyConnPool) Remove(ctx context.Context, cn *Conn, reason error) {\n \tp.ch <- cn\n }\n \n+// RemoveWithoutTurn has the same behavior as Remove for StickyConnPool\n+// since StickyConnPool doesn't use a turn-based queue system.\n+func (p *StickyConnPool) RemoveWithoutTurn(ctx context.Context, cn *Conn, reason error) {\n+\tp.Remove(ctx, cn, reason)\n+}\n+\n func (p *StickyConnPool) Close() error {\n \tif shared := atomic.AddInt32(&p.shared, -1); shared > 0 {\n \t\treturn nil\n@@ -196,6 +202,13 @@ func (p *StickyConnPool) IdleLen() int {\n \treturn len(p.ch)\n }\n \n+// Size returns the maximum pool size, which is always 1 for StickyConnPool.\n+func (p *StickyConnPool) Size() int { return 1 }\n+\n func (p *StickyConnPool) Stats() *Stats {\n \treturn &Stats{}\n }\n+\n+func (p *StickyConnPool) AddPoolHook(hook PoolHook) {}\n+\n+func (p *StickyConnPool) RemovePoolHook(hook PoolHook) {}"
    },
    {
      "sha": "5b29659eac1837b4258bb576de2400b86c0afd87",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/pool/pubsub.go",
      "status": "added",
      "additions": 80,
      "deletions": 0,
      "changes": 80,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpubsub.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpubsub.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fpubsub.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,80 @@\n+package pool\n+\n+import (\n+\t\"context\"\n+\t\"net\"\n+\t\"sync\"\n+\t\"sync/atomic\"\n+)\n+\n+type PubSubStats struct {\n+\tCreated   uint32\n+\tUntracked uint32\n+\tActive    uint32\n+}\n+\n+// PubSubPool manages a pool of PubSub connections.\n+type PubSubPool struct {\n+\topt       *Options\n+\tnetDialer func(ctx context.Context, network, addr string) (net.Conn, error)\n+\n+\t// Map to track active PubSub connections\n+\tactiveConns sync.Map // map[uint64]*Conn (connID -> conn)\n+\tclosed      atomic.Bool\n+\tstats       PubSubStats\n+}\n+\n+// NewPubSubPool implements a pool for PubSub connections.\n+// It intentionally does not implement the Pooler interface\n+func NewPubSubPool(opt *Options, netDialer func(ctx context.Context, network, addr string) (net.Conn, error)) *PubSubPool {\n+\treturn &PubSubPool{\n+\t\topt:       opt,\n+\t\tnetDialer: netDialer,\n+\t}\n+}\n+\n+func (p *PubSubPool) NewConn(ctx context.Context, network string, addr string, channels []string) (*Conn, error) {\n+\tif p.closed.Load() {\n+\t\treturn nil, ErrClosed\n+\t}\n+\n+\tnetConn, err := p.netDialer(ctx, network, addr)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tcn := NewConnWithBufferSize(netConn, p.opt.ReadBufferSize, p.opt.WriteBufferSize)\n+\tcn.pubsub = true\n+\tatomic.AddUint32(&p.stats.Created, 1)\n+\treturn cn, nil\n+\n+}\n+\n+func (p *PubSubPool) TrackConn(cn *Conn) {\n+\tatomic.AddUint32(&p.stats.Active, 1)\n+\tp.activeConns.Store(cn.GetID(), cn)\n+}\n+\n+func (p *PubSubPool) UntrackConn(cn *Conn) {\n+\tatomic.AddUint32(&p.stats.Active, ^uint32(0))\n+\tatomic.AddUint32(&p.stats.Untracked, 1)\n+\tp.activeConns.Delete(cn.GetID())\n+}\n+\n+func (p *PubSubPool) Close() error {\n+\tp.closed.Store(true)\n+\tp.activeConns.Range(func(key, value interface{}) bool {\n+\t\tcn := value.(*Conn)\n+\t\t_ = cn.Close()\n+\t\treturn true\n+\t})\n+\treturn nil\n+}\n+\n+func (p *PubSubPool) Stats() *PubSubStats {\n+\t// load stats atomically\n+\treturn &PubSubStats{\n+\t\tCreated:   atomic.LoadUint32(&p.stats.Created),\n+\t\tUntracked: atomic.LoadUint32(&p.stats.Untracked),\n+\t\tActive:    atomic.LoadUint32(&p.stats.Active),\n+\t}\n+}"
    },
    {
      "sha": "6f9e4bfa9558f4fb89361e23698cf08c5fd3367b",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/pool/want_conn.go",
      "status": "added",
      "additions": 93,
      "deletions": 0,
      "changes": 93,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fwant_conn.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fwant_conn.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fpool%2Fwant_conn.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,93 @@\n+package pool\n+\n+import (\n+\t\"context\"\n+\t\"sync\"\n+)\n+\n+type wantConn struct {\n+\tmu        sync.Mutex      // protects ctx, done and sending of the result\n+\tctx       context.Context // context for dial, cleared after delivered or canceled\n+\tcancelCtx context.CancelFunc\n+\tdone      bool                // true after delivered or canceled\n+\tresult    chan wantConnResult // channel to deliver connection or error\n+}\n+\n+// getCtxForDial returns context for dial or nil if connection was delivered or canceled.\n+func (w *wantConn) getCtxForDial() context.Context {\n+\tw.mu.Lock()\n+\tdefer w.mu.Unlock()\n+\n+\treturn w.ctx\n+}\n+\n+func (w *wantConn) tryDeliver(cn *Conn, err error) bool {\n+\tw.mu.Lock()\n+\tdefer w.mu.Unlock()\n+\tif w.done {\n+\t\treturn false\n+\t}\n+\n+\tw.done = true\n+\tw.ctx = nil\n+\n+\tw.result <- wantConnResult{cn: cn, err: err}\n+\tclose(w.result)\n+\n+\treturn true\n+}\n+\n+func (w *wantConn) cancel() *Conn {\n+\tw.mu.Lock()\n+\tvar cn *Conn\n+\tif w.done {\n+\t\tselect {\n+\t\tcase result := <-w.result:\n+\t\t\tcn = result.cn\n+\t\tdefault:\n+\t\t}\n+\t} else {\n+\t\tclose(w.result)\n+\t}\n+\n+\tw.done = true\n+\tw.ctx = nil\n+\tw.mu.Unlock()\n+\n+\treturn cn\n+}\n+\n+type wantConnResult struct {\n+\tcn  *Conn\n+\terr error\n+}\n+\n+type wantConnQueue struct {\n+\tmu    sync.RWMutex\n+\titems []*wantConn\n+}\n+\n+func newWantConnQueue() *wantConnQueue {\n+\treturn &wantConnQueue{\n+\t\titems: make([]*wantConn, 0),\n+\t}\n+}\n+\n+func (q *wantConnQueue) enqueue(w *wantConn) {\n+\tq.mu.Lock()\n+\tdefer q.mu.Unlock()\n+\tq.items = append(q.items, w)\n+}\n+\n+func (q *wantConnQueue) dequeue() (*wantConn, bool) {\n+\tq.mu.Lock()\n+\tdefer q.mu.Unlock()\n+\n+\tif len(q.items) == 0 {\n+\t\treturn nil, false\n+\t}\n+\n+\titem := q.items[0]\n+\tq.items = q.items[1:]\n+\treturn item, true\n+}"
    },
    {
      "sha": "bac68f796528fec8a64d7859f41e745bf0a7ba19",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/proto/reader.go",
      "status": "modified",
      "additions": 99,
      "deletions": 3,
      "changes": 102,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fproto%2Freader.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fproto%2Freader.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fproto%2Freader.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -12,6 +12,9 @@ import (\n \t\"github.com/redis/go-redis/v9/internal/util\"\n )\n \n+// DefaultBufferSize is the default size for read/write buffers (32 KiB).\n+const DefaultBufferSize = 32 * 1024\n+\n // redis resp protocol data type.\n const (\n \tRespStatus    = '+' // +<string>\\r\\n\n@@ -47,7 +50,8 @@ func (e RedisError) Error() string { return string(e) }\n func (RedisError) RedisError() {}\n \n func ParseErrorReply(line []byte) error {\n-\treturn RedisError(line[1:])\n+\tmsg := string(line[1:])\n+\treturn parseTypedRedisError(msg)\n }\n \n //------------------------------------------------------------------------------\n@@ -58,7 +62,13 @@ type Reader struct {\n \n func NewReader(rd io.Reader) *Reader {\n \treturn &Reader{\n-\t\trd: bufio.NewReader(rd),\n+\t\trd: bufio.NewReaderSize(rd, DefaultBufferSize),\n+\t}\n+}\n+\n+func NewReaderSize(rd io.Reader, size int) *Reader {\n+\treturn &Reader{\n+\t\trd: bufio.NewReaderSize(rd, size),\n \t}\n }\n \n@@ -90,6 +100,92 @@ func (r *Reader) PeekReplyType() (byte, error) {\n \treturn b[0], nil\n }\n \n+func (r *Reader) PeekPushNotificationName() (string, error) {\n+\t// \"prime\" the buffer by peeking at the next byte\n+\tc, err := r.Peek(1)\n+\tif err != nil {\n+\t\treturn \"\", err\n+\t}\n+\tif c[0] != RespPush {\n+\t\treturn \"\", fmt.Errorf(\"redis: can't peek push notification name, next reply is not a push notification\")\n+\t}\n+\n+\t// peek 36 bytes at most, should be enough to read the push notification name\n+\ttoPeek := 36\n+\tbuffered := r.Buffered()\n+\tif buffered == 0 {\n+\t\treturn \"\", fmt.Errorf(\"redis: can't peek push notification name, no data available\")\n+\t}\n+\tif buffered < toPeek {\n+\t\ttoPeek = buffered\n+\t}\n+\tbuf, err := r.rd.Peek(toPeek)\n+\tif err != nil {\n+\t\treturn \"\", err\n+\t}\n+\tif buf[0] != RespPush {\n+\t\treturn \"\", fmt.Errorf(\"redis: can't parse push notification: %q\", buf)\n+\t}\n+\n+\tif len(buf) < 3 {\n+\t\treturn \"\", fmt.Errorf(\"redis: can't parse push notification: %q\", buf)\n+\t}\n+\n+\t// remove push notification type\n+\tbuf = buf[1:]\n+\t// remove first line - e.g. >2\\r\\n\n+\tfor i := 0; i < len(buf)-1; i++ {\n+\t\tif buf[i] == '\\r' && buf[i+1] == '\\n' {\n+\t\t\tbuf = buf[i+2:]\n+\t\t\tbreak\n+\t\t} else {\n+\t\t\tif buf[i] < '0' || buf[i] > '9' {\n+\t\t\t\treturn \"\", fmt.Errorf(\"redis: can't parse push notification: %q\", buf)\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif len(buf) < 2 {\n+\t\treturn \"\", fmt.Errorf(\"redis: can't parse push notification: %q\", buf)\n+\t}\n+\t// next line should be $<length><string>\\r\\n or +<length><string>\\r\\n\n+\t// should have the type of the push notification name and it's length\n+\tif buf[0] != RespString && buf[0] != RespStatus {\n+\t\treturn \"\", fmt.Errorf(\"redis: can't parse push notification name: %q\", buf)\n+\t}\n+\ttypeOfName := buf[0]\n+\t// remove the type of the push notification name\n+\tbuf = buf[1:]\n+\tif typeOfName == RespString {\n+\t\t// remove the length of the string\n+\t\tif len(buf) < 2 {\n+\t\t\treturn \"\", fmt.Errorf(\"redis: can't parse push notification name: %q\", buf)\n+\t\t}\n+\t\tfor i := 0; i < len(buf)-1; i++ {\n+\t\t\tif buf[i] == '\\r' && buf[i+1] == '\\n' {\n+\t\t\t\tbuf = buf[i+2:]\n+\t\t\t\tbreak\n+\t\t\t} else {\n+\t\t\t\tif buf[i] < '0' || buf[i] > '9' {\n+\t\t\t\t\treturn \"\", fmt.Errorf(\"redis: can't parse push notification name: %q\", buf)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif len(buf) < 2 {\n+\t\treturn \"\", fmt.Errorf(\"redis: can't parse push notification name: %q\", buf)\n+\t}\n+\t// keep only the notification name\n+\tfor i := 0; i < len(buf)-1; i++ {\n+\t\tif buf[i] == '\\r' && buf[i+1] == '\\n' {\n+\t\t\tbuf = buf[:i]\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\n+\treturn util.BytesToString(buf), nil\n+}\n+\n // ReadLine Return a valid reply, it will check the protocol or redis error,\n // and discard the attribute type.\n func (r *Reader) ReadLine() ([]byte, error) {\n@@ -106,7 +202,7 @@ func (r *Reader) ReadLine() ([]byte, error) {\n \t\tvar blobErr string\n \t\tblobErr, err = r.readStringReply(line)\n \t\tif err == nil {\n-\t\t\terr = RedisError(blobErr)\n+\t\t\terr = parseTypedRedisError(blobErr)\n \t\t}\n \t\treturn nil, err\n \tcase RespAttr:"
    },
    {
      "sha": "f553e2f962074cf2db5d4cb8a99fee149daf3173",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/proto/redis_errors.go",
      "status": "added",
      "additions": 488,
      "deletions": 0,
      "changes": 488,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fproto%2Fredis_errors.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fproto%2Fredis_errors.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fproto%2Fredis_errors.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,488 @@\n+package proto\n+\n+import (\n+\t\"errors\"\n+\t\"strings\"\n+)\n+\n+// Typed Redis errors for better error handling with wrapping support.\n+// These errors maintain backward compatibility by keeping the same error messages.\n+\n+// LoadingError is returned when Redis is loading the dataset in memory.\n+type LoadingError struct {\n+\tmsg string\n+}\n+\n+func (e *LoadingError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *LoadingError) RedisError() {}\n+\n+// NewLoadingError creates a new LoadingError with the given message.\n+func NewLoadingError(msg string) *LoadingError {\n+\treturn &LoadingError{msg: msg}\n+}\n+\n+// ReadOnlyError is returned when trying to write to a read-only replica.\n+type ReadOnlyError struct {\n+\tmsg string\n+}\n+\n+func (e *ReadOnlyError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *ReadOnlyError) RedisError() {}\n+\n+// NewReadOnlyError creates a new ReadOnlyError with the given message.\n+func NewReadOnlyError(msg string) *ReadOnlyError {\n+\treturn &ReadOnlyError{msg: msg}\n+}\n+\n+// MovedError is returned when a key has been moved to a different node in a cluster.\n+type MovedError struct {\n+\tmsg  string\n+\taddr string\n+}\n+\n+func (e *MovedError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *MovedError) RedisError() {}\n+\n+// Addr returns the address of the node where the key has been moved.\n+func (e *MovedError) Addr() string {\n+\treturn e.addr\n+}\n+\n+// NewMovedError creates a new MovedError with the given message and address.\n+func NewMovedError(msg string, addr string) *MovedError {\n+\treturn &MovedError{msg: msg, addr: addr}\n+}\n+\n+// AskError is returned when a key is being migrated and the client should ask another node.\n+type AskError struct {\n+\tmsg  string\n+\taddr string\n+}\n+\n+func (e *AskError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *AskError) RedisError() {}\n+\n+// Addr returns the address of the node to ask.\n+func (e *AskError) Addr() string {\n+\treturn e.addr\n+}\n+\n+// NewAskError creates a new AskError with the given message and address.\n+func NewAskError(msg string, addr string) *AskError {\n+\treturn &AskError{msg: msg, addr: addr}\n+}\n+\n+// ClusterDownError is returned when the cluster is down.\n+type ClusterDownError struct {\n+\tmsg string\n+}\n+\n+func (e *ClusterDownError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *ClusterDownError) RedisError() {}\n+\n+// NewClusterDownError creates a new ClusterDownError with the given message.\n+func NewClusterDownError(msg string) *ClusterDownError {\n+\treturn &ClusterDownError{msg: msg}\n+}\n+\n+// TryAgainError is returned when a command cannot be processed and should be retried.\n+type TryAgainError struct {\n+\tmsg string\n+}\n+\n+func (e *TryAgainError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *TryAgainError) RedisError() {}\n+\n+// NewTryAgainError creates a new TryAgainError with the given message.\n+func NewTryAgainError(msg string) *TryAgainError {\n+\treturn &TryAgainError{msg: msg}\n+}\n+\n+// MasterDownError is returned when the master is down.\n+type MasterDownError struct {\n+\tmsg string\n+}\n+\n+func (e *MasterDownError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *MasterDownError) RedisError() {}\n+\n+// NewMasterDownError creates a new MasterDownError with the given message.\n+func NewMasterDownError(msg string) *MasterDownError {\n+\treturn &MasterDownError{msg: msg}\n+}\n+\n+// MaxClientsError is returned when the maximum number of clients has been reached.\n+type MaxClientsError struct {\n+\tmsg string\n+}\n+\n+func (e *MaxClientsError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *MaxClientsError) RedisError() {}\n+\n+// NewMaxClientsError creates a new MaxClientsError with the given message.\n+func NewMaxClientsError(msg string) *MaxClientsError {\n+\treturn &MaxClientsError{msg: msg}\n+}\n+\n+// AuthError is returned when authentication fails.\n+type AuthError struct {\n+\tmsg string\n+}\n+\n+func (e *AuthError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *AuthError) RedisError() {}\n+\n+// NewAuthError creates a new AuthError with the given message.\n+func NewAuthError(msg string) *AuthError {\n+\treturn &AuthError{msg: msg}\n+}\n+\n+// PermissionError is returned when a user lacks required permissions.\n+type PermissionError struct {\n+\tmsg string\n+}\n+\n+func (e *PermissionError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *PermissionError) RedisError() {}\n+\n+// NewPermissionError creates a new PermissionError with the given message.\n+func NewPermissionError(msg string) *PermissionError {\n+\treturn &PermissionError{msg: msg}\n+}\n+\n+// ExecAbortError is returned when a transaction is aborted.\n+type ExecAbortError struct {\n+\tmsg string\n+}\n+\n+func (e *ExecAbortError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *ExecAbortError) RedisError() {}\n+\n+// NewExecAbortError creates a new ExecAbortError with the given message.\n+func NewExecAbortError(msg string) *ExecAbortError {\n+\treturn &ExecAbortError{msg: msg}\n+}\n+\n+// OOMError is returned when Redis is out of memory.\n+type OOMError struct {\n+\tmsg string\n+}\n+\n+func (e *OOMError) Error() string {\n+\treturn e.msg\n+}\n+\n+func (e *OOMError) RedisError() {}\n+\n+// NewOOMError creates a new OOMError with the given message.\n+func NewOOMError(msg string) *OOMError {\n+\treturn &OOMError{msg: msg}\n+}\n+\n+// parseTypedRedisError parses a Redis error message and returns a typed error if applicable.\n+// This function maintains backward compatibility by keeping the same error messages.\n+func parseTypedRedisError(msg string) error {\n+\t// Check for specific error patterns and return typed errors\n+\tswitch {\n+\tcase strings.HasPrefix(msg, \"LOADING \"):\n+\t\treturn NewLoadingError(msg)\n+\tcase strings.HasPrefix(msg, \"READONLY \"):\n+\t\treturn NewReadOnlyError(msg)\n+\tcase strings.HasPrefix(msg, \"MOVED \"):\n+\t\t// Extract address from \"MOVED <slot> <addr>\"\n+\t\taddr := extractAddr(msg)\n+\t\treturn NewMovedError(msg, addr)\n+\tcase strings.HasPrefix(msg, \"ASK \"):\n+\t\t// Extract address from \"ASK <slot> <addr>\"\n+\t\taddr := extractAddr(msg)\n+\t\treturn NewAskError(msg, addr)\n+\tcase strings.HasPrefix(msg, \"CLUSTERDOWN \"):\n+\t\treturn NewClusterDownError(msg)\n+\tcase strings.HasPrefix(msg, \"TRYAGAIN \"):\n+\t\treturn NewTryAgainError(msg)\n+\tcase strings.HasPrefix(msg, \"MASTERDOWN \"):\n+\t\treturn NewMasterDownError(msg)\n+\tcase msg == \"ERR max number of clients reached\":\n+\t\treturn NewMaxClientsError(msg)\n+\tcase strings.HasPrefix(msg, \"NOAUTH \"), strings.HasPrefix(msg, \"WRONGPASS \"), strings.Contains(msg, \"unauthenticated\"):\n+\t\treturn NewAuthError(msg)\n+\tcase strings.HasPrefix(msg, \"NOPERM \"):\n+\t\treturn NewPermissionError(msg)\n+\tcase strings.HasPrefix(msg, \"EXECABORT \"):\n+\t\treturn NewExecAbortError(msg)\n+\tcase strings.HasPrefix(msg, \"OOM \"):\n+\t\treturn NewOOMError(msg)\n+\tdefault:\n+\t\t// Return generic RedisError for unknown error types\n+\t\treturn RedisError(msg)\n+\t}\n+}\n+\n+// extractAddr extracts the address from MOVED/ASK error messages.\n+// Format: \"MOVED <slot> <addr>\" or \"ASK <slot> <addr>\"\n+func extractAddr(msg string) string {\n+\tind := strings.LastIndex(msg, \" \")\n+\tif ind == -1 {\n+\t\treturn \"\"\n+\t}\n+\treturn msg[ind+1:]\n+}\n+\n+// IsLoadingError checks if an error is a LoadingError, even if wrapped.\n+func IsLoadingError(err error) bool {\n+\tif err == nil {\n+\t\treturn false\n+\t}\n+\tvar loadingErr *LoadingError\n+\tif errors.As(err, &loadingErr) {\n+\t\treturn true\n+\t}\n+\t// Check if wrapped error is a RedisError with LOADING prefix\n+\tvar redisErr RedisError\n+\tif errors.As(err, &redisErr) && strings.HasPrefix(redisErr.Error(), \"LOADING \") {\n+\t\treturn true\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\treturn strings.HasPrefix(err.Error(), \"LOADING \")\n+}\n+\n+// IsReadOnlyError checks if an error is a ReadOnlyError, even if wrapped.\n+func IsReadOnlyError(err error) bool {\n+\tif err == nil {\n+\t\treturn false\n+\t}\n+\tvar readOnlyErr *ReadOnlyError\n+\tif errors.As(err, &readOnlyErr) {\n+\t\treturn true\n+\t}\n+\t// Check if wrapped error is a RedisError with READONLY prefix\n+\tvar redisErr RedisError\n+\tif errors.As(err, &redisErr) && strings.HasPrefix(redisErr.Error(), \"READONLY \") {\n+\t\treturn true\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\treturn strings.HasPrefix(err.Error(), \"READONLY \")\n+}\n+\n+// IsMovedError checks if an error is a MovedError, even if wrapped.\n+// Returns the error and a boolean indicating if it's a MovedError.\n+func IsMovedError(err error) (*MovedError, bool) {\n+\tif err == nil {\n+\t\treturn nil, false\n+\t}\n+\tvar movedErr *MovedError\n+\tif errors.As(err, &movedErr) {\n+\t\treturn movedErr, true\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\ts := err.Error()\n+\tif strings.HasPrefix(s, \"MOVED \") {\n+\t\t// Parse: MOVED 3999 127.0.0.1:6381\n+\t\tparts := strings.Split(s, \" \")\n+\t\tif len(parts) == 3 {\n+\t\t\treturn &MovedError{msg: s, addr: parts[2]}, true\n+\t\t}\n+\t}\n+\treturn nil, false\n+}\n+\n+// IsAskError checks if an error is an AskError, even if wrapped.\n+// Returns the error and a boolean indicating if it's an AskError.\n+func IsAskError(err error) (*AskError, bool) {\n+\tif err == nil {\n+\t\treturn nil, false\n+\t}\n+\tvar askErr *AskError\n+\tif errors.As(err, &askErr) {\n+\t\treturn askErr, true\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\ts := err.Error()\n+\tif strings.HasPrefix(s, \"ASK \") {\n+\t\t// Parse: ASK 3999 127.0.0.1:6381\n+\t\tparts := strings.Split(s, \" \")\n+\t\tif len(parts) == 3 {\n+\t\t\treturn &AskError{msg: s, addr: parts[2]}, true\n+\t\t}\n+\t}\n+\treturn nil, false\n+}\n+\n+// IsClusterDownError checks if an error is a ClusterDownError, even if wrapped.\n+func IsClusterDownError(err error) bool {\n+\tif err == nil {\n+\t\treturn false\n+\t}\n+\tvar clusterDownErr *ClusterDownError\n+\tif errors.As(err, &clusterDownErr) {\n+\t\treturn true\n+\t}\n+\t// Check if wrapped error is a RedisError with CLUSTERDOWN prefix\n+\tvar redisErr RedisError\n+\tif errors.As(err, &redisErr) && strings.HasPrefix(redisErr.Error(), \"CLUSTERDOWN \") {\n+\t\treturn true\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\treturn strings.HasPrefix(err.Error(), \"CLUSTERDOWN \")\n+}\n+\n+// IsTryAgainError checks if an error is a TryAgainError, even if wrapped.\n+func IsTryAgainError(err error) bool {\n+\tif err == nil {\n+\t\treturn false\n+\t}\n+\tvar tryAgainErr *TryAgainError\n+\tif errors.As(err, &tryAgainErr) {\n+\t\treturn true\n+\t}\n+\t// Check if wrapped error is a RedisError with TRYAGAIN prefix\n+\tvar redisErr RedisError\n+\tif errors.As(err, &redisErr) && strings.HasPrefix(redisErr.Error(), \"TRYAGAIN \") {\n+\t\treturn true\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\treturn strings.HasPrefix(err.Error(), \"TRYAGAIN \")\n+}\n+\n+// IsMasterDownError checks if an error is a MasterDownError, even if wrapped.\n+func IsMasterDownError(err error) bool {\n+\tif err == nil {\n+\t\treturn false\n+\t}\n+\tvar masterDownErr *MasterDownError\n+\tif errors.As(err, &masterDownErr) {\n+\t\treturn true\n+\t}\n+\t// Check if wrapped error is a RedisError with MASTERDOWN prefix\n+\tvar redisErr RedisError\n+\tif errors.As(err, &redisErr) && strings.HasPrefix(redisErr.Error(), \"MASTERDOWN \") {\n+\t\treturn true\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\treturn strings.HasPrefix(err.Error(), \"MASTERDOWN \")\n+}\n+\n+// IsMaxClientsError checks if an error is a MaxClientsError, even if wrapped.\n+func IsMaxClientsError(err error) bool {\n+\tif err == nil {\n+\t\treturn false\n+\t}\n+\tvar maxClientsErr *MaxClientsError\n+\tif errors.As(err, &maxClientsErr) {\n+\t\treturn true\n+\t}\n+\t// Check if wrapped error is a RedisError with max clients prefix\n+\tvar redisErr RedisError\n+\tif errors.As(err, &redisErr) && strings.HasPrefix(redisErr.Error(), \"ERR max number of clients reached\") {\n+\t\treturn true\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\treturn strings.HasPrefix(err.Error(), \"ERR max number of clients reached\")\n+}\n+\n+// IsAuthError checks if an error is an AuthError, even if wrapped.\n+func IsAuthError(err error) bool {\n+\tif err == nil {\n+\t\treturn false\n+\t}\n+\tvar authErr *AuthError\n+\tif errors.As(err, &authErr) {\n+\t\treturn true\n+\t}\n+\t// Check if wrapped error is a RedisError with auth error prefix\n+\tvar redisErr RedisError\n+\tif errors.As(err, &redisErr) {\n+\t\ts := redisErr.Error()\n+\t\treturn strings.HasPrefix(s, \"NOAUTH \") || strings.HasPrefix(s, \"WRONGPASS \") || strings.Contains(s, \"unauthenticated\")\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\ts := err.Error()\n+\treturn strings.HasPrefix(s, \"NOAUTH \") || strings.HasPrefix(s, \"WRONGPASS \") || strings.Contains(s, \"unauthenticated\")\n+}\n+\n+// IsPermissionError checks if an error is a PermissionError, even if wrapped.\n+func IsPermissionError(err error) bool {\n+\tif err == nil {\n+\t\treturn false\n+\t}\n+\tvar permErr *PermissionError\n+\tif errors.As(err, &permErr) {\n+\t\treturn true\n+\t}\n+\t// Check if wrapped error is a RedisError with NOPERM prefix\n+\tvar redisErr RedisError\n+\tif errors.As(err, &redisErr) && strings.HasPrefix(redisErr.Error(), \"NOPERM \") {\n+\t\treturn true\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\treturn strings.HasPrefix(err.Error(), \"NOPERM \")\n+}\n+\n+// IsExecAbortError checks if an error is an ExecAbortError, even if wrapped.\n+func IsExecAbortError(err error) bool {\n+\tif err == nil {\n+\t\treturn false\n+\t}\n+\tvar execAbortErr *ExecAbortError\n+\tif errors.As(err, &execAbortErr) {\n+\t\treturn true\n+\t}\n+\t// Check if wrapped error is a RedisError with EXECABORT prefix\n+\tvar redisErr RedisError\n+\tif errors.As(err, &redisErr) && strings.HasPrefix(redisErr.Error(), \"EXECABORT \") {\n+\t\treturn true\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\treturn strings.HasPrefix(err.Error(), \"EXECABORT \")\n+}\n+\n+// IsOOMError checks if an error is an OOMError, even if wrapped.\n+func IsOOMError(err error) bool {\n+\tif err == nil {\n+\t\treturn false\n+\t}\n+\tvar oomErr *OOMError\n+\tif errors.As(err, &oomErr) {\n+\t\treturn true\n+\t}\n+\t// Check if wrapped error is a RedisError with OOM prefix\n+\tvar redisErr RedisError\n+\tif errors.As(err, &redisErr) && strings.HasPrefix(redisErr.Error(), \"OOM \") {\n+\t\treturn true\n+\t}\n+\t// Fallback to string checking for backward compatibility\n+\treturn strings.HasPrefix(err.Error(), \"OOM \")\n+}"
    },
    {
      "sha": "38e66c68872d1249381aa0f56722ad2251069b4f",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/proto/writer.go",
      "status": "modified",
      "additions": 53,
      "deletions": 0,
      "changes": 53,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fproto%2Fwriter.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fproto%2Fwriter.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fproto%2Fwriter.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -66,72 +66,125 @@ func (w *Writer) WriteArg(v interface{}) error {\n \tcase string:\n \t\treturn w.string(v)\n \tcase *string:\n+\t\tif v == nil {\n+\t\t\treturn w.string(\"\")\n+\t\t}\n \t\treturn w.string(*v)\n \tcase []byte:\n \t\treturn w.bytes(v)\n \tcase int:\n \t\treturn w.int(int64(v))\n \tcase *int:\n+\t\tif v == nil {\n+\t\t\treturn w.int(0)\n+\t\t}\n \t\treturn w.int(int64(*v))\n \tcase int8:\n \t\treturn w.int(int64(v))\n \tcase *int8:\n+\t\tif v == nil {\n+\t\t\treturn w.int(0)\n+\t\t}\n \t\treturn w.int(int64(*v))\n \tcase int16:\n \t\treturn w.int(int64(v))\n \tcase *int16:\n+\t\tif v == nil {\n+\t\t\treturn w.int(0)\n+\t\t}\n \t\treturn w.int(int64(*v))\n \tcase int32:\n \t\treturn w.int(int64(v))\n \tcase *int32:\n+\t\tif v == nil {\n+\t\t\treturn w.int(0)\n+\t\t}\n \t\treturn w.int(int64(*v))\n \tcase int64:\n \t\treturn w.int(v)\n \tcase *int64:\n+\t\tif v == nil {\n+\t\t\treturn w.int(0)\n+\t\t}\n \t\treturn w.int(*v)\n \tcase uint:\n \t\treturn w.uint(uint64(v))\n \tcase *uint:\n+\t\tif v == nil {\n+\t\t\treturn w.uint(0)\n+\t\t}\n \t\treturn w.uint(uint64(*v))\n \tcase uint8:\n \t\treturn w.uint(uint64(v))\n \tcase *uint8:\n+\t\tif v == nil {\n+\t\t\treturn w.string(\"\")\n+\t\t}\n \t\treturn w.uint(uint64(*v))\n \tcase uint16:\n \t\treturn w.uint(uint64(v))\n \tcase *uint16:\n+\t\tif v == nil {\n+\t\t\treturn w.uint(0)\n+\t\t}\n \t\treturn w.uint(uint64(*v))\n \tcase uint32:\n \t\treturn w.uint(uint64(v))\n \tcase *uint32:\n+\t\tif v == nil {\n+\t\t\treturn w.uint(0)\n+\t\t}\n \t\treturn w.uint(uint64(*v))\n \tcase uint64:\n \t\treturn w.uint(v)\n \tcase *uint64:\n+\t\tif v == nil {\n+\t\t\treturn w.uint(0)\n+\t\t}\n \t\treturn w.uint(*v)\n \tcase float32:\n \t\treturn w.float(float64(v))\n \tcase *float32:\n+\t\tif v == nil {\n+\t\t\treturn w.float(0)\n+\t\t}\n \t\treturn w.float(float64(*v))\n \tcase float64:\n \t\treturn w.float(v)\n \tcase *float64:\n+\t\tif v == nil {\n+\t\t\treturn w.float(0)\n+\t\t}\n \t\treturn w.float(*v)\n \tcase bool:\n \t\tif v {\n \t\t\treturn w.int(1)\n \t\t}\n \t\treturn w.int(0)\n \tcase *bool:\n+\t\tif v == nil {\n+\t\t\treturn w.int(0)\n+\t\t}\n \t\tif *v {\n \t\t\treturn w.int(1)\n \t\t}\n \t\treturn w.int(0)\n \tcase time.Time:\n \t\tw.numBuf = v.AppendFormat(w.numBuf[:0], time.RFC3339Nano)\n \t\treturn w.bytes(w.numBuf)\n+\tcase *time.Time:\n+\t\tif v == nil {\n+\t\t\tv = &time.Time{}\n+\t\t}\n+\t\tw.numBuf = v.AppendFormat(w.numBuf[:0], time.RFC3339Nano)\n+\t\treturn w.bytes(w.numBuf)\n \tcase time.Duration:\n \t\treturn w.int(v.Nanoseconds())\n+\tcase *time.Duration:\n+\t\tif v == nil {\n+\t\t\treturn w.int(0)\n+\t\t}\n+\t\treturn w.int(v.Nanoseconds())\n \tcase encoding.BinaryMarshaler:\n \t\tb, err := v.MarshalBinary()\n \t\tif err != nil {"
    },
    {
      "sha": "190bbebeac0cb9ca2318316472669edd7b8511f6",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/redis.go",
      "status": "added",
      "additions": 3,
      "deletions": 0,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fredis.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fredis.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fredis.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,3 @@\n+package internal\n+\n+const RedisNull = \"<nil>\""
    },
    {
      "sha": "a1dfca5ff3fed0dde4c6966df5211bef47b2e826",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/semaphore.go",
      "status": "added",
      "additions": 193,
      "deletions": 0,
      "changes": 193,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fsemaphore.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fsemaphore.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Fsemaphore.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,193 @@\n+package internal\n+\n+import (\n+\t\"context\"\n+\t\"sync\"\n+\t\"time\"\n+)\n+\n+var semTimers = sync.Pool{\n+\tNew: func() interface{} {\n+\t\tt := time.NewTimer(time.Hour)\n+\t\tt.Stop()\n+\t\treturn t\n+\t},\n+}\n+\n+// FastSemaphore is a channel-based semaphore optimized for performance.\n+// It uses a fast path that avoids timer allocation when tokens are available.\n+// The channel is pre-filled with tokens: Acquire = receive, Release = send.\n+// Closing the semaphore unblocks all waiting goroutines.\n+//\n+// Performance: ~30 ns/op with zero allocations on fast path.\n+// Fairness: Eventual fairness (no starvation) but not strict FIFO.\n+type FastSemaphore struct {\n+\ttokens chan struct{}\n+\tmax    int32\n+}\n+\n+// NewFastSemaphore creates a new fast semaphore with the given capacity.\n+func NewFastSemaphore(capacity int32) *FastSemaphore {\n+\tch := make(chan struct{}, capacity)\n+\t// Pre-fill with tokens\n+\tfor i := int32(0); i < capacity; i++ {\n+\t\tch <- struct{}{}\n+\t}\n+\treturn &FastSemaphore{\n+\t\ttokens: ch,\n+\t\tmax:    capacity,\n+\t}\n+}\n+\n+// TryAcquire attempts to acquire a token without blocking.\n+// Returns true if successful, false if no tokens available.\n+func (s *FastSemaphore) TryAcquire() bool {\n+\tselect {\n+\tcase <-s.tokens:\n+\t\treturn true\n+\tdefault:\n+\t\treturn false\n+\t}\n+}\n+\n+// Acquire acquires a token, blocking if necessary until one is available.\n+// Returns an error if the context is cancelled or the timeout expires.\n+// Uses a fast path to avoid timer allocation when tokens are immediately available.\n+func (s *FastSemaphore) Acquire(ctx context.Context, timeout time.Duration, timeoutErr error) error {\n+\t// Check context first\n+\tselect {\n+\tcase <-ctx.Done():\n+\t\treturn ctx.Err()\n+\tdefault:\n+\t}\n+\n+\t// Try fast path first (no timer needed)\n+\tselect {\n+\tcase <-s.tokens:\n+\t\treturn nil\n+\tdefault:\n+\t}\n+\n+\t// Slow path: need to wait with timeout\n+\ttimer := semTimers.Get().(*time.Timer)\n+\tdefer semTimers.Put(timer)\n+\ttimer.Reset(timeout)\n+\n+\tselect {\n+\tcase <-s.tokens:\n+\t\tif !timer.Stop() {\n+\t\t\t<-timer.C\n+\t\t}\n+\t\treturn nil\n+\tcase <-ctx.Done():\n+\t\tif !timer.Stop() {\n+\t\t\t<-timer.C\n+\t\t}\n+\t\treturn ctx.Err()\n+\tcase <-timer.C:\n+\t\treturn timeoutErr\n+\t}\n+}\n+\n+// AcquireBlocking acquires a token, blocking indefinitely until one is available.\n+func (s *FastSemaphore) AcquireBlocking() {\n+\t<-s.tokens\n+}\n+\n+// Release releases a token back to the semaphore.\n+func (s *FastSemaphore) Release() {\n+\ts.tokens <- struct{}{}\n+}\n+\n+// Close closes the semaphore, unblocking all waiting goroutines.\n+// After close, all Acquire calls will receive a closed channel signal.\n+func (s *FastSemaphore) Close() {\n+\tclose(s.tokens)\n+}\n+\n+// Len returns the current number of acquired tokens.\n+func (s *FastSemaphore) Len() int32 {\n+\treturn s.max - int32(len(s.tokens))\n+}\n+\n+// FIFOSemaphore is a channel-based semaphore with strict FIFO ordering.\n+// Unlike FastSemaphore, this guarantees that threads are served in the exact order they call Acquire().\n+// The channel is pre-filled with tokens: Acquire = receive, Release = send.\n+// Closing the semaphore unblocks all waiting goroutines.\n+//\n+// Performance: ~115 ns/op with zero allocations (slower than FastSemaphore due to timer allocation).\n+// Fairness: Strict FIFO ordering guaranteed by Go runtime.\n+type FIFOSemaphore struct {\n+\ttokens chan struct{}\n+\tmax    int32\n+}\n+\n+// NewFIFOSemaphore creates a new FIFO semaphore with the given capacity.\n+func NewFIFOSemaphore(capacity int32) *FIFOSemaphore {\n+\tch := make(chan struct{}, capacity)\n+\t// Pre-fill with tokens\n+\tfor i := int32(0); i < capacity; i++ {\n+\t\tch <- struct{}{}\n+\t}\n+\treturn &FIFOSemaphore{\n+\t\ttokens: ch,\n+\t\tmax:    capacity,\n+\t}\n+}\n+\n+// TryAcquire attempts to acquire a token without blocking.\n+// Returns true if successful, false if no tokens available.\n+func (s *FIFOSemaphore) TryAcquire() bool {\n+\tselect {\n+\tcase <-s.tokens:\n+\t\treturn true\n+\tdefault:\n+\t\treturn false\n+\t}\n+}\n+\n+// Acquire acquires a token, blocking if necessary until one is available.\n+// Returns an error if the context is cancelled or the timeout expires.\n+// Always uses timer to guarantee FIFO ordering (no fast path).\n+func (s *FIFOSemaphore) Acquire(ctx context.Context, timeout time.Duration, timeoutErr error) error {\n+\t// No fast path - always use timer to guarantee FIFO\n+\ttimer := semTimers.Get().(*time.Timer)\n+\tdefer semTimers.Put(timer)\n+\ttimer.Reset(timeout)\n+\n+\tselect {\n+\tcase <-s.tokens:\n+\t\tif !timer.Stop() {\n+\t\t\t<-timer.C\n+\t\t}\n+\t\treturn nil\n+\tcase <-ctx.Done():\n+\t\tif !timer.Stop() {\n+\t\t\t<-timer.C\n+\t\t}\n+\t\treturn ctx.Err()\n+\tcase <-timer.C:\n+\t\treturn timeoutErr\n+\t}\n+}\n+\n+// AcquireBlocking acquires a token, blocking indefinitely until one is available.\n+func (s *FIFOSemaphore) AcquireBlocking() {\n+\t<-s.tokens\n+}\n+\n+// Release releases a token back to the semaphore.\n+func (s *FIFOSemaphore) Release() {\n+\ts.tokens <- struct{}{}\n+}\n+\n+// Close closes the semaphore, unblocking all waiting goroutines.\n+// After close, all Acquire calls will receive a closed channel signal.\n+func (s *FIFOSemaphore) Close() {\n+\tclose(s.tokens)\n+}\n+\n+// Len returns the current number of acquired tokens.\n+func (s *FIFOSemaphore) Len() int32 {\n+\treturn s.max - int32(len(s.tokens))\n+}\n\\ No newline at end of file"
    },
    {
      "sha": "f77775ff40545bb73445049cb3ee1d5704a07cc1",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/util.go",
      "status": "modified",
      "additions": 1,
      "deletions": 16,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Futil.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Futil.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Futil.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -49,22 +49,7 @@ func isLower(s string) bool {\n }\n \n func ReplaceSpaces(s string) string {\n-\t// Pre-allocate a builder with the same length as s to minimize allocations.\n-\t// This is a basic optimization; adjust the initial size based on your use case.\n-\tvar builder strings.Builder\n-\tbuilder.Grow(len(s))\n-\n-\tfor _, char := range s {\n-\t\tif char == ' ' {\n-\t\t\t// Replace space with a hyphen.\n-\t\t\tbuilder.WriteRune('-')\n-\t\t} else {\n-\t\t\t// Copy the character as-is.\n-\t\t\tbuilder.WriteRune(char)\n-\t\t}\n-\t}\n-\n-\treturn builder.String()\n+\treturn strings.ReplaceAll(s, \" \", \"-\")\n }\n \n func GetAddr(addr string) string {"
    },
    {
      "sha": "b743a4f0eb3d9d0d26927f0ac4db51af4bc8aa4b",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/util/convert.go",
      "status": "added",
      "additions": 41,
      "deletions": 0,
      "changes": 41,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Futil%2Fconvert.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Futil%2Fconvert.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Futil%2Fconvert.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,41 @@\n+package util\n+\n+import (\n+\t\"fmt\"\n+\t\"math\"\n+\t\"strconv\"\n+)\n+\n+// ParseFloat parses a Redis RESP3 float reply into a Go float64,\n+// handling \"inf\", \"-inf\", \"nan\" per Redis conventions.\n+func ParseStringToFloat(s string) (float64, error) {\n+\tswitch s {\n+\tcase \"inf\":\n+\t\treturn math.Inf(1), nil\n+\tcase \"-inf\":\n+\t\treturn math.Inf(-1), nil\n+\tcase \"nan\", \"-nan\":\n+\t\treturn math.NaN(), nil\n+\t}\n+\treturn strconv.ParseFloat(s, 64)\n+}\n+\n+// MustParseFloat is like ParseFloat but panics on parse errors.\n+func MustParseFloat(s string) float64 {\n+\tf, err := ParseStringToFloat(s)\n+\tif err != nil {\n+\t\tpanic(fmt.Sprintf(\"redis: failed to parse float %q: %v\", s, err))\n+\t}\n+\treturn f\n+}\n+\n+// SafeIntToInt32 safely converts an int to int32, returning an error if overflow would occur.\n+func SafeIntToInt32(value int, fieldName string) (int32, error) {\n+\tif value > math.MaxInt32 {\n+\t\treturn 0, fmt.Errorf(\"redis: %s value %d exceeds maximum allowed value %d\", fieldName, value, math.MaxInt32)\n+\t}\n+\tif value < math.MinInt32 {\n+\t\treturn 0, fmt.Errorf(\"redis: %s value %d is below minimum allowed value %d\", fieldName, value, math.MinInt32)\n+\t}\n+\treturn int32(value), nil\n+}"
    },
    {
      "sha": "e707c47a645c05f84a7a51529850a88108b410cf",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/internal/util/math.go",
      "status": "added",
      "additions": 17,
      "deletions": 0,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Futil%2Fmath.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Futil%2Fmath.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Finternal%2Futil%2Fmath.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,17 @@\n+package util\n+\n+// Max returns the maximum of two integers\n+func Max(a, b int) int {\n+\tif a > b {\n+\t\treturn a\n+\t}\n+\treturn b\n+}\n+\n+// Min returns the minimum of two integers\n+func Min(a, b int) int {\n+\tif a < b {\n+\t\treturn a\n+\t}\n+\treturn b\n+}"
    },
    {
      "sha": "2b9fa527ee00e40995b9c8ef44f6c0229dc58107",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/json.go",
      "status": "modified",
      "additions": 16,
      "deletions": 0,
      "changes": 16,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fjson.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fjson.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fjson.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -82,6 +82,7 @@ func (cmd *JSONCmd) SetVal(val string) {\n \tcmd.val = val\n }\n \n+// Val returns the result of the JSON.GET command as a string.\n func (cmd *JSONCmd) Val() string {\n \tif len(cmd.val) == 0 && cmd.expanded != nil {\n \t\tval, err := json.Marshal(cmd.expanded)\n@@ -100,6 +101,7 @@ func (cmd *JSONCmd) Result() (string, error) {\n \treturn cmd.Val(), cmd.Err()\n }\n \n+// Expanded returns the result of the JSON.GET command as unmarshalled JSON.\n func (cmd *JSONCmd) Expanded() (interface{}, error) {\n \tif len(cmd.val) != 0 && cmd.expanded == nil {\n \t\terr := json.Unmarshal([]byte(cmd.val), &cmd.expanded)\n@@ -113,11 +115,17 @@ func (cmd *JSONCmd) Expanded() (interface{}, error) {\n \n func (cmd *JSONCmd) readReply(rd *proto.Reader) error {\n \t// nil response from JSON.(M)GET (cmd.baseCmd.err will be \"redis: nil\")\n+\t// This happens when the key doesn't exist\n \tif cmd.baseCmd.Err() == Nil {\n \t\tcmd.val = \"\"\n \t\treturn Nil\n \t}\n \n+\t// Handle other base command errors\n+\tif cmd.baseCmd.Err() != nil {\n+\t\treturn cmd.baseCmd.Err()\n+\t}\n+\n \tif readType, err := rd.PeekReplyType(); err != nil {\n \t\treturn err\n \t} else if readType == proto.RespArray {\n@@ -127,6 +135,13 @@ func (cmd *JSONCmd) readReply(rd *proto.Reader) error {\n \t\t\treturn err\n \t\t}\n \n+\t\t// Empty array means no results found for JSON path, but key exists\n+\t\t// This should return \"[]\", not an error\n+\t\tif size == 0 {\n+\t\t\tcmd.val = \"[]\"\n+\t\t\treturn nil\n+\t\t}\n+\n \t\texpanded := make([]interface{}, size)\n \n \t\tfor i := 0; i < size; i++ {\n@@ -141,6 +156,7 @@ func (cmd *JSONCmd) readReply(rd *proto.Reader) error {\n \t\t\treturn err\n \t\t} else if str == \"\" || err == Nil {\n \t\t\tcmd.val = \"\"\n+\t\t\treturn Nil\n \t\t} else {\n \t\t\tcmd.val = str\n \t\t}"
    },
    {
      "sha": "caa4f705eb95cb1ca5f0015cc0b5f38f6294b703",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/FEATURES.md",
      "status": "added",
      "additions": 218,
      "deletions": 0,
      "changes": 218,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2FFEATURES.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2FFEATURES.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2FFEATURES.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,218 @@\n+# Maintenance Notifications - FEATURES\n+\n+## Overview\n+\n+The Maintenance Notifications feature enables seamless Redis connection handoffs during cluster maintenance operations without dropping active connections. This feature leverages Redis RESP3 push notifications to provide zero-downtime maintenance for Redis Enterprise and compatible Redis deployments.\n+\n+## Important\n+\n+Using Maintenance Notifications may affect the read and write timeouts by relaxing them during maintenance operations.\n+This is necessary to prevent false failures due to increased latency during handoffs. The relaxed timeouts are automatically applied and removed as needed.\n+\n+## Key Features\n+\n+### Seamless Connection Handoffs\n+- **Zero-Downtime Maintenance**: Automatically handles connection transitions during cluster operations\n+- **Active Operation Preservation**: Transfers in-flight operations to new connections without interruption\n+- **Graceful Degradation**: Falls back to standard reconnection if handoff fails\n+\n+### Push Notification Support\n+Supports all Redis Enterprise maintenance notification types:\n+- **MOVING** - Slot moving to a new node\n+- **MIGRATING** - Slot in migration state\n+- **MIGRATED** - Migration completed\n+- **FAILING_OVER** - Node failing over\n+- **FAILED_OVER** - Failover completed\n+\n+### Circuit Breaker Pattern\n+- **Endpoint-Specific Failure Tracking**: Prevents repeated connection attempts to failing endpoints\n+- **Automatic Recovery Testing**: Half-open state allows gradual recovery validation\n+- **Configurable Thresholds**: Customize failure thresholds and reset timeouts\n+\n+### Flexible Configuration\n+- **Auto-Detection Mode**: Automatically detects server support for maintenance notifications\n+- **Multiple Endpoint Types**: Support for internal/external IP/FQDN endpoint resolution\n+- **Auto-Scaling Workers**: Automatically sizes worker pool based on connection pool size\n+- **Timeout Management**: Separate timeouts for relaxed (during maintenance) and normal operations\n+\n+### Extensible Hook System\n+- **Pre/Post Processing Hooks**: Monitor and customize notification handling\n+- **Built-in Hooks**: Logging and metrics collection hooks included\n+- **Custom Hook Support**: Implement custom business logic around maintenance events\n+\n+### Comprehensive Monitoring\n+- **Metrics Collection**: Track notification counts, processing times, and error rates\n+- **Circuit Breaker Stats**: Monitor endpoint health and circuit breaker states\n+- **Operation Tracking**: Track active handoff operations and their lifecycle\n+\n+## Architecture Highlights\n+\n+### Event-Driven Handoff System\n+- **Asynchronous Processing**: Non-blocking handoff operations using worker pool pattern\n+- **Queue-Based Architecture**: Configurable queue size with auto-scaling support\n+- **Retry Mechanism**: Configurable retry attempts with exponential backoff\n+\n+### Connection Pool Integration\n+- **Pool Hook Interface**: Seamless integration with go-redis connection pool\n+- **Connection State Management**: Atomic flags for connection usability tracking\n+- **Graceful Shutdown**: Ensures all in-flight handoffs complete before shutdown\n+\n+### Thread-Safe Design\n+- **Lock-Free Operations**: Atomic operations for high-performance state tracking\n+- **Concurrent-Safe Maps**: sync.Map for tracking active operations\n+- **Minimal Lock Contention**: Read-write locks only where necessary\n+\n+## Configuration Options\n+\n+### Operation Modes\n+- **`ModeDisabled`**: Maintenance notifications completely disabled\n+- **`ModeEnabled`**: Forcefully enabled (fails if server doesn't support)\n+- **`ModeAuto`**: Auto-detect server support (recommended default)\n+\n+### Endpoint Types\n+- **`EndpointTypeAuto`**: Auto-detect based on current connection\n+- **`EndpointTypeInternalIP`**: Use internal IP addresses\n+- **`EndpointTypeInternalFQDN`**: Use internal fully qualified domain names\n+- **`EndpointTypeExternalIP`**: Use external IP addresses\n+- **`EndpointTypeExternalFQDN`**: Use external fully qualified domain names\n+- **`EndpointTypeNone`**: No endpoint (reconnect with current configuration)\n+\n+### Timeout Configuration\n+- **`RelaxedTimeout`**: Extended timeout during maintenance operations (default: 10s)\n+- **`HandoffTimeout`**: Maximum time for handoff completion (default: 15s)\n+- **`PostHandoffRelaxedDuration`**: Relaxed period after handoff (default: 2RelaxedTimeout)\n+\n+### Worker Pool Configuration\n+- **`MaxWorkers`**: Maximum concurrent handoff workers (auto-calculated if 0)\n+- **`HandoffQueueSize`**: Handoff queue capacity (auto-calculated if 0)\n+- **`MaxHandoffRetries`**: Maximum retry attempts for failed handoffs (default: 3)\n+\n+### Circuit Breaker Configuration\n+- **`CircuitBreakerFailureThreshold`**: Failures before opening circuit (default: 5)\n+- **`CircuitBreakerResetTimeout`**: Time before testing recovery (default: 60s)\n+- **`CircuitBreakerMaxRequests`**: Max requests in half-open state (default: 3)\n+\n+## Auto-Scaling Formulas\n+\n+### Worker Pool Sizing\n+When `MaxWorkers = 0` (auto-calculate):\n+```\n+MaxWorkers = min(PoolSize/2, max(10, PoolSize/3))\n+```\n+\n+### Queue Sizing\n+When `HandoffQueueSize = 0` (auto-calculate):\n+```\n+QueueSize = max(20  MaxWorkers, PoolSize)\n+Capped by: min(MaxActiveConns + 1, 5  PoolSize)\n+```\n+\n+### Examples\n+- **Pool Size 100**: 33 workers, 660 queue (capped at 500)\n+- **Pool Size 100 + MaxActiveConns 150**: 33 workers, 151 queue\n+- **Pool Size 50**: 16 workers, 320 queue (capped at 250)\n+\n+## Performance Characteristics\n+\n+### Throughput\n+- **Non-Blocking Handoffs**: Client operations continue during handoffs\n+- **Concurrent Processing**: Multiple handoffs processed in parallel\n+- **Minimal Overhead**: Lock-free atomic operations for state tracking\n+\n+### Latency\n+- **Relaxed Timeouts**: Extended timeouts during maintenance prevent false failures\n+- **Fast Path**: Connections not undergoing handoff have zero overhead\n+- **Graceful Degradation**: Failed handoffs fall back to standard reconnection\n+\n+### Resource Usage\n+- **Memory Efficient**: Bounded queue sizes prevent memory exhaustion\n+- **Worker Pool**: Fixed worker count prevents goroutine explosion\n+- **Connection Reuse**: Handoff reuses existing connection objects\n+\n+## Testing\n+\n+### Unit Tests\n+- Comprehensive unit test coverage for all components\n+- Mock-based testing for isolation\n+- Concurrent operation testing\n+\n+### Integration Tests\n+- Pool integration tests with real connection handoffs\n+- Circuit breaker behavior validation\n+- Hook system integration testing\n+\n+### E2E Tests\n+- Real Redis Enterprise cluster testing\n+- Multiple scenario coverage (timeouts, endpoint types, stress tests)\n+- Fault injection testing\n+- TLS configuration testing\n+\n+## Compatibility\n+\n+### Requirements\n+- **Redis Protocol**: RESP3 required for push notifications\n+- **Redis Version**: Redis Enterprise or compatible Redis with maintenance notifications\n+- **Go Version**: Go 1.18+ (uses generics and atomic types)\n+\n+### Client Support\n+#### Currently Supported\n+- **Standalone Client** (`redis.NewClient`)\n+\n+#### Planned Support\n+- **Cluster Client** (not yet supported)\n+ \n+#### Will Not Support\n+- **Failover Client** (no planned support)\n+- **Ring Client** (no planned support)\n+\n+## Migration Guide\n+\n+### Enabling Maintenance Notifications\n+\n+**Before:**\n+```go\n+client := redis.NewClient(&redis.Options{\n+    Addr:     \"localhost:6379\",\n+    Protocol: 2, // RESP2\n+})\n+```\n+\n+**After:**\n+```go\n+client := redis.NewClient(&redis.Options{\n+    Addr:     \"localhost:6379\",\n+    Protocol: 3, // RESP3 required\n+    MaintNotificationsConfig: &maintnotifications.Config{\n+        Mode: maintnotifications.ModeAuto,\n+    },\n+})\n+```\n+\n+### Adding Monitoring\n+\n+```go\n+// Get the manager from the client\n+manager := client.GetMaintNotificationsManager()\n+if manager != nil {\n+    // Add logging hook\n+    loggingHook := maintnotifications.NewLoggingHook(2) // Info level\n+    manager.AddNotificationHook(loggingHook)\n+    \n+    // Add metrics hook\n+    metricsHook := maintnotifications.NewMetricsHook()\n+    manager.AddNotificationHook(metricsHook)\n+}\n+```\n+\n+## Known Limitations\n+\n+1. **Standalone Only**: Currently only supported in standalone Redis clients\n+2. **RESP3 Required**: Push notifications require RESP3 protocol\n+3. **Server Support**: Requires Redis Enterprise or compatible Redis with maintenance notifications\n+4. **Single Connection Commands**: Some commands (MULTI/EXEC, WATCH) may need special handling\n+5. **No Failover/Ring Client Support**: Failover and Ring clients are not supported and there are no plans to add support\n+\n+## Future Enhancements\n+\n+- Cluster client support\n+- Enhanced metrics and observability\n\\ No newline at end of file"
    },
    {
      "sha": "2ac6b9cb1d7e6359be6d2a6a89da78ae44c90195",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/README.md",
      "status": "added",
      "additions": 67,
      "deletions": 0,
      "changes": 67,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2FREADME.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2FREADME.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2FREADME.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,67 @@\n+# Maintenance Notifications\n+\n+Seamless Redis connection handoffs during cluster maintenance operations without dropping connections.\n+\n+##  **Important Note**\n+**Maintenance notifications are currently supported only in standalone Redis clients.** Cluster clients (ClusterClient, FailoverClient, etc.) do not yet support this functionality.\n+\n+## Quick Start\n+\n+```go\n+client := redis.NewClient(&redis.Options{\n+    Addr:     \"localhost:6379\",\n+    Protocol: 3, // RESP3 required\n+\tMaintNotificationsConfig: &maintnotifications.Config{\n+        Mode: maintnotifications.ModeEnabled,\n+    },\n+})\n+```\n+\n+## Modes\n+\n+- **`ModeDisabled`** - Maintenance notifications disabled\n+- **`ModeEnabled`** - Forcefully enabled (fails if server doesn't support)\n+- **`ModeAuto`** - Auto-detect server support (default)\n+\n+## Configuration\n+\n+```go\n+&maintnotifications.Config{\n+    Mode:                       maintnotifications.ModeAuto,\n+    EndpointType:               maintnotifications.EndpointTypeAuto,\n+    RelaxedTimeout:             10 * time.Second,\n+    HandoffTimeout:             15 * time.Second,\n+    MaxHandoffRetries:          3,\n+    MaxWorkers:                 0,    // Auto-calculated\n+    HandoffQueueSize:           0,    // Auto-calculated\n+    PostHandoffRelaxedDuration: 0,    // 2 * RelaxedTimeout\n+}\n+```\n+\n+### Endpoint Types\n+\n+- **`EndpointTypeAuto`** - Auto-detect based on connection (default)\n+- **`EndpointTypeInternalIP`** - Internal IP address\n+- **`EndpointTypeInternalFQDN`** - Internal FQDN\n+- **`EndpointTypeExternalIP`** - External IP address\n+- **`EndpointTypeExternalFQDN`** - External FQDN\n+- **`EndpointTypeNone`** - No endpoint (reconnect with current config)\n+\n+### Auto-Scaling\n+\n+**Workers**: `min(PoolSize/2, max(10, PoolSize/3))` when auto-calculated\n+**Queue**: `max(20Workers, PoolSize)` capped by `MaxActiveConns+1` or `5PoolSize`\n+\n+**Examples:**\n+- Pool 100: 33 workers, 660 queue (capped at 500)\n+- Pool 100 + MaxActiveConns 150: 33 workers, 151 queue\n+\n+## How It Works\n+\n+1. Redis sends push notifications about cluster maintenance operations\n+2. Client creates new connections to updated endpoints\n+3. Active operations transfer to new connections\n+4. Old connections close gracefully\n+\n+\n+## For more information, see [FEATURES](FEATURES.md)"
    },
    {
      "sha": "cb76b6447fb1e7eb63950b0ad1e615e2c4107183",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/circuit_breaker.go",
      "status": "added",
      "additions": 353,
      "deletions": 0,
      "changes": 353,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fcircuit_breaker.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fcircuit_breaker.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fcircuit_breaker.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,353 @@\n+package maintnotifications\n+\n+import (\n+\t\"context\"\n+\t\"sync\"\n+\t\"sync/atomic\"\n+\t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/maintnotifications/logs\"\n+)\n+\n+// CircuitBreakerState represents the state of a circuit breaker\n+type CircuitBreakerState int32\n+\n+const (\n+\t// CircuitBreakerClosed - normal operation, requests allowed\n+\tCircuitBreakerClosed CircuitBreakerState = iota\n+\t// CircuitBreakerOpen - failing fast, requests rejected\n+\tCircuitBreakerOpen\n+\t// CircuitBreakerHalfOpen - testing if service recovered\n+\tCircuitBreakerHalfOpen\n+)\n+\n+func (s CircuitBreakerState) String() string {\n+\tswitch s {\n+\tcase CircuitBreakerClosed:\n+\t\treturn \"closed\"\n+\tcase CircuitBreakerOpen:\n+\t\treturn \"open\"\n+\tcase CircuitBreakerHalfOpen:\n+\t\treturn \"half-open\"\n+\tdefault:\n+\t\treturn \"unknown\"\n+\t}\n+}\n+\n+// CircuitBreaker implements the circuit breaker pattern for endpoint-specific failure handling\n+type CircuitBreaker struct {\n+\t// Configuration\n+\tfailureThreshold int           // Number of failures before opening\n+\tresetTimeout     time.Duration // How long to stay open before testing\n+\tmaxRequests      int           // Max requests allowed in half-open state\n+\n+\t// State tracking (atomic for lock-free access)\n+\tstate           atomic.Int32 // CircuitBreakerState\n+\tfailures        atomic.Int64 // Current failure count\n+\tsuccesses       atomic.Int64 // Success count in half-open state\n+\trequests        atomic.Int64 // Request count in half-open state\n+\tlastFailureTime atomic.Int64 // Unix timestamp of last failure\n+\tlastSuccessTime atomic.Int64 // Unix timestamp of last success\n+\n+\t// Endpoint identification\n+\tendpoint string\n+\tconfig   *Config\n+}\n+\n+// newCircuitBreaker creates a new circuit breaker for an endpoint\n+func newCircuitBreaker(endpoint string, config *Config) *CircuitBreaker {\n+\t// Use configuration values with sensible defaults\n+\tfailureThreshold := 5\n+\tresetTimeout := 60 * time.Second\n+\tmaxRequests := 3\n+\n+\tif config != nil {\n+\t\tfailureThreshold = config.CircuitBreakerFailureThreshold\n+\t\tresetTimeout = config.CircuitBreakerResetTimeout\n+\t\tmaxRequests = config.CircuitBreakerMaxRequests\n+\t}\n+\n+\treturn &CircuitBreaker{\n+\t\tfailureThreshold: failureThreshold,\n+\t\tresetTimeout:     resetTimeout,\n+\t\tmaxRequests:      maxRequests,\n+\t\tendpoint:         endpoint,\n+\t\tconfig:           config,\n+\t\tstate:            atomic.Int32{}, // Defaults to CircuitBreakerClosed (0)\n+\t}\n+}\n+\n+// IsOpen returns true if the circuit breaker is open (rejecting requests)\n+func (cb *CircuitBreaker) IsOpen() bool {\n+\tstate := CircuitBreakerState(cb.state.Load())\n+\treturn state == CircuitBreakerOpen\n+}\n+\n+// shouldAttemptReset checks if enough time has passed to attempt reset\n+func (cb *CircuitBreaker) shouldAttemptReset() bool {\n+\tlastFailure := time.Unix(cb.lastFailureTime.Load(), 0)\n+\treturn time.Since(lastFailure) >= cb.resetTimeout\n+}\n+\n+// Execute runs the given function with circuit breaker protection\n+func (cb *CircuitBreaker) Execute(fn func() error) error {\n+\t// Single atomic state load for consistency\n+\tstate := CircuitBreakerState(cb.state.Load())\n+\n+\tswitch state {\n+\tcase CircuitBreakerOpen:\n+\t\tif cb.shouldAttemptReset() {\n+\t\t\t// Attempt transition to half-open\n+\t\t\tif cb.state.CompareAndSwap(int32(CircuitBreakerOpen), int32(CircuitBreakerHalfOpen)) {\n+\t\t\t\tcb.requests.Store(0)\n+\t\t\t\tcb.successes.Store(0)\n+\t\t\t\tif internal.LogLevel.InfoOrAbove() {\n+\t\t\t\t\tinternal.Logger.Printf(context.Background(), logs.CircuitBreakerTransitioningToHalfOpen(cb.endpoint))\n+\t\t\t\t}\n+\t\t\t\t// Fall through to half-open logic\n+\t\t\t} else {\n+\t\t\t\treturn ErrCircuitBreakerOpen\n+\t\t\t}\n+\t\t} else {\n+\t\t\treturn ErrCircuitBreakerOpen\n+\t\t}\n+\t\tfallthrough\n+\tcase CircuitBreakerHalfOpen:\n+\t\trequests := cb.requests.Add(1)\n+\t\tif requests > int64(cb.maxRequests) {\n+\t\t\tcb.requests.Add(-1) // Revert the increment\n+\t\t\treturn ErrCircuitBreakerOpen\n+\t\t}\n+\t}\n+\n+\t// Execute the function with consistent state\n+\terr := fn()\n+\n+\tif err != nil {\n+\t\tcb.recordFailure()\n+\t\treturn err\n+\t}\n+\n+\tcb.recordSuccess()\n+\treturn nil\n+}\n+\n+// recordFailure records a failure and potentially opens the circuit\n+func (cb *CircuitBreaker) recordFailure() {\n+\tcb.lastFailureTime.Store(time.Now().Unix())\n+\tfailures := cb.failures.Add(1)\n+\n+\tstate := CircuitBreakerState(cb.state.Load())\n+\n+\tswitch state {\n+\tcase CircuitBreakerClosed:\n+\t\tif failures >= int64(cb.failureThreshold) {\n+\t\t\tif cb.state.CompareAndSwap(int32(CircuitBreakerClosed), int32(CircuitBreakerOpen)) {\n+\t\t\t\tif internal.LogLevel.WarnOrAbove() {\n+\t\t\t\t\tinternal.Logger.Printf(context.Background(), logs.CircuitBreakerOpened(cb.endpoint, failures))\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\tcase CircuitBreakerHalfOpen:\n+\t\t// Any failure in half-open state immediately opens the circuit\n+\t\tif cb.state.CompareAndSwap(int32(CircuitBreakerHalfOpen), int32(CircuitBreakerOpen)) {\n+\t\t\tif internal.LogLevel.WarnOrAbove() {\n+\t\t\t\tinternal.Logger.Printf(context.Background(), logs.CircuitBreakerReopened(cb.endpoint))\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+// recordSuccess records a success and potentially closes the circuit\n+func (cb *CircuitBreaker) recordSuccess() {\n+\tcb.lastSuccessTime.Store(time.Now().Unix())\n+\n+\tstate := CircuitBreakerState(cb.state.Load())\n+\n+\tswitch state {\n+\tcase CircuitBreakerClosed:\n+\t\t// Reset failure count on success in closed state\n+\t\tcb.failures.Store(0)\n+\tcase CircuitBreakerHalfOpen:\n+\t\tsuccesses := cb.successes.Add(1)\n+\n+\t\t// If we've had enough successful requests, close the circuit\n+\t\tif successes >= int64(cb.maxRequests) {\n+\t\t\tif cb.state.CompareAndSwap(int32(CircuitBreakerHalfOpen), int32(CircuitBreakerClosed)) {\n+\t\t\t\tcb.failures.Store(0)\n+\t\t\t\tif internal.LogLevel.InfoOrAbove() {\n+\t\t\t\t\tinternal.Logger.Printf(context.Background(), logs.CircuitBreakerClosed(cb.endpoint, successes))\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+// GetState returns the current state of the circuit breaker\n+func (cb *CircuitBreaker) GetState() CircuitBreakerState {\n+\treturn CircuitBreakerState(cb.state.Load())\n+}\n+\n+// GetStats returns current statistics for monitoring\n+func (cb *CircuitBreaker) GetStats() CircuitBreakerStats {\n+\treturn CircuitBreakerStats{\n+\t\tEndpoint:        cb.endpoint,\n+\t\tState:           cb.GetState(),\n+\t\tFailures:        cb.failures.Load(),\n+\t\tSuccesses:       cb.successes.Load(),\n+\t\tRequests:        cb.requests.Load(),\n+\t\tLastFailureTime: time.Unix(cb.lastFailureTime.Load(), 0),\n+\t\tLastSuccessTime: time.Unix(cb.lastSuccessTime.Load(), 0),\n+\t}\n+}\n+\n+// CircuitBreakerStats provides statistics about a circuit breaker\n+type CircuitBreakerStats struct {\n+\tEndpoint        string\n+\tState           CircuitBreakerState\n+\tFailures        int64\n+\tSuccesses       int64\n+\tRequests        int64\n+\tLastFailureTime time.Time\n+\tLastSuccessTime time.Time\n+}\n+\n+// CircuitBreakerEntry wraps a circuit breaker with access tracking\n+type CircuitBreakerEntry struct {\n+\tbreaker    *CircuitBreaker\n+\tlastAccess atomic.Int64 // Unix timestamp\n+\tcreated    time.Time\n+}\n+\n+// CircuitBreakerManager manages circuit breakers for multiple endpoints\n+type CircuitBreakerManager struct {\n+\tbreakers    sync.Map // map[string]*CircuitBreakerEntry\n+\tconfig      *Config\n+\tcleanupStop chan struct{}\n+\tcleanupMu   sync.Mutex\n+\tlastCleanup atomic.Int64 // Unix timestamp\n+}\n+\n+// newCircuitBreakerManager creates a new circuit breaker manager\n+func newCircuitBreakerManager(config *Config) *CircuitBreakerManager {\n+\tcbm := &CircuitBreakerManager{\n+\t\tconfig:      config,\n+\t\tcleanupStop: make(chan struct{}),\n+\t}\n+\tcbm.lastCleanup.Store(time.Now().Unix())\n+\n+\t// Start background cleanup goroutine\n+\tgo cbm.cleanupLoop()\n+\n+\treturn cbm\n+}\n+\n+// GetCircuitBreaker returns the circuit breaker for an endpoint, creating it if necessary\n+func (cbm *CircuitBreakerManager) GetCircuitBreaker(endpoint string) *CircuitBreaker {\n+\tnow := time.Now().Unix()\n+\n+\tif entry, ok := cbm.breakers.Load(endpoint); ok {\n+\t\tcbEntry := entry.(*CircuitBreakerEntry)\n+\t\tcbEntry.lastAccess.Store(now)\n+\t\treturn cbEntry.breaker\n+\t}\n+\n+\t// Create new circuit breaker with metadata\n+\tnewBreaker := newCircuitBreaker(endpoint, cbm.config)\n+\tnewEntry := &CircuitBreakerEntry{\n+\t\tbreaker: newBreaker,\n+\t\tcreated: time.Now(),\n+\t}\n+\tnewEntry.lastAccess.Store(now)\n+\n+\tactual, _ := cbm.breakers.LoadOrStore(endpoint, newEntry)\n+\treturn actual.(*CircuitBreakerEntry).breaker\n+}\n+\n+// GetAllStats returns statistics for all circuit breakers\n+func (cbm *CircuitBreakerManager) GetAllStats() []CircuitBreakerStats {\n+\tvar stats []CircuitBreakerStats\n+\tcbm.breakers.Range(func(key, value interface{}) bool {\n+\t\tentry := value.(*CircuitBreakerEntry)\n+\t\tstats = append(stats, entry.breaker.GetStats())\n+\t\treturn true\n+\t})\n+\treturn stats\n+}\n+\n+// cleanupLoop runs background cleanup of unused circuit breakers\n+func (cbm *CircuitBreakerManager) cleanupLoop() {\n+\tticker := time.NewTicker(5 * time.Minute) // Cleanup every 5 minutes\n+\tdefer ticker.Stop()\n+\n+\tfor {\n+\t\tselect {\n+\t\tcase <-ticker.C:\n+\t\t\tcbm.cleanup()\n+\t\tcase <-cbm.cleanupStop:\n+\t\t\treturn\n+\t\t}\n+\t}\n+}\n+\n+// cleanup removes circuit breakers that haven't been accessed recently\n+func (cbm *CircuitBreakerManager) cleanup() {\n+\t// Prevent concurrent cleanups\n+\tif !cbm.cleanupMu.TryLock() {\n+\t\treturn\n+\t}\n+\tdefer cbm.cleanupMu.Unlock()\n+\n+\tnow := time.Now()\n+\tcutoff := now.Add(-30 * time.Minute).Unix() // 30 minute TTL\n+\n+\tvar toDelete []string\n+\tcount := 0\n+\n+\tcbm.breakers.Range(func(key, value interface{}) bool {\n+\t\tendpoint := key.(string)\n+\t\tentry := value.(*CircuitBreakerEntry)\n+\n+\t\tcount++\n+\n+\t\t// Remove if not accessed recently\n+\t\tif entry.lastAccess.Load() < cutoff {\n+\t\t\ttoDelete = append(toDelete, endpoint)\n+\t\t}\n+\n+\t\treturn true\n+\t})\n+\n+\t// Delete expired entries\n+\tfor _, endpoint := range toDelete {\n+\t\tcbm.breakers.Delete(endpoint)\n+\t}\n+\n+\t// Log cleanup results\n+\tif len(toDelete) > 0 && internal.LogLevel.InfoOrAbove() {\n+\t\tinternal.Logger.Printf(context.Background(), logs.CircuitBreakerCleanup(len(toDelete), count))\n+\t}\n+\n+\tcbm.lastCleanup.Store(now.Unix())\n+}\n+\n+// Shutdown stops the cleanup goroutine\n+func (cbm *CircuitBreakerManager) Shutdown() {\n+\tclose(cbm.cleanupStop)\n+}\n+\n+// Reset resets all circuit breakers (useful for testing)\n+func (cbm *CircuitBreakerManager) Reset() {\n+\tcbm.breakers.Range(func(key, value interface{}) bool {\n+\t\tentry := value.(*CircuitBreakerEntry)\n+\t\tbreaker := entry.breaker\n+\t\tbreaker.state.Store(int32(CircuitBreakerClosed))\n+\t\tbreaker.failures.Store(0)\n+\t\tbreaker.successes.Store(0)\n+\t\tbreaker.requests.Store(0)\n+\t\tbreaker.lastFailureTime.Store(0)\n+\t\tbreaker.lastSuccessTime.Store(0)\n+\t\treturn true\n+\t})\n+}"
    },
    {
      "sha": "cbf4f6b22ba8f5b9c1fd33774eb8d0f5dbf178ba",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/config.go",
      "status": "added",
      "additions": 458,
      "deletions": 0,
      "changes": 458,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fconfig.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fconfig.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fconfig.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,458 @@\n+package maintnotifications\n+\n+import (\n+\t\"context\"\n+\t\"net\"\n+\t\"runtime\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/maintnotifications/logs\"\n+\t\"github.com/redis/go-redis/v9/internal/util\"\n+)\n+\n+// Mode represents the maintenance notifications mode\n+type Mode string\n+\n+// Constants for maintenance push notifications modes\n+const (\n+\tModeDisabled Mode = \"disabled\" // Client doesn't send CLIENT MAINT_NOTIFICATIONS ON command\n+\tModeEnabled  Mode = \"enabled\"  // Client forcefully sends command, interrupts connection on error\n+\tModeAuto     Mode = \"auto\"     // Client tries to send command, disables feature on error\n+)\n+\n+// IsValid returns true if the maintenance notifications mode is valid\n+func (m Mode) IsValid() bool {\n+\tswitch m {\n+\tcase ModeDisabled, ModeEnabled, ModeAuto:\n+\t\treturn true\n+\tdefault:\n+\t\treturn false\n+\t}\n+}\n+\n+// String returns the string representation of the mode\n+func (m Mode) String() string {\n+\treturn string(m)\n+}\n+\n+// EndpointType represents the type of endpoint to request in MOVING notifications\n+type EndpointType string\n+\n+// Constants for endpoint types\n+const (\n+\tEndpointTypeAuto         EndpointType = \"auto\"          // Auto-detect based on connection\n+\tEndpointTypeInternalIP   EndpointType = \"internal-ip\"   // Internal IP address\n+\tEndpointTypeInternalFQDN EndpointType = \"internal-fqdn\" // Internal FQDN\n+\tEndpointTypeExternalIP   EndpointType = \"external-ip\"   // External IP address\n+\tEndpointTypeExternalFQDN EndpointType = \"external-fqdn\" // External FQDN\n+\tEndpointTypeNone         EndpointType = \"none\"          // No endpoint (reconnect with current config)\n+)\n+\n+// IsValid returns true if the endpoint type is valid\n+func (e EndpointType) IsValid() bool {\n+\tswitch e {\n+\tcase EndpointTypeAuto, EndpointTypeInternalIP, EndpointTypeInternalFQDN,\n+\t\tEndpointTypeExternalIP, EndpointTypeExternalFQDN, EndpointTypeNone:\n+\t\treturn true\n+\tdefault:\n+\t\treturn false\n+\t}\n+}\n+\n+// String returns the string representation of the endpoint type\n+func (e EndpointType) String() string {\n+\treturn string(e)\n+}\n+\n+// Config provides configuration options for maintenance notifications\n+type Config struct {\n+\t// Mode controls how client maintenance notifications are handled.\n+\t// Valid values: ModeDisabled, ModeEnabled, ModeAuto\n+\t// Default: ModeAuto\n+\tMode Mode\n+\n+\t// EndpointType specifies the type of endpoint to request in MOVING notifications.\n+\t// Valid values: EndpointTypeAuto, EndpointTypeInternalIP, EndpointTypeInternalFQDN,\n+\t//               EndpointTypeExternalIP, EndpointTypeExternalFQDN, EndpointTypeNone\n+\t// Default: EndpointTypeAuto\n+\tEndpointType EndpointType\n+\n+\t// RelaxedTimeout is the concrete timeout value to use during\n+\t// MIGRATING/FAILING_OVER states to accommodate increased latency.\n+\t// This applies to both read and write timeouts.\n+\t// Default: 10 seconds\n+\tRelaxedTimeout time.Duration\n+\n+\t// HandoffTimeout is the maximum time to wait for connection handoff to complete.\n+\t// If handoff takes longer than this, the old connection will be forcibly closed.\n+\t// Default: 15 seconds (matches server-side eviction timeout)\n+\tHandoffTimeout time.Duration\n+\n+\t// MaxWorkers is the maximum number of worker goroutines for processing handoff requests.\n+\t// Workers are created on-demand and automatically cleaned up when idle.\n+\t// If zero, defaults to min(10, PoolSize/2) to handle bursts effectively.\n+\t// If explicitly set, enforces minimum of PoolSize/2\n+\t//\n+\t// Default: min(PoolSize/2, max(10, PoolSize/3)), Minimum when set: PoolSize/2\n+\tMaxWorkers int\n+\n+\t// HandoffQueueSize is the size of the buffered channel used to queue handoff requests.\n+\t// If the queue is full, new handoff requests will be rejected.\n+\t// Scales with both worker count and pool size for better burst handling.\n+\t//\n+\t// Default: max(20MaxWorkers, PoolSize), capped by MaxActiveConns+1 (if set) or 5PoolSize\n+\t// When set: minimum 200, capped by MaxActiveConns+1 (if set) or 5PoolSize\n+\tHandoffQueueSize int\n+\n+\t// PostHandoffRelaxedDuration is how long to keep relaxed timeouts on the new connection\n+\t// after a handoff completes. This provides additional resilience during cluster transitions.\n+\t// Default: 2 * RelaxedTimeout\n+\tPostHandoffRelaxedDuration time.Duration\n+\n+\t// Circuit breaker configuration for endpoint failure handling\n+\t// CircuitBreakerFailureThreshold is the number of failures before opening the circuit.\n+\t// Default: 5\n+\tCircuitBreakerFailureThreshold int\n+\n+\t// CircuitBreakerResetTimeout is how long to wait before testing if the endpoint recovered.\n+\t// Default: 60 seconds\n+\tCircuitBreakerResetTimeout time.Duration\n+\n+\t// CircuitBreakerMaxRequests is the maximum number of requests allowed in half-open state.\n+\t// Default: 3\n+\tCircuitBreakerMaxRequests int\n+\n+\t// MaxHandoffRetries is the maximum number of times to retry a failed handoff.\n+\t// After this many retries, the connection will be removed from the pool.\n+\t// Default: 3\n+\tMaxHandoffRetries int\n+}\n+\n+func (c *Config) IsEnabled() bool {\n+\treturn c != nil && c.Mode != ModeDisabled\n+}\n+\n+// DefaultConfig returns a Config with sensible defaults.\n+func DefaultConfig() *Config {\n+\treturn &Config{\n+\t\tMode:                       ModeAuto,         // Enable by default for Redis Cloud\n+\t\tEndpointType:               EndpointTypeAuto, // Auto-detect based on connection\n+\t\tRelaxedTimeout:             10 * time.Second,\n+\t\tHandoffTimeout:             15 * time.Second,\n+\t\tMaxWorkers:                 0, // Auto-calculated based on pool size\n+\t\tHandoffQueueSize:           0, // Auto-calculated based on max workers\n+\t\tPostHandoffRelaxedDuration: 0, // Auto-calculated based on relaxed timeout\n+\n+\t\t// Circuit breaker configuration\n+\t\tCircuitBreakerFailureThreshold: 5,\n+\t\tCircuitBreakerResetTimeout:     60 * time.Second,\n+\t\tCircuitBreakerMaxRequests:      3,\n+\n+\t\t// Connection Handoff Configuration\n+\t\tMaxHandoffRetries: 3,\n+\t}\n+}\n+\n+// Validate checks if the configuration is valid.\n+func (c *Config) Validate() error {\n+\tif c.RelaxedTimeout <= 0 {\n+\t\treturn ErrInvalidRelaxedTimeout\n+\t}\n+\tif c.HandoffTimeout <= 0 {\n+\t\treturn ErrInvalidHandoffTimeout\n+\t}\n+\t// Validate worker configuration\n+\t// Allow 0 for auto-calculation, but negative values are invalid\n+\tif c.MaxWorkers < 0 {\n+\t\treturn ErrInvalidHandoffWorkers\n+\t}\n+\t// HandoffQueueSize validation - allow 0 for auto-calculation\n+\tif c.HandoffQueueSize < 0 {\n+\t\treturn ErrInvalidHandoffQueueSize\n+\t}\n+\tif c.PostHandoffRelaxedDuration < 0 {\n+\t\treturn ErrInvalidPostHandoffRelaxedDuration\n+\t}\n+\n+\t// Circuit breaker validation\n+\tif c.CircuitBreakerFailureThreshold < 1 {\n+\t\treturn ErrInvalidCircuitBreakerFailureThreshold\n+\t}\n+\tif c.CircuitBreakerResetTimeout < 0 {\n+\t\treturn ErrInvalidCircuitBreakerResetTimeout\n+\t}\n+\tif c.CircuitBreakerMaxRequests < 1 {\n+\t\treturn ErrInvalidCircuitBreakerMaxRequests\n+\t}\n+\n+\t// Validate Mode (maintenance notifications mode)\n+\tif !c.Mode.IsValid() {\n+\t\treturn ErrInvalidMaintNotifications\n+\t}\n+\n+\t// Validate EndpointType\n+\tif !c.EndpointType.IsValid() {\n+\t\treturn ErrInvalidEndpointType\n+\t}\n+\n+\t// Validate configuration fields\n+\tif c.MaxHandoffRetries < 1 || c.MaxHandoffRetries > 10 {\n+\t\treturn ErrInvalidHandoffRetries\n+\t}\n+\n+\treturn nil\n+}\n+\n+// ApplyDefaults applies default values to any zero-value fields in the configuration.\n+// This ensures that partially configured structs get sensible defaults for missing fields.\n+func (c *Config) ApplyDefaults() *Config {\n+\treturn c.ApplyDefaultsWithPoolSize(0)\n+}\n+\n+// ApplyDefaultsWithPoolSize applies default values to any zero-value fields in the configuration,\n+// using the provided pool size to calculate worker defaults.\n+// This ensures that partially configured structs get sensible defaults for missing fields.\n+func (c *Config) ApplyDefaultsWithPoolSize(poolSize int) *Config {\n+\treturn c.ApplyDefaultsWithPoolConfig(poolSize, 0)\n+}\n+\n+// ApplyDefaultsWithPoolConfig applies default values to any zero-value fields in the configuration,\n+// using the provided pool size and max active connections to calculate worker and queue defaults.\n+// This ensures that partially configured structs get sensible defaults for missing fields.\n+func (c *Config) ApplyDefaultsWithPoolConfig(poolSize int, maxActiveConns int) *Config {\n+\tif c == nil {\n+\t\treturn DefaultConfig().ApplyDefaultsWithPoolSize(poolSize)\n+\t}\n+\n+\tdefaults := DefaultConfig()\n+\tresult := &Config{}\n+\n+\t// Apply defaults for enum fields (empty/zero means not set)\n+\tresult.Mode = defaults.Mode\n+\tif c.Mode != \"\" {\n+\t\tresult.Mode = c.Mode\n+\t}\n+\n+\tresult.EndpointType = defaults.EndpointType\n+\tif c.EndpointType != \"\" {\n+\t\tresult.EndpointType = c.EndpointType\n+\t}\n+\n+\t// Apply defaults for duration fields (zero means not set)\n+\tresult.RelaxedTimeout = defaults.RelaxedTimeout\n+\tif c.RelaxedTimeout > 0 {\n+\t\tresult.RelaxedTimeout = c.RelaxedTimeout\n+\t}\n+\n+\tresult.HandoffTimeout = defaults.HandoffTimeout\n+\tif c.HandoffTimeout > 0 {\n+\t\tresult.HandoffTimeout = c.HandoffTimeout\n+\t}\n+\n+\t// Copy worker configuration\n+\tresult.MaxWorkers = c.MaxWorkers\n+\n+\t// Apply worker defaults based on pool size\n+\tresult.applyWorkerDefaults(poolSize)\n+\n+\t// Apply queue size defaults with new scaling approach\n+\t// Default: max(20x workers, PoolSize), capped by maxActiveConns or 5x pool size\n+\tworkerBasedSize := result.MaxWorkers * 20\n+\tpoolBasedSize := poolSize\n+\tresult.HandoffQueueSize = util.Max(workerBasedSize, poolBasedSize)\n+\tif c.HandoffQueueSize > 0 {\n+\t\t// When explicitly set: enforce minimum of 200\n+\t\tresult.HandoffQueueSize = util.Max(200, c.HandoffQueueSize)\n+\t}\n+\n+\t// Cap queue size: use maxActiveConns+1 if set, otherwise 5x pool size\n+\tvar queueCap int\n+\tif maxActiveConns > 0 {\n+\t\tqueueCap = maxActiveConns + 1\n+\t\t// Ensure queue cap is at least 2 for very small maxActiveConns\n+\t\tif queueCap < 2 {\n+\t\t\tqueueCap = 2\n+\t\t}\n+\t} else {\n+\t\tqueueCap = poolSize * 5\n+\t}\n+\tresult.HandoffQueueSize = util.Min(result.HandoffQueueSize, queueCap)\n+\n+\t// Ensure minimum queue size of 2 (fallback for very small pools)\n+\tif result.HandoffQueueSize < 2 {\n+\t\tresult.HandoffQueueSize = 2\n+\t}\n+\n+\tresult.PostHandoffRelaxedDuration = result.RelaxedTimeout * 2\n+\tif c.PostHandoffRelaxedDuration > 0 {\n+\t\tresult.PostHandoffRelaxedDuration = c.PostHandoffRelaxedDuration\n+\t}\n+\n+\t// Apply defaults for configuration fields\n+\tresult.MaxHandoffRetries = defaults.MaxHandoffRetries\n+\tif c.MaxHandoffRetries > 0 {\n+\t\tresult.MaxHandoffRetries = c.MaxHandoffRetries\n+\t}\n+\n+\t// Circuit breaker configuration\n+\tresult.CircuitBreakerFailureThreshold = defaults.CircuitBreakerFailureThreshold\n+\tif c.CircuitBreakerFailureThreshold > 0 {\n+\t\tresult.CircuitBreakerFailureThreshold = c.CircuitBreakerFailureThreshold\n+\t}\n+\n+\tresult.CircuitBreakerResetTimeout = defaults.CircuitBreakerResetTimeout\n+\tif c.CircuitBreakerResetTimeout > 0 {\n+\t\tresult.CircuitBreakerResetTimeout = c.CircuitBreakerResetTimeout\n+\t}\n+\n+\tresult.CircuitBreakerMaxRequests = defaults.CircuitBreakerMaxRequests\n+\tif c.CircuitBreakerMaxRequests > 0 {\n+\t\tresult.CircuitBreakerMaxRequests = c.CircuitBreakerMaxRequests\n+\t}\n+\n+\tif internal.LogLevel.DebugOrAbove() {\n+\t\tinternal.Logger.Printf(context.Background(), logs.DebugLoggingEnabled())\n+\t\tinternal.Logger.Printf(context.Background(), logs.ConfigDebug(result))\n+\t}\n+\treturn result\n+}\n+\n+// Clone creates a deep copy of the configuration.\n+func (c *Config) Clone() *Config {\n+\tif c == nil {\n+\t\treturn DefaultConfig()\n+\t}\n+\n+\treturn &Config{\n+\t\tMode:                       c.Mode,\n+\t\tEndpointType:               c.EndpointType,\n+\t\tRelaxedTimeout:             c.RelaxedTimeout,\n+\t\tHandoffTimeout:             c.HandoffTimeout,\n+\t\tMaxWorkers:                 c.MaxWorkers,\n+\t\tHandoffQueueSize:           c.HandoffQueueSize,\n+\t\tPostHandoffRelaxedDuration: c.PostHandoffRelaxedDuration,\n+\n+\t\t// Circuit breaker configuration\n+\t\tCircuitBreakerFailureThreshold: c.CircuitBreakerFailureThreshold,\n+\t\tCircuitBreakerResetTimeout:     c.CircuitBreakerResetTimeout,\n+\t\tCircuitBreakerMaxRequests:      c.CircuitBreakerMaxRequests,\n+\n+\t\t// Configuration fields\n+\t\tMaxHandoffRetries: c.MaxHandoffRetries,\n+\t}\n+}\n+\n+// applyWorkerDefaults calculates and applies worker defaults based on pool size\n+func (c *Config) applyWorkerDefaults(poolSize int) {\n+\t// Calculate defaults based on pool size\n+\tif poolSize <= 0 {\n+\t\tpoolSize = 10 * runtime.GOMAXPROCS(0)\n+\t}\n+\n+\t// When not set: min(poolSize/2, max(10, poolSize/3)) - balanced scaling approach\n+\toriginalMaxWorkers := c.MaxWorkers\n+\tc.MaxWorkers = util.Min(poolSize/2, util.Max(10, poolSize/3))\n+\tif originalMaxWorkers != 0 {\n+\t\t// When explicitly set: max(poolSize/2, set_value) - ensure at least poolSize/2 workers\n+\t\tc.MaxWorkers = util.Max(poolSize/2, originalMaxWorkers)\n+\t}\n+\n+\t// Ensure minimum of 1 worker (fallback for very small pools)\n+\tif c.MaxWorkers < 1 {\n+\t\tc.MaxWorkers = 1\n+\t}\n+}\n+\n+// DetectEndpointType automatically detects the appropriate endpoint type\n+// based on the connection address and TLS configuration.\n+//\n+// For IP addresses:\n+//   - If TLS is enabled: requests FQDN for proper certificate validation\n+//   - If TLS is disabled: requests IP for better performance\n+//\n+// For hostnames:\n+//   - If TLS is enabled: always requests FQDN for proper certificate validation\n+//   - If TLS is disabled: requests IP for better performance\n+//\n+// Internal vs External detection:\n+//   - For IPs: uses private IP range detection\n+//   - For hostnames: uses heuristics based on common internal naming patterns\n+func DetectEndpointType(addr string, tlsEnabled bool) EndpointType {\n+\t// Extract host from \"host:port\" format\n+\thost, _, err := net.SplitHostPort(addr)\n+\tif err != nil {\n+\t\thost = addr // Assume no port\n+\t}\n+\n+\t// Check if the host is an IP address or hostname\n+\tip := net.ParseIP(host)\n+\tisIPAddress := ip != nil\n+\tvar endpointType EndpointType\n+\n+\tif isIPAddress {\n+\t\t// Address is an IP - determine if it's private or public\n+\t\tisPrivate := ip.IsPrivate() || ip.IsLoopback() || ip.IsLinkLocalUnicast()\n+\n+\t\tif tlsEnabled {\n+\t\t\t// TLS with IP addresses - still prefer FQDN for certificate validation\n+\t\t\tif isPrivate {\n+\t\t\t\tendpointType = EndpointTypeInternalFQDN\n+\t\t\t} else {\n+\t\t\t\tendpointType = EndpointTypeExternalFQDN\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// No TLS - can use IP addresses directly\n+\t\t\tif isPrivate {\n+\t\t\t\tendpointType = EndpointTypeInternalIP\n+\t\t\t} else {\n+\t\t\t\tendpointType = EndpointTypeExternalIP\n+\t\t\t}\n+\t\t}\n+\t} else {\n+\t\t// Address is a hostname\n+\t\tisInternalHostname := isInternalHostname(host)\n+\t\tif isInternalHostname {\n+\t\t\tendpointType = EndpointTypeInternalFQDN\n+\t\t} else {\n+\t\t\tendpointType = EndpointTypeExternalFQDN\n+\t\t}\n+\t}\n+\n+\treturn endpointType\n+}\n+\n+// isInternalHostname determines if a hostname appears to be internal/private.\n+// This is a heuristic based on common naming patterns.\n+func isInternalHostname(hostname string) bool {\n+\t// Convert to lowercase for comparison\n+\thostname = strings.ToLower(hostname)\n+\n+\t// Common internal hostname patterns\n+\tinternalPatterns := []string{\n+\t\t\"localhost\",\n+\t\t\".local\",\n+\t\t\".internal\",\n+\t\t\".corp\",\n+\t\t\".lan\",\n+\t\t\".intranet\",\n+\t\t\".private\",\n+\t}\n+\n+\t// Check for exact match or suffix match\n+\tfor _, pattern := range internalPatterns {\n+\t\tif hostname == pattern || strings.HasSuffix(hostname, pattern) {\n+\t\t\treturn true\n+\t\t}\n+\t}\n+\n+\t// Check for RFC 1918 style hostnames (e.g., redis-1, db-server, etc.)\n+\t// If hostname doesn't contain dots, it's likely internal\n+\tif !strings.Contains(hostname, \".\") {\n+\t\treturn true\n+\t}\n+\n+\t// Default to external for fully qualified domain names\n+\treturn false\n+}"
    },
    {
      "sha": "049656bddc22a7c5138d8cf8bf0a00a3f92e494b",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/errors.go",
      "status": "added",
      "additions": 76,
      "deletions": 0,
      "changes": 76,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Ferrors.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Ferrors.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Ferrors.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,76 @@\n+package maintnotifications\n+\n+import (\n+\t\"errors\"\n+\n+\t\"github.com/redis/go-redis/v9/internal/maintnotifications/logs\"\n+)\n+\n+// Configuration errors\n+var (\n+\tErrInvalidRelaxedTimeout             = errors.New(logs.InvalidRelaxedTimeoutError())\n+\tErrInvalidHandoffTimeout             = errors.New(logs.InvalidHandoffTimeoutError())\n+\tErrInvalidHandoffWorkers             = errors.New(logs.InvalidHandoffWorkersError())\n+\tErrInvalidHandoffQueueSize           = errors.New(logs.InvalidHandoffQueueSizeError())\n+\tErrInvalidPostHandoffRelaxedDuration = errors.New(logs.InvalidPostHandoffRelaxedDurationError())\n+\tErrInvalidEndpointType               = errors.New(logs.InvalidEndpointTypeError())\n+\tErrInvalidMaintNotifications         = errors.New(logs.InvalidMaintNotificationsError())\n+\tErrMaxHandoffRetriesReached          = errors.New(logs.MaxHandoffRetriesReachedError())\n+\n+\t// Configuration validation errors\n+\n+\t// ErrInvalidHandoffRetries is returned when the number of handoff retries is invalid\n+\tErrInvalidHandoffRetries = errors.New(logs.InvalidHandoffRetriesError())\n+)\n+\n+// Integration errors\n+var (\n+\t// ErrInvalidClient is returned when the client does not support push notifications\n+\tErrInvalidClient = errors.New(logs.InvalidClientError())\n+)\n+\n+// Handoff errors\n+var (\n+\t// ErrHandoffQueueFull is returned when the handoff queue is full\n+\tErrHandoffQueueFull = errors.New(logs.HandoffQueueFullError())\n+)\n+\n+// Notification errors\n+var (\n+\t// ErrInvalidNotification is returned when a notification is in an invalid format\n+\tErrInvalidNotification = errors.New(logs.InvalidNotificationError())\n+)\n+\n+// connection handoff errors\n+var (\n+\t// ErrConnectionMarkedForHandoff is returned when a connection is marked for handoff\n+\t// and should not be used until the handoff is complete\n+\tErrConnectionMarkedForHandoff = errors.New(logs.ConnectionMarkedForHandoffErrorMessage)\n+\t// ErrConnectionMarkedForHandoffWithState is returned when a connection is marked for handoff\n+\t// and should not be used until the handoff is complete\n+\tErrConnectionMarkedForHandoffWithState = errors.New(logs.ConnectionMarkedForHandoffErrorMessage + \" with state\")\n+\t// ErrConnectionInvalidHandoffState is returned when a connection is in an invalid state for handoff\n+\tErrConnectionInvalidHandoffState = errors.New(logs.ConnectionInvalidHandoffStateErrorMessage)\n+)\n+\n+// shutdown errors\n+var (\n+\t// ErrShutdown is returned when the maintnotifications manager is shutdown\n+\tErrShutdown = errors.New(logs.ShutdownError())\n+)\n+\n+// circuit breaker errors\n+var (\n+\t// ErrCircuitBreakerOpen is returned when the circuit breaker is open\n+\tErrCircuitBreakerOpen = errors.New(logs.CircuitBreakerOpenErrorMessage)\n+)\n+\n+// circuit breaker configuration errors\n+var (\n+\t// ErrInvalidCircuitBreakerFailureThreshold is returned when the circuit breaker failure threshold is invalid\n+\tErrInvalidCircuitBreakerFailureThreshold = errors.New(logs.InvalidCircuitBreakerFailureThresholdError())\n+\t// ErrInvalidCircuitBreakerResetTimeout is returned when the circuit breaker reset timeout is invalid\n+\tErrInvalidCircuitBreakerResetTimeout = errors.New(logs.InvalidCircuitBreakerResetTimeoutError())\n+\t// ErrInvalidCircuitBreakerMaxRequests is returned when the circuit breaker max requests is invalid\n+\tErrInvalidCircuitBreakerMaxRequests = errors.New(logs.InvalidCircuitBreakerMaxRequestsError())\n+)"
    },
    {
      "sha": "3a34655715a8545f241ddfa8a6413537362f90e0",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/example_hooks.go",
      "status": "added",
      "additions": 101,
      "deletions": 0,
      "changes": 101,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fexample_hooks.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fexample_hooks.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fexample_hooks.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,101 @@\n+package maintnotifications\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/maintnotifications/logs\"\n+\t\"github.com/redis/go-redis/v9/internal/pool\"\n+\t\"github.com/redis/go-redis/v9/push\"\n+)\n+\n+// contextKey is a custom type for context keys to avoid collisions\n+type contextKey string\n+\n+const (\n+\tstartTimeKey contextKey = \"maint_notif_start_time\"\n+)\n+\n+// MetricsHook collects metrics about notification processing.\n+type MetricsHook struct {\n+\tNotificationCounts map[string]int64\n+\tProcessingTimes    map[string]time.Duration\n+\tErrorCounts        map[string]int64\n+\tHandoffCounts      int64 // Total handoffs initiated\n+\tHandoffSuccesses   int64 // Successful handoffs\n+\tHandoffFailures    int64 // Failed handoffs\n+}\n+\n+// NewMetricsHook creates a new metrics collection hook.\n+func NewMetricsHook() *MetricsHook {\n+\treturn &MetricsHook{\n+\t\tNotificationCounts: make(map[string]int64),\n+\t\tProcessingTimes:    make(map[string]time.Duration),\n+\t\tErrorCounts:        make(map[string]int64),\n+\t}\n+}\n+\n+// PreHook records the start time for processing metrics.\n+func (mh *MetricsHook) PreHook(ctx context.Context, notificationCtx push.NotificationHandlerContext, notificationType string, notification []interface{}) ([]interface{}, bool) {\n+\tmh.NotificationCounts[notificationType]++\n+\n+\t// Log connection information if available\n+\tif conn, ok := notificationCtx.Conn.(*pool.Conn); ok {\n+\t\tinternal.Logger.Printf(ctx, logs.MetricsHookProcessingNotification(notificationType, conn.GetID()))\n+\t}\n+\n+\t// Store start time in context for duration calculation\n+\tstartTime := time.Now()\n+\t_ = context.WithValue(ctx, startTimeKey, startTime) // Context not used further\n+\n+\treturn notification, true\n+}\n+\n+// PostHook records processing completion and any errors.\n+func (mh *MetricsHook) PostHook(ctx context.Context, notificationCtx push.NotificationHandlerContext, notificationType string, notification []interface{}, result error) {\n+\t// Calculate processing duration\n+\tif startTime, ok := ctx.Value(startTimeKey).(time.Time); ok {\n+\t\tduration := time.Since(startTime)\n+\t\tmh.ProcessingTimes[notificationType] = duration\n+\t}\n+\n+\t// Record errors\n+\tif result != nil {\n+\t\tmh.ErrorCounts[notificationType]++\n+\n+\t\t// Log error details with connection information\n+\t\tif conn, ok := notificationCtx.Conn.(*pool.Conn); ok {\n+\t\t\tinternal.Logger.Printf(ctx, logs.MetricsHookRecordedError(notificationType, conn.GetID(), result))\n+\t\t}\n+\t}\n+}\n+\n+// GetMetrics returns a summary of collected metrics.\n+func (mh *MetricsHook) GetMetrics() map[string]interface{} {\n+\treturn map[string]interface{}{\n+\t\t\"notification_counts\": mh.NotificationCounts,\n+\t\t\"processing_times\":    mh.ProcessingTimes,\n+\t\t\"error_counts\":        mh.ErrorCounts,\n+\t}\n+}\n+\n+// ExampleCircuitBreakerMonitor demonstrates how to monitor circuit breaker status\n+func ExampleCircuitBreakerMonitor(poolHook *PoolHook) {\n+\t// Get circuit breaker statistics\n+\tstats := poolHook.GetCircuitBreakerStats()\n+\n+\tfor _, stat := range stats {\n+\t\tfmt.Printf(\"Circuit Breaker for %s:\\n\", stat.Endpoint)\n+\t\tfmt.Printf(\"  State: %s\\n\", stat.State)\n+\t\tfmt.Printf(\"  Failures: %d\\n\", stat.Failures)\n+\t\tfmt.Printf(\"  Last Failure: %v\\n\", stat.LastFailureTime)\n+\t\tfmt.Printf(\"  Last Success: %v\\n\", stat.LastSuccessTime)\n+\n+\t\t// Alert if circuit breaker is open\n+\t\tif stat.State.String() == \"open\" {\n+\t\t\tfmt.Printf(\"    ALERT: Circuit breaker is OPEN for %s\\n\", stat.Endpoint)\n+\t\t}\n+\t}\n+}"
    },
    {
      "sha": "5b60e39b5933e54c66964d3c065b056e5adc3565",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/handoff_worker.go",
      "status": "added",
      "additions": 512,
      "deletions": 0,
      "changes": 512,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fhandoff_worker.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fhandoff_worker.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fhandoff_worker.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,512 @@\n+package maintnotifications\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"net\"\n+\t\"sync\"\n+\t\"sync/atomic\"\n+\t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/maintnotifications/logs\"\n+\t\"github.com/redis/go-redis/v9/internal/pool\"\n+)\n+\n+// handoffWorkerManager manages background workers and queue for connection handoffs\n+type handoffWorkerManager struct {\n+\t// Event-driven handoff support\n+\thandoffQueue chan HandoffRequest // Queue for handoff requests\n+\tshutdown     chan struct{}       // Shutdown signal\n+\tshutdownOnce sync.Once           // Ensure clean shutdown\n+\tworkerWg     sync.WaitGroup      // Track worker goroutines\n+\n+\t// On-demand worker management\n+\tmaxWorkers     int\n+\tactiveWorkers  atomic.Int32\n+\tworkerTimeout  time.Duration // How long workers wait for work before exiting\n+\tworkersScaling atomic.Bool\n+\n+\t// Simple state tracking\n+\tpending sync.Map // map[uint64]int64 (connID -> seqID)\n+\n+\t// Configuration for the maintenance notifications\n+\tconfig *Config\n+\n+\t// Pool hook reference for handoff processing\n+\tpoolHook *PoolHook\n+\n+\t// Circuit breaker manager for endpoint failure handling\n+\tcircuitBreakerManager *CircuitBreakerManager\n+}\n+\n+// newHandoffWorkerManager creates a new handoff worker manager\n+func newHandoffWorkerManager(config *Config, poolHook *PoolHook) *handoffWorkerManager {\n+\treturn &handoffWorkerManager{\n+\t\thandoffQueue:          make(chan HandoffRequest, config.HandoffQueueSize),\n+\t\tshutdown:              make(chan struct{}),\n+\t\tmaxWorkers:            config.MaxWorkers,\n+\t\tactiveWorkers:         atomic.Int32{},   // Start with no workers - create on demand\n+\t\tworkerTimeout:         15 * time.Second, // Workers exit after 15s of inactivity\n+\t\tconfig:                config,\n+\t\tpoolHook:              poolHook,\n+\t\tcircuitBreakerManager: newCircuitBreakerManager(config),\n+\t}\n+}\n+\n+// getCurrentWorkers returns the current number of active workers (for testing)\n+func (hwm *handoffWorkerManager) getCurrentWorkers() int {\n+\treturn int(hwm.activeWorkers.Load())\n+}\n+\n+// getPendingMap returns the pending map for testing purposes\n+func (hwm *handoffWorkerManager) getPendingMap() *sync.Map {\n+\treturn &hwm.pending\n+}\n+\n+// getMaxWorkers returns the max workers for testing purposes\n+func (hwm *handoffWorkerManager) getMaxWorkers() int {\n+\treturn hwm.maxWorkers\n+}\n+\n+// getHandoffQueue returns the handoff queue for testing purposes\n+func (hwm *handoffWorkerManager) getHandoffQueue() chan HandoffRequest {\n+\treturn hwm.handoffQueue\n+}\n+\n+// getCircuitBreakerStats returns circuit breaker statistics for monitoring\n+func (hwm *handoffWorkerManager) getCircuitBreakerStats() []CircuitBreakerStats {\n+\treturn hwm.circuitBreakerManager.GetAllStats()\n+}\n+\n+// resetCircuitBreakers resets all circuit breakers (useful for testing)\n+func (hwm *handoffWorkerManager) resetCircuitBreakers() {\n+\thwm.circuitBreakerManager.Reset()\n+}\n+\n+// isHandoffPending returns true if the given connection has a pending handoff\n+func (hwm *handoffWorkerManager) isHandoffPending(conn *pool.Conn) bool {\n+\t_, pending := hwm.pending.Load(conn.GetID())\n+\treturn pending\n+}\n+\n+// ensureWorkerAvailable ensures at least one worker is available to process requests\n+// Creates a new worker if needed and under the max limit\n+func (hwm *handoffWorkerManager) ensureWorkerAvailable() {\n+\tselect {\n+\tcase <-hwm.shutdown:\n+\t\treturn\n+\tdefault:\n+\t\tif hwm.workersScaling.CompareAndSwap(false, true) {\n+\t\t\tdefer hwm.workersScaling.Store(false)\n+\t\t\t// Check if we need a new worker\n+\t\t\tcurrentWorkers := hwm.activeWorkers.Load()\n+\t\t\tworkersWas := currentWorkers\n+\t\t\tfor currentWorkers < int32(hwm.maxWorkers) {\n+\t\t\t\thwm.workerWg.Add(1)\n+\t\t\t\tgo hwm.onDemandWorker()\n+\t\t\t\tcurrentWorkers++\n+\t\t\t}\n+\t\t\t// workersWas is always <= currentWorkers\n+\t\t\t// currentWorkers will be maxWorkers, but if we have a worker that was closed\n+\t\t\t// while we were creating new workers, just add the difference between\n+\t\t\t// the currentWorkers and the number of workers we observed initially (i.e. the number of workers we created)\n+\t\t\thwm.activeWorkers.Add(currentWorkers - workersWas)\n+\t\t}\n+\t}\n+}\n+\n+// onDemandWorker processes handoff requests and exits when idle\n+func (hwm *handoffWorkerManager) onDemandWorker() {\n+\tdefer func() {\n+\t\t// Handle panics to ensure proper cleanup\n+\t\tif r := recover(); r != nil {\n+\t\t\tinternal.Logger.Printf(context.Background(), logs.WorkerPanicRecovered(r))\n+\t\t}\n+\n+\t\t// Decrement active worker count when exiting\n+\t\thwm.activeWorkers.Add(-1)\n+\t\thwm.workerWg.Done()\n+\t}()\n+\n+\t// Create reusable timer to prevent timer leaks\n+\ttimer := time.NewTimer(hwm.workerTimeout)\n+\tdefer timer.Stop()\n+\n+\tfor {\n+\t\t// Reset timer for next iteration\n+\t\tif !timer.Stop() {\n+\t\t\tselect {\n+\t\t\tcase <-timer.C:\n+\t\t\tdefault:\n+\t\t\t}\n+\t\t}\n+\t\ttimer.Reset(hwm.workerTimeout)\n+\n+\t\tselect {\n+\t\tcase <-hwm.shutdown:\n+\t\t\tif internal.LogLevel.InfoOrAbove() {\n+\t\t\t\tinternal.Logger.Printf(context.Background(), logs.WorkerExitingDueToShutdown())\n+\t\t\t}\n+\t\t\treturn\n+\t\tcase <-timer.C:\n+\t\t\t// Worker has been idle for too long, exit to save resources\n+\t\t\tif internal.LogLevel.InfoOrAbove() {\n+\t\t\t\tinternal.Logger.Printf(context.Background(), logs.WorkerExitingDueToInactivityTimeout(hwm.workerTimeout))\n+\t\t\t}\n+\t\t\treturn\n+\t\tcase request := <-hwm.handoffQueue:\n+\t\t\t// Check for shutdown before processing\n+\t\t\tselect {\n+\t\t\tcase <-hwm.shutdown:\n+\t\t\t\tif internal.LogLevel.InfoOrAbove() {\n+\t\t\t\t\tinternal.Logger.Printf(context.Background(), logs.WorkerExitingDueToShutdownWhileProcessing())\n+\t\t\t\t}\n+\t\t\t\t// Clean up the request before exiting\n+\t\t\t\thwm.pending.Delete(request.ConnID)\n+\t\t\t\treturn\n+\t\t\tdefault:\n+\t\t\t\t// Process the request\n+\t\t\t\thwm.processHandoffRequest(request)\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+// processHandoffRequest processes a single handoff request\n+func (hwm *handoffWorkerManager) processHandoffRequest(request HandoffRequest) {\n+\tif internal.LogLevel.InfoOrAbove() {\n+\t\tinternal.Logger.Printf(context.Background(), logs.HandoffStarted(request.Conn.GetID(), request.Endpoint))\n+\t}\n+\n+\t// Create a context with handoff timeout from config\n+\thandoffTimeout := 15 * time.Second // Default timeout\n+\tif hwm.config != nil && hwm.config.HandoffTimeout > 0 {\n+\t\thandoffTimeout = hwm.config.HandoffTimeout\n+\t}\n+\tctx, cancel := context.WithTimeout(context.Background(), handoffTimeout)\n+\tdefer cancel()\n+\n+\t// Create a context that also respects the shutdown signal\n+\tshutdownCtx, shutdownCancel := context.WithCancel(ctx)\n+\tdefer shutdownCancel()\n+\n+\t// Monitor shutdown signal in a separate goroutine\n+\tgo func() {\n+\t\tselect {\n+\t\tcase <-hwm.shutdown:\n+\t\t\tshutdownCancel()\n+\t\tcase <-shutdownCtx.Done():\n+\t\t}\n+\t}()\n+\n+\t// Perform the handoff with cancellable context\n+\tshouldRetry, err := hwm.performConnectionHandoff(shutdownCtx, request.Conn)\n+\tminRetryBackoff := 500 * time.Millisecond\n+\tif err != nil {\n+\t\tif shouldRetry {\n+\t\t\tnow := time.Now()\n+\t\t\tdeadline, ok := shutdownCtx.Deadline()\n+\t\t\tthirdOfTimeout := handoffTimeout / 3\n+\t\t\tif !ok || deadline.Before(now) {\n+\t\t\t\t// wait half the timeout before retrying if no deadline or deadline has passed\n+\t\t\t\tdeadline = now.Add(thirdOfTimeout)\n+\t\t\t}\n+\t\t\tafterTime := deadline.Sub(now)\n+\t\t\tif afterTime < minRetryBackoff {\n+\t\t\t\tafterTime = minRetryBackoff\n+\t\t\t}\n+\n+\t\t\tif internal.LogLevel.InfoOrAbove() {\n+\t\t\t\t// Get current retry count for better logging\n+\t\t\t\tcurrentRetries := request.Conn.HandoffRetries()\n+\t\t\t\tmaxRetries := 3 // Default fallback\n+\t\t\t\tif hwm.config != nil {\n+\t\t\t\t\tmaxRetries = hwm.config.MaxHandoffRetries\n+\t\t\t\t}\n+\t\t\t\tinternal.Logger.Printf(context.Background(), logs.HandoffFailed(request.ConnID, request.Endpoint, currentRetries, maxRetries, err))\n+\t\t\t}\n+\t\t\t// Schedule retry - keep connection in pending map until retry is queued\n+\t\t\ttime.AfterFunc(afterTime, func() {\n+\t\t\t\tif err := hwm.queueHandoff(request.Conn); err != nil {\n+\t\t\t\t\tif internal.LogLevel.WarnOrAbove() {\n+\t\t\t\t\t\tinternal.Logger.Printf(context.Background(), logs.CannotQueueHandoffForRetry(err))\n+\t\t\t\t\t}\n+\t\t\t\t\t// Failed to queue retry - remove from pending and close connection\n+\t\t\t\t\thwm.pending.Delete(request.Conn.GetID())\n+\t\t\t\t\thwm.closeConnFromRequest(context.Background(), request, err)\n+\t\t\t\t} else {\n+\t\t\t\t\t// Successfully queued retry - remove from pending (will be re-added by queueHandoff)\n+\t\t\t\t\thwm.pending.Delete(request.Conn.GetID())\n+\t\t\t\t}\n+\t\t\t})\n+\t\t\treturn\n+\t\t} else {\n+\t\t\t// Won't retry - remove from pending and close connection\n+\t\t\thwm.pending.Delete(request.Conn.GetID())\n+\t\t\tgo hwm.closeConnFromRequest(ctx, request, err)\n+\t\t}\n+\n+\t\t// Clear handoff state if not returned for retry\n+\t\tseqID := request.Conn.GetMovingSeqID()\n+\t\tconnID := request.Conn.GetID()\n+\t\tif hwm.poolHook.operationsManager != nil {\n+\t\t\thwm.poolHook.operationsManager.UntrackOperationWithConnID(seqID, connID)\n+\t\t}\n+\t} else {\n+\t\t// Success - remove from pending map\n+\t\thwm.pending.Delete(request.Conn.GetID())\n+\t}\n+}\n+\n+// queueHandoff queues a handoff request for processing\n+// if err is returned, connection will be removed from pool\n+func (hwm *handoffWorkerManager) queueHandoff(conn *pool.Conn) error {\n+\t// Get handoff info atomically to prevent race conditions\n+\tshouldHandoff, endpoint, seqID := conn.GetHandoffInfo()\n+\n+\t// on retries the connection will not be marked for handoff, but it will have retries > 0\n+\t// if shouldHandoff is false and retries is 0, then we are not retrying and not do a handoff\n+\tif !shouldHandoff && conn.HandoffRetries() == 0 {\n+\t\tif internal.LogLevel.InfoOrAbove() {\n+\t\t\tinternal.Logger.Printf(context.Background(), logs.ConnectionNotMarkedForHandoff(conn.GetID()))\n+\t\t}\n+\t\treturn errors.New(logs.ConnectionNotMarkedForHandoffError(conn.GetID()))\n+\t}\n+\n+\t// Create handoff request with atomically retrieved data\n+\trequest := HandoffRequest{\n+\t\tConn:     conn,\n+\t\tConnID:   conn.GetID(),\n+\t\tEndpoint: endpoint,\n+\t\tSeqID:    seqID,\n+\t\tPool:     hwm.poolHook.pool, // Include pool for connection removal on failure\n+\t}\n+\n+\tselect {\n+\t// priority to shutdown\n+\tcase <-hwm.shutdown:\n+\t\treturn ErrShutdown\n+\tdefault:\n+\t\tselect {\n+\t\tcase <-hwm.shutdown:\n+\t\t\treturn ErrShutdown\n+\t\tcase hwm.handoffQueue <- request:\n+\t\t\t// Store in pending map\n+\t\t\thwm.pending.Store(request.ConnID, request.SeqID)\n+\t\t\t// Ensure we have a worker to process this request\n+\t\t\thwm.ensureWorkerAvailable()\n+\t\t\treturn nil\n+\t\tdefault:\n+\t\t\tselect {\n+\t\t\tcase <-hwm.shutdown:\n+\t\t\t\treturn ErrShutdown\n+\t\t\tcase hwm.handoffQueue <- request:\n+\t\t\t\t// Store in pending map\n+\t\t\t\thwm.pending.Store(request.ConnID, request.SeqID)\n+\t\t\t\t// Ensure we have a worker to process this request\n+\t\t\t\thwm.ensureWorkerAvailable()\n+\t\t\t\treturn nil\n+\t\t\tcase <-time.After(100 * time.Millisecond): // give workers a chance to process\n+\t\t\t\t// Queue is full - log and attempt scaling\n+\t\t\t\tqueueLen := len(hwm.handoffQueue)\n+\t\t\t\tqueueCap := cap(hwm.handoffQueue)\n+\t\t\t\tif internal.LogLevel.WarnOrAbove() {\n+\t\t\t\t\tinternal.Logger.Printf(context.Background(), logs.HandoffQueueFull(queueLen, queueCap))\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Ensure we have workers available to handle the load\n+\thwm.ensureWorkerAvailable()\n+\treturn ErrHandoffQueueFull\n+}\n+\n+// shutdownWorkers gracefully shuts down the worker manager, waiting for workers to complete\n+func (hwm *handoffWorkerManager) shutdownWorkers(ctx context.Context) error {\n+\thwm.shutdownOnce.Do(func() {\n+\t\tclose(hwm.shutdown)\n+\t\t// workers will exit when they finish their current request\n+\n+\t\t// Shutdown circuit breaker manager cleanup goroutine\n+\t\tif hwm.circuitBreakerManager != nil {\n+\t\t\thwm.circuitBreakerManager.Shutdown()\n+\t\t}\n+\t})\n+\n+\t// Wait for workers to complete\n+\tdone := make(chan struct{})\n+\tgo func() {\n+\t\thwm.workerWg.Wait()\n+\t\tclose(done)\n+\t}()\n+\n+\tselect {\n+\tcase <-done:\n+\t\treturn nil\n+\tcase <-ctx.Done():\n+\t\treturn ctx.Err()\n+\t}\n+}\n+\n+// performConnectionHandoff performs the actual connection handoff\n+// When error is returned, the connection handoff should be retried if err is not ErrMaxHandoffRetriesReached\n+func (hwm *handoffWorkerManager) performConnectionHandoff(ctx context.Context, conn *pool.Conn) (shouldRetry bool, err error) {\n+\t// Clear handoff state after successful handoff\n+\tconnID := conn.GetID()\n+\n+\tnewEndpoint := conn.GetHandoffEndpoint()\n+\tif newEndpoint == \"\" {\n+\t\treturn false, ErrConnectionInvalidHandoffState\n+\t}\n+\n+\t// Use circuit breaker to protect against failing endpoints\n+\tcircuitBreaker := hwm.circuitBreakerManager.GetCircuitBreaker(newEndpoint)\n+\n+\t// Check if circuit breaker is open before attempting handoff\n+\tif circuitBreaker.IsOpen() {\n+\t\tinternal.Logger.Printf(ctx, logs.CircuitBreakerOpen(connID, newEndpoint))\n+\t\treturn false, ErrCircuitBreakerOpen // Don't retry when circuit breaker is open\n+\t}\n+\n+\t// Perform the handoff\n+\tshouldRetry, err = hwm.performHandoffInternal(ctx, conn, newEndpoint, connID)\n+\n+\t// Update circuit breaker based on result\n+\tif err != nil {\n+\t\t// Only track dial/network errors in circuit breaker, not initialization errors\n+\t\tif shouldRetry {\n+\t\t\tcircuitBreaker.recordFailure()\n+\t\t}\n+\t\treturn shouldRetry, err\n+\t}\n+\n+\t// Success - record in circuit breaker\n+\tcircuitBreaker.recordSuccess()\n+\treturn false, nil\n+}\n+\n+// performHandoffInternal performs the actual handoff logic (extracted for circuit breaker integration)\n+func (hwm *handoffWorkerManager) performHandoffInternal(\n+\tctx context.Context,\n+\tconn *pool.Conn,\n+\tnewEndpoint string,\n+\tconnID uint64,\n+) (shouldRetry bool, err error) {\n+\tretries := conn.IncrementAndGetHandoffRetries(1)\n+\tinternal.Logger.Printf(ctx, logs.HandoffRetryAttempt(connID, retries, newEndpoint, conn.RemoteAddr().String()))\n+\tmaxRetries := 3 // Default fallback\n+\tif hwm.config != nil {\n+\t\tmaxRetries = hwm.config.MaxHandoffRetries\n+\t}\n+\n+\tif retries > maxRetries {\n+\t\tif internal.LogLevel.WarnOrAbove() {\n+\t\t\tinternal.Logger.Printf(ctx, logs.ReachedMaxHandoffRetries(connID, newEndpoint, maxRetries))\n+\t\t}\n+\t\t// won't retry on ErrMaxHandoffRetriesReached\n+\t\treturn false, ErrMaxHandoffRetriesReached\n+\t}\n+\n+\t// Create endpoint-specific dialer\n+\tendpointDialer := hwm.createEndpointDialer(newEndpoint)\n+\n+\t// Create new connection to the new endpoint\n+\tnewNetConn, err := endpointDialer(ctx)\n+\tif err != nil {\n+\t\tinternal.Logger.Printf(ctx, logs.FailedToDialNewEndpoint(connID, newEndpoint, err))\n+\t\t// will retry\n+\t\t// Maybe a network error - retry after a delay\n+\t\treturn true, err\n+\t}\n+\n+\t// Get the old connection\n+\toldConn := conn.GetNetConn()\n+\n+\t// Apply relaxed timeout to the new connection for the configured post-handoff duration\n+\t// This gives the new connection more time to handle operations during cluster transition\n+\t// Setting this here (before initing the connection) ensures that the connection is going\n+\t// to use the relaxed timeout for the first operation (auth/ACL select)\n+\tif hwm.config != nil && hwm.config.PostHandoffRelaxedDuration > 0 {\n+\t\trelaxedTimeout := hwm.config.RelaxedTimeout\n+\t\t// Set relaxed timeout with deadline - no background goroutine needed\n+\t\tdeadline := time.Now().Add(hwm.config.PostHandoffRelaxedDuration)\n+\t\tconn.SetRelaxedTimeoutWithDeadline(relaxedTimeout, relaxedTimeout, deadline)\n+\n+\t\tif internal.LogLevel.InfoOrAbove() {\n+\t\t\tinternal.Logger.Printf(context.Background(), logs.ApplyingRelaxedTimeoutDueToPostHandoff(connID, relaxedTimeout, deadline.Format(\"15:04:05.000\")))\n+\t\t}\n+\t}\n+\n+\t// Replace the connection and execute initialization\n+\terr = conn.SetNetConnAndInitConn(ctx, newNetConn)\n+\tif err != nil {\n+\t\t// won't retry\n+\t\t// Initialization failed - remove the connection\n+\t\treturn false, err\n+\t}\n+\tdefer func() {\n+\t\tif oldConn != nil {\n+\t\t\toldConn.Close()\n+\t\t}\n+\t}()\n+\n+\t// Clear handoff state will:\n+\t// - set the connection as usable again\n+\t// - clear the handoff state (shouldHandoff, endpoint, seqID)\n+\t// - reset the handoff retries to 0\n+\t// Note: Theoretically there may be a short window where the connection is in the pool\n+\t// and IDLE (initConn completed) but still has handoff state set.\n+\tconn.ClearHandoffState()\n+\tinternal.Logger.Printf(ctx, logs.HandoffSucceeded(connID, newEndpoint))\n+\n+\t// successfully completed the handoff, no retry needed and no error\n+\treturn false, nil\n+}\n+\n+// createEndpointDialer creates a dialer function that connects to a specific endpoint\n+func (hwm *handoffWorkerManager) createEndpointDialer(endpoint string) func(context.Context) (net.Conn, error) {\n+\treturn func(ctx context.Context) (net.Conn, error) {\n+\t\t// Parse endpoint to extract host and port\n+\t\thost, port, err := net.SplitHostPort(endpoint)\n+\t\tif err != nil {\n+\t\t\t// If no port specified, assume default Redis port\n+\t\t\thost = endpoint\n+\t\t\tif port == \"\" {\n+\t\t\t\tport = \"6379\"\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Use the base dialer to connect to the new endpoint\n+\t\treturn hwm.poolHook.baseDialer(ctx, hwm.poolHook.network, net.JoinHostPort(host, port))\n+\t}\n+}\n+\n+// closeConnFromRequest closes the connection and logs the reason\n+func (hwm *handoffWorkerManager) closeConnFromRequest(ctx context.Context, request HandoffRequest, err error) {\n+\tpooler := request.Pool\n+\tconn := request.Conn\n+\n+\t// Clear handoff state before closing\n+\tconn.ClearHandoffState()\n+\n+\tif pooler != nil {\n+\t\t// Use RemoveWithoutTurn instead of Remove to avoid freeing a turn that we don't have.\n+\t\t// The handoff worker doesn't call Get(), so it doesn't have a turn to free.\n+\t\t// Remove() is meant to be called after Get() and frees a turn.\n+\t\t// RemoveWithoutTurn() removes and closes the connection without affecting the queue.\n+\t\tpooler.RemoveWithoutTurn(ctx, conn, err)\n+\t\tif internal.LogLevel.WarnOrAbove() {\n+\t\t\tinternal.Logger.Printf(ctx, logs.RemovingConnectionFromPool(conn.GetID(), err))\n+\t\t}\n+\t} else {\n+\t\terr := conn.Close() // Close the connection if no pool provided\n+\t\tif err != nil {\n+\t\t\tinternal.Logger.Printf(ctx, \"redis: failed to close connection: %v\", err)\n+\t\t}\n+\t\tif internal.LogLevel.WarnOrAbove() {\n+\t\t\tinternal.Logger.Printf(ctx, logs.NoPoolProvidedCannotRemove(conn.GetID(), err))\n+\t\t}\n+\t}\n+}"
    },
    {
      "sha": "ee3c3819c2a4920dff8a401c026b2e75a699963a",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/hooks.go",
      "status": "added",
      "additions": 60,
      "deletions": 0,
      "changes": 60,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fhooks.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fhooks.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fhooks.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,60 @@\n+package maintnotifications\n+\n+import (\n+\t\"context\"\n+\t\"slices\"\n+\n+\t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/maintnotifications/logs\"\n+\t\"github.com/redis/go-redis/v9/internal/pool\"\n+\t\"github.com/redis/go-redis/v9/push\"\n+)\n+\n+// LoggingHook is an example hook implementation that logs all notifications.\n+type LoggingHook struct {\n+\tLogLevel int // 0=Error, 1=Warn, 2=Info, 3=Debug\n+}\n+\n+// PreHook logs the notification before processing and allows modification.\n+func (lh *LoggingHook) PreHook(ctx context.Context, notificationCtx push.NotificationHandlerContext, notificationType string, notification []interface{}) ([]interface{}, bool) {\n+\tif lh.LogLevel >= 2 { // Info level\n+\t\t// Log the notification type and content\n+\t\tconnID := uint64(0)\n+\t\tif conn, ok := notificationCtx.Conn.(*pool.Conn); ok {\n+\t\t\tconnID = conn.GetID()\n+\t\t}\n+\t\tseqID := int64(0)\n+\t\tif slices.Contains(maintenanceNotificationTypes, notificationType) {\n+\t\t\t// seqID is the second element in the notification array\n+\t\t\tif len(notification) > 1 {\n+\t\t\t\tif parsedSeqID, ok := notification[1].(int64); !ok {\n+\t\t\t\t\tseqID = 0\n+\t\t\t\t} else {\n+\t\t\t\t\tseqID = parsedSeqID\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t}\n+\t\tinternal.Logger.Printf(ctx, logs.ProcessingNotification(connID, seqID, notificationType, notification))\n+\t}\n+\treturn notification, true // Continue processing with unmodified notification\n+}\n+\n+// PostHook logs the result after processing.\n+func (lh *LoggingHook) PostHook(ctx context.Context, notificationCtx push.NotificationHandlerContext, notificationType string, notification []interface{}, result error) {\n+\tconnID := uint64(0)\n+\tif conn, ok := notificationCtx.Conn.(*pool.Conn); ok {\n+\t\tconnID = conn.GetID()\n+\t}\n+\tif result != nil && lh.LogLevel >= 1 { // Warning level\n+\t\tinternal.Logger.Printf(ctx, logs.ProcessingNotificationFailed(connID, notificationType, result, notification))\n+\t} else if lh.LogLevel >= 3 { // Debug level\n+\t\tinternal.Logger.Printf(ctx, logs.ProcessingNotificationSucceeded(connID, notificationType))\n+\t}\n+}\n+\n+// NewLoggingHook creates a new logging hook with the specified log level.\n+// Log levels: 0=Error, 1=Warn, 2=Info, 3=Debug\n+func NewLoggingHook(logLevel int) *LoggingHook {\n+\treturn &LoggingHook{LogLevel: logLevel}\n+}"
    },
    {
      "sha": "775c163e140cb43ab7a8913093d222fd44ff7831",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/manager.go",
      "status": "added",
      "additions": 320,
      "deletions": 0,
      "changes": 320,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fmanager.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fmanager.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fmanager.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,320 @@\n+package maintnotifications\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"net\"\n+\t\"sync\"\n+\t\"sync/atomic\"\n+\t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/interfaces\"\n+\t\"github.com/redis/go-redis/v9/internal/maintnotifications/logs\"\n+\t\"github.com/redis/go-redis/v9/internal/pool\"\n+\t\"github.com/redis/go-redis/v9/push\"\n+)\n+\n+// Push notification type constants for maintenance\n+const (\n+\tNotificationMoving      = \"MOVING\"\n+\tNotificationMigrating   = \"MIGRATING\"\n+\tNotificationMigrated    = \"MIGRATED\"\n+\tNotificationFailingOver = \"FAILING_OVER\"\n+\tNotificationFailedOver  = \"FAILED_OVER\"\n+)\n+\n+// maintenanceNotificationTypes contains all notification types that maintenance handles\n+var maintenanceNotificationTypes = []string{\n+\tNotificationMoving,\n+\tNotificationMigrating,\n+\tNotificationMigrated,\n+\tNotificationFailingOver,\n+\tNotificationFailedOver,\n+}\n+\n+// NotificationHook is called before and after notification processing\n+// PreHook can modify the notification and return false to skip processing\n+// PostHook is called after successful processing\n+type NotificationHook interface {\n+\tPreHook(ctx context.Context, notificationCtx push.NotificationHandlerContext, notificationType string, notification []interface{}) ([]interface{}, bool)\n+\tPostHook(ctx context.Context, notificationCtx push.NotificationHandlerContext, notificationType string, notification []interface{}, result error)\n+}\n+\n+// MovingOperationKey provides a unique key for tracking MOVING operations\n+// that combines sequence ID with connection identifier to handle duplicate\n+// sequence IDs across multiple connections to the same node.\n+type MovingOperationKey struct {\n+\tSeqID  int64  // Sequence ID from MOVING notification\n+\tConnID uint64 // Unique connection identifier\n+}\n+\n+// String returns a string representation of the key for debugging\n+func (k MovingOperationKey) String() string {\n+\treturn fmt.Sprintf(\"seq:%d-conn:%d\", k.SeqID, k.ConnID)\n+}\n+\n+// Manager provides a simplified upgrade functionality with hooks and atomic state.\n+type Manager struct {\n+\tclient  interfaces.ClientInterface\n+\tconfig  *Config\n+\toptions interfaces.OptionsInterface\n+\tpool    pool.Pooler\n+\n+\t// MOVING operation tracking - using sync.Map for better concurrent performance\n+\tactiveMovingOps sync.Map // map[MovingOperationKey]*MovingOperation\n+\n+\t// Atomic state tracking - no locks needed for state queries\n+\tactiveOperationCount atomic.Int64 // Number of active operations\n+\tclosed               atomic.Bool  // Manager closed state\n+\n+\t// Notification hooks for extensibility\n+\thooks        []NotificationHook\n+\thooksMu      sync.RWMutex // Protects hooks slice\n+\tpoolHooksRef *PoolHook\n+}\n+\n+// MovingOperation tracks an active MOVING operation.\n+type MovingOperation struct {\n+\tSeqID       int64\n+\tNewEndpoint string\n+\tStartTime   time.Time\n+\tDeadline    time.Time\n+}\n+\n+// NewManager creates a new simplified manager.\n+func NewManager(client interfaces.ClientInterface, pool pool.Pooler, config *Config) (*Manager, error) {\n+\tif client == nil {\n+\t\treturn nil, ErrInvalidClient\n+\t}\n+\n+\thm := &Manager{\n+\t\tclient:  client,\n+\t\tpool:    pool,\n+\t\toptions: client.GetOptions(),\n+\t\tconfig:  config.Clone(),\n+\t\thooks:   make([]NotificationHook, 0),\n+\t}\n+\n+\t// Set up push notification handling\n+\tif err := hm.setupPushNotifications(); err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\treturn hm, nil\n+}\n+\n+// GetPoolHook creates a pool hook with a custom dialer.\n+func (hm *Manager) InitPoolHook(baseDialer func(context.Context, string, string) (net.Conn, error)) {\n+\tpoolHook := hm.createPoolHook(baseDialer)\n+\thm.pool.AddPoolHook(poolHook)\n+}\n+\n+// setupPushNotifications sets up push notification handling by registering with the client's processor.\n+func (hm *Manager) setupPushNotifications() error {\n+\tprocessor := hm.client.GetPushProcessor()\n+\tif processor == nil {\n+\t\treturn ErrInvalidClient // Client doesn't support push notifications\n+\t}\n+\n+\t// Create our notification handler\n+\thandler := &NotificationHandler{manager: hm, operationsManager: hm}\n+\n+\t// Register handlers for all upgrade notifications with the client's processor\n+\tfor _, notificationType := range maintenanceNotificationTypes {\n+\t\tif err := processor.RegisterHandler(notificationType, handler, true); err != nil {\n+\t\t\treturn errors.New(logs.FailedToRegisterHandler(notificationType, err))\n+\t\t}\n+\t}\n+\n+\treturn nil\n+}\n+\n+// TrackMovingOperationWithConnID starts a new MOVING operation with a specific connection ID.\n+func (hm *Manager) TrackMovingOperationWithConnID(ctx context.Context, newEndpoint string, deadline time.Time, seqID int64, connID uint64) error {\n+\t// Create composite key\n+\tkey := MovingOperationKey{\n+\t\tSeqID:  seqID,\n+\t\tConnID: connID,\n+\t}\n+\n+\t// Create MOVING operation record\n+\tmovingOp := &MovingOperation{\n+\t\tSeqID:       seqID,\n+\t\tNewEndpoint: newEndpoint,\n+\t\tStartTime:   time.Now(),\n+\t\tDeadline:    deadline,\n+\t}\n+\n+\t// Use LoadOrStore for atomic check-and-set operation\n+\tif _, loaded := hm.activeMovingOps.LoadOrStore(key, movingOp); loaded {\n+\t\t// Duplicate MOVING notification, ignore\n+\t\tif internal.LogLevel.DebugOrAbove() { // Debug level\n+\t\t\tinternal.Logger.Printf(context.Background(), logs.DuplicateMovingOperation(connID, newEndpoint, seqID))\n+\t\t}\n+\t\treturn nil\n+\t}\n+\tif internal.LogLevel.DebugOrAbove() { // Debug level\n+\t\tinternal.Logger.Printf(context.Background(), logs.TrackingMovingOperation(connID, newEndpoint, seqID))\n+\t}\n+\n+\t// Increment active operation count atomically\n+\thm.activeOperationCount.Add(1)\n+\n+\treturn nil\n+}\n+\n+// UntrackOperationWithConnID completes a MOVING operation with a specific connection ID.\n+func (hm *Manager) UntrackOperationWithConnID(seqID int64, connID uint64) {\n+\t// Create composite key\n+\tkey := MovingOperationKey{\n+\t\tSeqID:  seqID,\n+\t\tConnID: connID,\n+\t}\n+\n+\t// Remove from active operations atomically\n+\tif _, loaded := hm.activeMovingOps.LoadAndDelete(key); loaded {\n+\t\tif internal.LogLevel.DebugOrAbove() { // Debug level\n+\t\t\tinternal.Logger.Printf(context.Background(), logs.UntrackingMovingOperation(connID, seqID))\n+\t\t}\n+\t\t// Decrement active operation count only if operation existed\n+\t\thm.activeOperationCount.Add(-1)\n+\t} else {\n+\t\tif internal.LogLevel.DebugOrAbove() { // Debug level\n+\t\t\tinternal.Logger.Printf(context.Background(), logs.OperationNotTracked(connID, seqID))\n+\t\t}\n+\t}\n+}\n+\n+// GetActiveMovingOperations returns active operations with composite keys.\n+// WARNING: This method creates a new map and copies all operations on every call.\n+// Use sparingly, especially in hot paths or high-frequency logging.\n+func (hm *Manager) GetActiveMovingOperations() map[MovingOperationKey]*MovingOperation {\n+\tresult := make(map[MovingOperationKey]*MovingOperation)\n+\n+\t// Iterate over sync.Map to build result\n+\thm.activeMovingOps.Range(func(key, value interface{}) bool {\n+\t\tk := key.(MovingOperationKey)\n+\t\top := value.(*MovingOperation)\n+\n+\t\t// Create a copy to avoid sharing references\n+\t\tresult[k] = &MovingOperation{\n+\t\t\tSeqID:       op.SeqID,\n+\t\t\tNewEndpoint: op.NewEndpoint,\n+\t\t\tStartTime:   op.StartTime,\n+\t\t\tDeadline:    op.Deadline,\n+\t\t}\n+\t\treturn true // Continue iteration\n+\t})\n+\n+\treturn result\n+}\n+\n+// IsHandoffInProgress returns true if any handoff is in progress.\n+// Uses atomic counter for lock-free operation.\n+func (hm *Manager) IsHandoffInProgress() bool {\n+\treturn hm.activeOperationCount.Load() > 0\n+}\n+\n+// GetActiveOperationCount returns the number of active operations.\n+// Uses atomic counter for lock-free operation.\n+func (hm *Manager) GetActiveOperationCount() int64 {\n+\treturn hm.activeOperationCount.Load()\n+}\n+\n+// Close closes the manager.\n+func (hm *Manager) Close() error {\n+\t// Use atomic operation for thread-safe close check\n+\tif !hm.closed.CompareAndSwap(false, true) {\n+\t\treturn nil // Already closed\n+\t}\n+\n+\t// Shutdown the pool hook if it exists\n+\tif hm.poolHooksRef != nil {\n+\t\t// Use a timeout to prevent hanging indefinitely\n+\t\tshutdownCtx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\tdefer cancel()\n+\n+\t\terr := hm.poolHooksRef.Shutdown(shutdownCtx)\n+\t\tif err != nil {\n+\t\t\t// was not able to close pool hook, keep closed state false\n+\t\t\thm.closed.Store(false)\n+\t\t\treturn err\n+\t\t}\n+\t\t// Remove the pool hook from the pool\n+\t\tif hm.pool != nil {\n+\t\t\thm.pool.RemovePoolHook(hm.poolHooksRef)\n+\t\t}\n+\t}\n+\n+\t// Clear all active operations\n+\thm.activeMovingOps.Range(func(key, value interface{}) bool {\n+\t\thm.activeMovingOps.Delete(key)\n+\t\treturn true\n+\t})\n+\n+\t// Reset counter\n+\thm.activeOperationCount.Store(0)\n+\n+\treturn nil\n+}\n+\n+// GetState returns current state using atomic counter for lock-free operation.\n+func (hm *Manager) GetState() State {\n+\tif hm.activeOperationCount.Load() > 0 {\n+\t\treturn StateMoving\n+\t}\n+\treturn StateIdle\n+}\n+\n+// processPreHooks calls all pre-hooks and returns the modified notification and whether to continue processing.\n+func (hm *Manager) processPreHooks(ctx context.Context, notificationCtx push.NotificationHandlerContext, notificationType string, notification []interface{}) ([]interface{}, bool) {\n+\thm.hooksMu.RLock()\n+\tdefer hm.hooksMu.RUnlock()\n+\n+\tcurrentNotification := notification\n+\n+\tfor _, hook := range hm.hooks {\n+\t\tmodifiedNotification, shouldContinue := hook.PreHook(ctx, notificationCtx, notificationType, currentNotification)\n+\t\tif !shouldContinue {\n+\t\t\treturn modifiedNotification, false\n+\t\t}\n+\t\tcurrentNotification = modifiedNotification\n+\t}\n+\n+\treturn currentNotification, true\n+}\n+\n+// processPostHooks calls all post-hooks with the processing result.\n+func (hm *Manager) processPostHooks(ctx context.Context, notificationCtx push.NotificationHandlerContext, notificationType string, notification []interface{}, result error) {\n+\thm.hooksMu.RLock()\n+\tdefer hm.hooksMu.RUnlock()\n+\n+\tfor _, hook := range hm.hooks {\n+\t\thook.PostHook(ctx, notificationCtx, notificationType, notification, result)\n+\t}\n+}\n+\n+// createPoolHook creates a pool hook with this manager already set.\n+func (hm *Manager) createPoolHook(baseDialer func(context.Context, string, string) (net.Conn, error)) *PoolHook {\n+\tif hm.poolHooksRef != nil {\n+\t\treturn hm.poolHooksRef\n+\t}\n+\t// Get pool size from client options for better worker defaults\n+\tpoolSize := 0\n+\tif hm.options != nil {\n+\t\tpoolSize = hm.options.GetPoolSize()\n+\t}\n+\n+\thm.poolHooksRef = NewPoolHookWithPoolSize(baseDialer, hm.options.GetNetwork(), hm.config, hm, poolSize)\n+\thm.poolHooksRef.SetPool(hm.pool)\n+\n+\treturn hm.poolHooksRef\n+}\n+\n+func (hm *Manager) AddNotificationHook(notificationHook NotificationHook) {\n+\thm.hooksMu.Lock()\n+\tdefer hm.hooksMu.Unlock()\n+\thm.hooks = append(hm.hooks, notificationHook)\n+}"
    },
    {
      "sha": "9ea0558bf8e6f9e897427a61414f2224b9b190e6",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/pool_hook.go",
      "status": "added",
      "additions": 182,
      "deletions": 0,
      "changes": 182,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fpool_hook.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fpool_hook.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fpool_hook.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,182 @@\n+package maintnotifications\n+\n+import (\n+\t\"context\"\n+\t\"net\"\n+\t\"sync\"\n+\t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/maintnotifications/logs\"\n+\t\"github.com/redis/go-redis/v9/internal/pool\"\n+)\n+\n+// OperationsManagerInterface defines the interface for completing handoff operations\n+type OperationsManagerInterface interface {\n+\tTrackMovingOperationWithConnID(ctx context.Context, newEndpoint string, deadline time.Time, seqID int64, connID uint64) error\n+\tUntrackOperationWithConnID(seqID int64, connID uint64)\n+}\n+\n+// HandoffRequest represents a request to handoff a connection to a new endpoint\n+type HandoffRequest struct {\n+\tConn     *pool.Conn\n+\tConnID   uint64 // Unique connection identifier\n+\tEndpoint string\n+\tSeqID    int64\n+\tPool     pool.Pooler // Pool to remove connection from on failure\n+}\n+\n+// PoolHook implements pool.PoolHook for Redis-specific connection handling\n+// with maintenance notifications support.\n+type PoolHook struct {\n+\t// Base dialer for creating connections to new endpoints during handoffs\n+\t// args are network and address\n+\tbaseDialer func(context.Context, string, string) (net.Conn, error)\n+\n+\t// Network type (e.g., \"tcp\", \"unix\")\n+\tnetwork string\n+\n+\t// Worker manager for background handoff processing\n+\tworkerManager *handoffWorkerManager\n+\n+\t// Configuration for the maintenance notifications\n+\tconfig *Config\n+\n+\t// Operations manager interface for operation completion tracking\n+\toperationsManager OperationsManagerInterface\n+\n+\t// Pool interface for removing connections on handoff failure\n+\tpool pool.Pooler\n+}\n+\n+// NewPoolHook creates a new pool hook\n+func NewPoolHook(baseDialer func(context.Context, string, string) (net.Conn, error), network string, config *Config, operationsManager OperationsManagerInterface) *PoolHook {\n+\treturn NewPoolHookWithPoolSize(baseDialer, network, config, operationsManager, 0)\n+}\n+\n+// NewPoolHookWithPoolSize creates a new pool hook with pool size for better worker defaults\n+func NewPoolHookWithPoolSize(baseDialer func(context.Context, string, string) (net.Conn, error), network string, config *Config, operationsManager OperationsManagerInterface, poolSize int) *PoolHook {\n+\t// Apply defaults if config is nil or has zero values\n+\tif config == nil {\n+\t\tconfig = config.ApplyDefaultsWithPoolSize(poolSize)\n+\t}\n+\n+\tph := &PoolHook{\n+\t\t// baseDialer is used to create connections to new endpoints during handoffs\n+\t\tbaseDialer:        baseDialer,\n+\t\tnetwork:           network,\n+\t\tconfig:            config,\n+\t\toperationsManager: operationsManager,\n+\t}\n+\n+\t// Create worker manager\n+\tph.workerManager = newHandoffWorkerManager(config, ph)\n+\n+\treturn ph\n+}\n+\n+// SetPool sets the pool interface for removing connections on handoff failure\n+func (ph *PoolHook) SetPool(pooler pool.Pooler) {\n+\tph.pool = pooler\n+}\n+\n+// GetCurrentWorkers returns the current number of active workers (for testing)\n+func (ph *PoolHook) GetCurrentWorkers() int {\n+\treturn ph.workerManager.getCurrentWorkers()\n+}\n+\n+// IsHandoffPending returns true if the given connection has a pending handoff\n+func (ph *PoolHook) IsHandoffPending(conn *pool.Conn) bool {\n+\treturn ph.workerManager.isHandoffPending(conn)\n+}\n+\n+// GetPendingMap returns the pending map for testing purposes\n+func (ph *PoolHook) GetPendingMap() *sync.Map {\n+\treturn ph.workerManager.getPendingMap()\n+}\n+\n+// GetMaxWorkers returns the max workers for testing purposes\n+func (ph *PoolHook) GetMaxWorkers() int {\n+\treturn ph.workerManager.getMaxWorkers()\n+}\n+\n+// GetHandoffQueue returns the handoff queue for testing purposes\n+func (ph *PoolHook) GetHandoffQueue() chan HandoffRequest {\n+\treturn ph.workerManager.getHandoffQueue()\n+}\n+\n+// GetCircuitBreakerStats returns circuit breaker statistics for monitoring\n+func (ph *PoolHook) GetCircuitBreakerStats() []CircuitBreakerStats {\n+\treturn ph.workerManager.getCircuitBreakerStats()\n+}\n+\n+// ResetCircuitBreakers resets all circuit breakers (useful for testing)\n+func (ph *PoolHook) ResetCircuitBreakers() {\n+\tph.workerManager.resetCircuitBreakers()\n+}\n+\n+// OnGet is called when a connection is retrieved from the pool\n+func (ph *PoolHook) OnGet(_ context.Context, conn *pool.Conn, _ bool) (accept bool, err error) {\n+\t// Check if connection is marked for handoff\n+\t// This prevents using connections that have received MOVING notifications\n+\tif conn.ShouldHandoff() {\n+\t\treturn false, ErrConnectionMarkedForHandoffWithState\n+\t}\n+\n+\t// Check if connection is usable (not in UNUSABLE or CLOSED state)\n+\t// This ensures we don't return connections that are currently being handed off or re-authenticated.\n+\tif !conn.IsUsable() {\n+\t\treturn false, ErrConnectionMarkedForHandoff\n+\t}\n+\n+\treturn true, nil\n+}\n+\n+// OnPut is called when a connection is returned to the pool\n+func (ph *PoolHook) OnPut(ctx context.Context, conn *pool.Conn) (shouldPool bool, shouldRemove bool, err error) {\n+\t// first check if we should handoff for faster rejection\n+\tif !conn.ShouldHandoff() {\n+\t\t// Default behavior (no handoff): pool the connection\n+\t\treturn true, false, nil\n+\t}\n+\n+\t// check pending handoff to not queue the same connection twice\n+\tif ph.workerManager.isHandoffPending(conn) {\n+\t\t// Default behavior (pending handoff): pool the connection\n+\t\treturn true, false, nil\n+\t}\n+\n+\tif err := ph.workerManager.queueHandoff(conn); err != nil {\n+\t\t// Failed to queue handoff, remove the connection\n+\t\tinternal.Logger.Printf(ctx, logs.FailedToQueueHandoff(conn.GetID(), err))\n+\t\t// Don't pool, remove connection, no error to caller\n+\t\treturn false, true, nil\n+\t}\n+\n+\t// Check if handoff was already processed by a worker before we can mark it as queued\n+\tif !conn.ShouldHandoff() {\n+\t\t// Handoff was already processed - this is normal and the connection should be pooled\n+\t\treturn true, false, nil\n+\t}\n+\n+\tif err := conn.MarkQueuedForHandoff(); err != nil {\n+\t\t// If marking fails, check if handoff was processed in the meantime\n+\t\tif !conn.ShouldHandoff() {\n+\t\t\t// Handoff was processed - this is normal, pool the connection\n+\t\t\treturn true, false, nil\n+\t\t}\n+\t\t// Other error - remove the connection\n+\t\treturn false, true, nil\n+\t}\n+\tinternal.Logger.Printf(ctx, logs.MarkedForHandoff(conn.GetID()))\n+\treturn true, false, nil\n+}\n+\n+func (ph *PoolHook) OnRemove(_ context.Context, _ *pool.Conn, _ error) {\n+\t// Not used\n+}\n+\n+// Shutdown gracefully shuts down the processor, waiting for workers to complete\n+func (ph *PoolHook) Shutdown(ctx context.Context) error {\n+\treturn ph.workerManager.shutdownWorkers(ctx)\n+}"
    },
    {
      "sha": "937b4ae82e804a58253e6b4c6bdab91f7faa1c3f",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/push_notification_handler.go",
      "status": "added",
      "additions": 282,
      "deletions": 0,
      "changes": 282,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fpush_notification_handler.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fpush_notification_handler.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fpush_notification_handler.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,282 @@\n+package maintnotifications\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"time\"\n+\n+\t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/maintnotifications/logs\"\n+\t\"github.com/redis/go-redis/v9/internal/pool\"\n+\t\"github.com/redis/go-redis/v9/push\"\n+)\n+\n+// NotificationHandler handles push notifications for the simplified manager.\n+type NotificationHandler struct {\n+\tmanager           *Manager\n+\toperationsManager OperationsManagerInterface\n+}\n+\n+// HandlePushNotification processes push notifications with hook support.\n+func (snh *NotificationHandler) HandlePushNotification(ctx context.Context, handlerCtx push.NotificationHandlerContext, notification []interface{}) error {\n+\tif len(notification) == 0 {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidNotificationFormat(notification))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\tnotificationType, ok := notification[0].(string)\n+\tif !ok {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidNotificationTypeFormat(notification[0]))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\t// Process pre-hooks - they can modify the notification or skip processing\n+\tmodifiedNotification, shouldContinue := snh.manager.processPreHooks(ctx, handlerCtx, notificationType, notification)\n+\tif !shouldContinue {\n+\t\treturn nil // Hooks decided to skip processing\n+\t}\n+\n+\tvar err error\n+\tswitch notificationType {\n+\tcase NotificationMoving:\n+\t\terr = snh.handleMoving(ctx, handlerCtx, modifiedNotification)\n+\tcase NotificationMigrating:\n+\t\terr = snh.handleMigrating(ctx, handlerCtx, modifiedNotification)\n+\tcase NotificationMigrated:\n+\t\terr = snh.handleMigrated(ctx, handlerCtx, modifiedNotification)\n+\tcase NotificationFailingOver:\n+\t\terr = snh.handleFailingOver(ctx, handlerCtx, modifiedNotification)\n+\tcase NotificationFailedOver:\n+\t\terr = snh.handleFailedOver(ctx, handlerCtx, modifiedNotification)\n+\tdefault:\n+\t\t// Ignore other notification types (e.g., pub/sub messages)\n+\t\terr = nil\n+\t}\n+\n+\t// Process post-hooks with the result\n+\tsnh.manager.processPostHooks(ctx, handlerCtx, notificationType, modifiedNotification, err)\n+\n+\treturn err\n+}\n+\n+// handleMoving processes MOVING notifications.\n+// [\"MOVING\", seqNum, timeS, endpoint] - per-connection handoff\n+func (snh *NotificationHandler) handleMoving(ctx context.Context, handlerCtx push.NotificationHandlerContext, notification []interface{}) error {\n+\tif len(notification) < 3 {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidNotification(\"MOVING\", notification))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\tseqID, ok := notification[1].(int64)\n+\tif !ok {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidSeqIDInMovingNotification(notification[1]))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\t// Extract timeS\n+\ttimeS, ok := notification[2].(int64)\n+\tif !ok {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidTimeSInMovingNotification(notification[2]))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\tnewEndpoint := \"\"\n+\tif len(notification) > 3 {\n+\t\t// Extract new endpoint\n+\t\tnewEndpoint, ok = notification[3].(string)\n+\t\tif !ok {\n+\t\t\tstringified := fmt.Sprintf(\"%v\", notification[3])\n+\t\t\t// this could be <nil> which is valid\n+\t\t\tif notification[3] == nil || stringified == internal.RedisNull {\n+\t\t\t\tnewEndpoint = \"\"\n+\t\t\t} else {\n+\t\t\t\tinternal.Logger.Printf(ctx, logs.InvalidNewEndpointInMovingNotification(notification[3]))\n+\t\t\t\treturn ErrInvalidNotification\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Get the connection that received this notification\n+\tconn := handlerCtx.Conn\n+\tif conn == nil {\n+\t\tinternal.Logger.Printf(ctx, logs.NoConnectionInHandlerContext(\"MOVING\"))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\t// Type assert to get the underlying pool connection\n+\tvar poolConn *pool.Conn\n+\tif pc, ok := conn.(*pool.Conn); ok {\n+\t\tpoolConn = pc\n+\t} else {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidConnectionTypeInHandlerContext(\"MOVING\", conn, handlerCtx))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\t// If the connection is closed or not pooled, we can ignore the notification\n+\t// this connection won't be remembered by the pool and will be garbage collected\n+\t// Keep pubsub connections around since they are not pooled but are long-lived\n+\t// and should be allowed to handoff (the pubsub instance will reconnect and change\n+\t// the underlying *pool.Conn)\n+\tif (poolConn.IsClosed() || !poolConn.IsPooled()) && !poolConn.IsPubSub() {\n+\t\treturn nil\n+\t}\n+\n+\tdeadline := time.Now().Add(time.Duration(timeS) * time.Second)\n+\t// If newEndpoint is empty, we should schedule a handoff to the current endpoint in timeS/2 seconds\n+\tif newEndpoint == \"\" || newEndpoint == internal.RedisNull {\n+\t\tif internal.LogLevel.DebugOrAbove() {\n+\t\t\tinternal.Logger.Printf(ctx, logs.SchedulingHandoffToCurrentEndpoint(poolConn.GetID(), float64(timeS)/2))\n+\t\t}\n+\t\t// same as current endpoint\n+\t\tnewEndpoint = snh.manager.options.GetAddr()\n+\t\t// delay the handoff for timeS/2 seconds to the same endpoint\n+\t\t// do this in a goroutine to avoid blocking the notification handler\n+\t\t// NOTE: This timer is started while parsing the notification, so the connection is not marked for handoff\n+\t\t// and there should be no possibility of a race condition or double handoff.\n+\t\ttime.AfterFunc(time.Duration(timeS/2)*time.Second, func() {\n+\t\t\tif poolConn == nil || poolConn.IsClosed() {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tif err := snh.markConnForHandoff(poolConn, newEndpoint, seqID, deadline); err != nil {\n+\t\t\t\t// Log error but don't fail the goroutine - use background context since original may be cancelled\n+\t\t\t\tinternal.Logger.Printf(context.Background(), logs.FailedToMarkForHandoff(poolConn.GetID(), err))\n+\t\t\t}\n+\t\t})\n+\t\treturn nil\n+\t}\n+\n+\treturn snh.markConnForHandoff(poolConn, newEndpoint, seqID, deadline)\n+}\n+\n+func (snh *NotificationHandler) markConnForHandoff(conn *pool.Conn, newEndpoint string, seqID int64, deadline time.Time) error {\n+\tif err := conn.MarkForHandoff(newEndpoint, seqID); err != nil {\n+\t\tinternal.Logger.Printf(context.Background(), logs.FailedToMarkForHandoff(conn.GetID(), err))\n+\t\t// Connection is already marked for handoff, which is acceptable\n+\t\t// This can happen if multiple MOVING notifications are received for the same connection\n+\t\treturn nil\n+\t}\n+\t// Optionally track in m\n+\tif snh.operationsManager != nil {\n+\t\tconnID := conn.GetID()\n+\t\t// Track the operation (ignore errors since this is optional)\n+\t\t_ = snh.operationsManager.TrackMovingOperationWithConnID(context.Background(), newEndpoint, deadline, seqID, connID)\n+\t} else {\n+\t\treturn errors.New(logs.ManagerNotInitialized())\n+\t}\n+\treturn nil\n+}\n+\n+// handleMigrating processes MIGRATING notifications.\n+func (snh *NotificationHandler) handleMigrating(ctx context.Context, handlerCtx push.NotificationHandlerContext, notification []interface{}) error {\n+\t// MIGRATING notifications indicate that a connection is about to be migrated\n+\t// Apply relaxed timeouts to the specific connection that received this notification\n+\tif len(notification) < 2 {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidNotification(\"MIGRATING\", notification))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\tif handlerCtx.Conn == nil {\n+\t\tinternal.Logger.Printf(ctx, logs.NoConnectionInHandlerContext(\"MIGRATING\"))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\tconn, ok := handlerCtx.Conn.(*pool.Conn)\n+\tif !ok {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidConnectionTypeInHandlerContext(\"MIGRATING\", handlerCtx.Conn, handlerCtx))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\t// Apply relaxed timeout to this specific connection\n+\tif internal.LogLevel.InfoOrAbove() {\n+\t\tinternal.Logger.Printf(ctx, logs.RelaxedTimeoutDueToNotification(conn.GetID(), \"MIGRATING\", snh.manager.config.RelaxedTimeout))\n+\t}\n+\tconn.SetRelaxedTimeout(snh.manager.config.RelaxedTimeout, snh.manager.config.RelaxedTimeout)\n+\treturn nil\n+}\n+\n+// handleMigrated processes MIGRATED notifications.\n+func (snh *NotificationHandler) handleMigrated(ctx context.Context, handlerCtx push.NotificationHandlerContext, notification []interface{}) error {\n+\t// MIGRATED notifications indicate that a connection migration has completed\n+\t// Restore normal timeouts for the specific connection that received this notification\n+\tif len(notification) < 2 {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidNotification(\"MIGRATED\", notification))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\tif handlerCtx.Conn == nil {\n+\t\tinternal.Logger.Printf(ctx, logs.NoConnectionInHandlerContext(\"MIGRATED\"))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\tconn, ok := handlerCtx.Conn.(*pool.Conn)\n+\tif !ok {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidConnectionTypeInHandlerContext(\"MIGRATED\", handlerCtx.Conn, handlerCtx))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\t// Clear relaxed timeout for this specific connection\n+\tif internal.LogLevel.InfoOrAbove() {\n+\t\tconnID := conn.GetID()\n+\t\tinternal.Logger.Printf(ctx, logs.UnrelaxedTimeout(connID))\n+\t}\n+\tconn.ClearRelaxedTimeout()\n+\treturn nil\n+}\n+\n+// handleFailingOver processes FAILING_OVER notifications.\n+func (snh *NotificationHandler) handleFailingOver(ctx context.Context, handlerCtx push.NotificationHandlerContext, notification []interface{}) error {\n+\t// FAILING_OVER notifications indicate that a connection is about to failover\n+\t// Apply relaxed timeouts to the specific connection that received this notification\n+\tif len(notification) < 2 {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidNotification(\"FAILING_OVER\", notification))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\tif handlerCtx.Conn == nil {\n+\t\tinternal.Logger.Printf(ctx, logs.NoConnectionInHandlerContext(\"FAILING_OVER\"))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\tconn, ok := handlerCtx.Conn.(*pool.Conn)\n+\tif !ok {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidConnectionTypeInHandlerContext(\"FAILING_OVER\", handlerCtx.Conn, handlerCtx))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\t// Apply relaxed timeout to this specific connection\n+\tif internal.LogLevel.InfoOrAbove() {\n+\t\tconnID := conn.GetID()\n+\t\tinternal.Logger.Printf(ctx, logs.RelaxedTimeoutDueToNotification(connID, \"FAILING_OVER\", snh.manager.config.RelaxedTimeout))\n+\t}\n+\tconn.SetRelaxedTimeout(snh.manager.config.RelaxedTimeout, snh.manager.config.RelaxedTimeout)\n+\treturn nil\n+}\n+\n+// handleFailedOver processes FAILED_OVER notifications.\n+func (snh *NotificationHandler) handleFailedOver(ctx context.Context, handlerCtx push.NotificationHandlerContext, notification []interface{}) error {\n+\t// FAILED_OVER notifications indicate that a connection failover has completed\n+\t// Restore normal timeouts for the specific connection that received this notification\n+\tif len(notification) < 2 {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidNotification(\"FAILED_OVER\", notification))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\tif handlerCtx.Conn == nil {\n+\t\tinternal.Logger.Printf(ctx, logs.NoConnectionInHandlerContext(\"FAILED_OVER\"))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\tconn, ok := handlerCtx.Conn.(*pool.Conn)\n+\tif !ok {\n+\t\tinternal.Logger.Printf(ctx, logs.InvalidConnectionTypeInHandlerContext(\"FAILED_OVER\", handlerCtx.Conn, handlerCtx))\n+\t\treturn ErrInvalidNotification\n+\t}\n+\n+\t// Clear relaxed timeout for this specific connection\n+\tif internal.LogLevel.InfoOrAbove() {\n+\t\tconnID := conn.GetID()\n+\t\tinternal.Logger.Printf(ctx, logs.UnrelaxedTimeout(connID))\n+\t}\n+\tconn.ClearRelaxedTimeout()\n+\treturn nil\n+}"
    },
    {
      "sha": "8180bcd97d974a10ea9fbe6eb1a9d3c773eb2b0c",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/maintnotifications/state.go",
      "status": "added",
      "additions": 24,
      "deletions": 0,
      "changes": 24,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fstate.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fstate.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fmaintnotifications%2Fstate.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,24 @@\n+package maintnotifications\n+\n+// State represents the current state of a maintenance operation\n+type State int\n+\n+const (\n+\t// StateIdle indicates no upgrade is in progress\n+\tStateIdle State = iota\n+\n+\t// StateHandoff indicates a connection handoff is in progress\n+\tStateMoving\n+)\n+\n+// String returns a string representation of the state.\n+func (s State) String() string {\n+\tswitch s {\n+\tcase StateIdle:\n+\t\treturn \"idle\"\n+\tcase StateMoving:\n+\t\treturn \"moving\"\n+\tdefault:\n+\t\treturn \"unknown\"\n+\t}\n+}"
    },
    {
      "sha": "9773e86f77526aee490edf5bb76659041ac44be4",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/options.go",
      "status": "modified",
      "additions": 274,
      "deletions": 51,
      "changes": 325,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Foptions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Foptions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Foptions.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -13,7 +13,12 @@ import (\n \t\"strings\"\n \t\"time\"\n \n+\t\"github.com/redis/go-redis/v9/auth\"\n \t\"github.com/redis/go-redis/v9/internal/pool\"\n+\t\"github.com/redis/go-redis/v9/internal/proto\"\n+\t\"github.com/redis/go-redis/v9/internal/util\"\n+\t\"github.com/redis/go-redis/v9/maintnotifications\"\n+\t\"github.com/redis/go-redis/v9/push\"\n )\n \n // Limiter is the interface of a rate limiter or a circuit breaker.\n@@ -29,10 +34,12 @@ type Limiter interface {\n \n // Options keeps the settings to set up redis connection.\n type Options struct {\n-\t// The network type, either tcp or unix.\n-\t// Default is tcp.\n+\t// Network type, either tcp or unix.\n+\t//\n+\t// default: is tcp.\n \tNetwork string\n-\t// host:port address.\n+\n+\t// Addr is the address formated as host:port\n \tAddr string\n \n \t// ClientName will execute the `CLIENT SETNAME ClientName` command for each conn.\n@@ -46,17 +53,21 @@ type Options struct {\n \tOnConnect func(ctx context.Context, cn *Conn) error\n \n \t// Protocol 2 or 3. Use the version to negotiate RESP version with redis-server.\n-\t// Default is 3.\n+\t//\n+\t// default: 3.\n \tProtocol int\n-\t// Use the specified Username to authenticate the current connection\n+\n+\t// Username is used to authenticate the current connection\n \t// with one of the connections defined in the ACL list when connecting\n \t// to a Redis 6.0 instance, or greater, that is using the Redis ACL system.\n \tUsername string\n-\t// Optional password. Must match the password specified in the\n-\t// requirepass server configuration option (if connecting to a Redis 5.0 instance, or lower),\n+\n+\t// Password is an optional password. Must match the password specified in the\n+\t// `requirepass` server configuration option (if connecting to a Redis 5.0 instance, or lower),\n \t// or the User Password when connecting to a Redis 6.0 instance, or greater,\n \t// that is using the Redis ACL system.\n \tPassword string\n+\n \t// CredentialsProvider allows the username and password to be updated\n \t// before reconnecting. It should return the current username and password.\n \tCredentialsProvider func() (username string, password string)\n@@ -67,85 +78,155 @@ type Options struct {\n \t// There will be a conflict between them; if CredentialsProviderContext exists, we will ignore CredentialsProvider.\n \tCredentialsProviderContext func(ctx context.Context) (username string, password string, err error)\n \n-\t// Database to be selected after connecting to the server.\n+\t// StreamingCredentialsProvider is used to retrieve the credentials\n+\t// for the connection from an external source. Those credentials may change\n+\t// during the connection lifetime. This is useful for managed identity\n+\t// scenarios where the credentials are retrieved from an external source.\n+\t//\n+\t// Currently, this is a placeholder for the future implementation.\n+\tStreamingCredentialsProvider auth.StreamingCredentialsProvider\n+\n+\t// DB is the database to be selected after connecting to the server.\n \tDB int\n \n-\t// Maximum number of retries before giving up.\n-\t// Default is 3 retries; -1 (not 0) disables retries.\n+\t// MaxRetries is the maximum number of retries before giving up.\n+\t// -1 (not 0) disables retries.\n+\t//\n+\t// default: 3 retries\n \tMaxRetries int\n-\t// Minimum backoff between each retry.\n-\t// Default is 8 milliseconds; -1 disables backoff.\n+\n+\t// MinRetryBackoff is the minimum backoff between each retry.\n+\t// -1 disables backoff.\n+\t//\n+\t// default: 8 milliseconds\n \tMinRetryBackoff time.Duration\n-\t// Maximum backoff between each retry.\n-\t// Default is 512 milliseconds; -1 disables backoff.\n+\n+\t// MaxRetryBackoff is the maximum backoff between each retry.\n+\t// -1 disables backoff.\n+\t// default: 512 milliseconds;\n \tMaxRetryBackoff time.Duration\n \n-\t// Dial timeout for establishing new connections.\n-\t// Default is 5 seconds.\n+\t// DialTimeout for establishing new connections.\n+\t//\n+\t// default: 5 seconds\n \tDialTimeout time.Duration\n-\t// Timeout for socket reads. If reached, commands will fail\n+\n+\t// DialerRetries is the maximum number of retry attempts when dialing fails.\n+\t//\n+\t// default: 5\n+\tDialerRetries int\n+\n+\t// DialerRetryTimeout is the backoff duration between retry attempts.\n+\t//\n+\t// default: 100 milliseconds\n+\tDialerRetryTimeout time.Duration\n+\n+\t// ReadTimeout for socket reads. If reached, commands will fail\n \t// with a timeout instead of blocking. Supported values:\n-\t//   - `0` - default timeout (3 seconds).\n-\t//   - `-1` - no timeout (block indefinitely).\n-\t//   - `-2` - disables SetReadDeadline calls completely.\n+\t//\n+\t//\t- `-1` - no timeout (block indefinitely).\n+\t//\t- `-2` - disables SetReadDeadline calls completely.\n+\t//\n+\t// default: 3 seconds\n \tReadTimeout time.Duration\n-\t// Timeout for socket writes. If reached, commands will fail\n+\n+\t// WriteTimeout for socket writes. If reached, commands will fail\n \t// with a timeout instead of blocking.  Supported values:\n-\t//   - `0` - default timeout (3 seconds).\n-\t//   - `-1` - no timeout (block indefinitely).\n-\t//   - `-2` - disables SetWriteDeadline calls completely.\n+\t//\n+\t//\t- `-1` - no timeout (block indefinitely).\n+\t//\t- `-2` - disables SetWriteDeadline calls completely.\n+\t//\n+\t// default: 3 seconds\n \tWriteTimeout time.Duration\n+\n \t// ContextTimeoutEnabled controls whether the client respects context timeouts and deadlines.\n \t// See https://redis.uptrace.dev/guide/go-redis-debugging.html#timeouts\n \tContextTimeoutEnabled bool\n \n-\t// Type of connection pool.\n-\t// true for FIFO pool, false for LIFO pool.\n+\t// ReadBufferSize is the size of the bufio.Reader buffer for each connection.\n+\t// Larger buffers can improve performance for commands that return large responses.\n+\t// Smaller buffers can improve memory usage for larger pools.\n+\t//\n+\t// default: 32KiB (32768 bytes)\n+\tReadBufferSize int\n+\n+\t// WriteBufferSize is the size of the bufio.Writer buffer for each connection.\n+\t// Larger buffers can improve performance for large pipelines and commands with many arguments.\n+\t// Smaller buffers can improve memory usage for larger pools.\n+\t//\n+\t// default: 32KiB (32768 bytes)\n+\tWriteBufferSize int\n+\n+\t// PoolFIFO type of connection pool.\n+\t//\n+\t//\t- true for FIFO pool\n+\t//\t- false for LIFO pool.\n+\t//\n \t// Note that FIFO has slightly higher overhead compared to LIFO,\n \t// but it helps closing idle connections faster reducing the pool size.\n+\t// default: false\n \tPoolFIFO bool\n-\t// Base number of socket connections.\n+\n+\t// PoolSize is the base number of socket connections.\n \t// Default is 10 connections per every available CPU as reported by runtime.GOMAXPROCS.\n \t// If there is not enough connections in the pool, new connections will be allocated in excess of PoolSize,\n \t// you can limit it through MaxActiveConns\n+\t//\n+\t// default: 10 * runtime.GOMAXPROCS(0)\n \tPoolSize int\n-\t// Amount of time client waits for connection if all connections\n+\n+\t// MaxConcurrentDials is the maximum number of concurrent connection creation goroutines.\n+\t// If <= 0, defaults to PoolSize. If > PoolSize, it will be capped at PoolSize.\n+\tMaxConcurrentDials int\n+\n+\t// PoolTimeout is the amount of time client waits for connection if all connections\n \t// are busy before returning an error.\n-\t// Default is ReadTimeout + 1 second.\n+\t//\n+\t// default: ReadTimeout + 1 second\n \tPoolTimeout time.Duration\n-\t// Minimum number of idle connections which is useful when establishing\n-\t// new connection is slow.\n-\t// Default is 0. the idle connections are not closed by default.\n+\n+\t// MinIdleConns is the minimum number of idle connections which is useful when establishing\n+\t// new connection is slow. The idle connections are not closed by default.\n+\t//\n+\t// default: 0\n \tMinIdleConns int\n-\t// Maximum number of idle connections.\n-\t// Default is 0. the idle connections are not closed by default.\n+\n+\t// MaxIdleConns is the maximum number of idle connections.\n+\t// The idle connections are not closed by default.\n+\t//\n+\t// default: 0\n \tMaxIdleConns int\n-\t// Maximum number of connections allocated by the pool at a given time.\n+\n+\t// MaxActiveConns is the maximum number of connections allocated by the pool at a given time.\n \t// When zero, there is no limit on the number of connections in the pool.\n+\t// If the pool is full, the next call to Get() will block until a connection is released.\n \tMaxActiveConns int\n+\n \t// ConnMaxIdleTime is the maximum amount of time a connection may be idle.\n \t// Should be less than server's timeout.\n \t//\n \t// Expired connections may be closed lazily before reuse.\n \t// If d <= 0, connections are not closed due to a connection's idle time.\n+\t// -1 disables idle timeout check.\n \t//\n-\t// Default is 30 minutes. -1 disables idle timeout check.\n+\t// default: 30 minutes\n \tConnMaxIdleTime time.Duration\n+\n \t// ConnMaxLifetime is the maximum amount of time a connection may be reused.\n \t//\n \t// Expired connections may be closed lazily before reuse.\n \t// If <= 0, connections are not closed due to a connection's age.\n \t//\n-\t// Default is to not close idle connections.\n+\t// default: 0\n \tConnMaxLifetime time.Duration\n \n-\t// TLS Config to use. When set, TLS will be negotiated.\n+\t// TLSConfig to use. When set, TLS will be negotiated.\n \tTLSConfig *tls.Config\n \n \t// Limiter interface used to implement circuit breaker or rate limiter.\n \tLimiter Limiter\n \n-\t// Enables read only queries on slave/follower nodes.\n+\t// readOnly enables read only queries on slave/follower nodes.\n \treadOnly bool\n \n \t// DisableIndentity - Disable set-lib on connect.\n@@ -161,10 +242,31 @@ type Options struct {\n \tDisableIdentity bool\n \n \t// Add suffix to client name. Default is empty.\n+\t// IdentitySuffix - add suffix to client name.\n \tIdentitySuffix string\n \n \t// UnstableResp3 enables Unstable mode for Redis Search module with RESP3.\n+\t// When unstable mode is enabled, the client will use RESP3 protocol and only be able to use RawResult\n \tUnstableResp3 bool\n+\n+\t// Push notifications are always enabled for RESP3 connections (Protocol: 3)\n+\t// and are not available for RESP2 connections. No configuration option is needed.\n+\n+\t// PushNotificationProcessor is the processor for handling push notifications.\n+\t// If nil, a default processor will be created for RESP3 connections.\n+\tPushNotificationProcessor push.NotificationProcessor\n+\n+\t// FailingTimeoutSeconds is the timeout in seconds for marking a cluster node as failing.\n+\t// When a node is marked as failing, it will be avoided for this duration.\n+\t// Default is 15 seconds.\n+\tFailingTimeoutSeconds int\n+\n+\t// MaintNotificationsConfig provides custom configuration for maintnotifications.\n+\t// When MaintNotificationsConfig.Mode is not \"disabled\", the client will handle\n+\t// cluster upgrade notifications gracefully and manage connection/pool state\n+\t// transitions seamlessly. Requires Protocol: 3 (RESP3) for push notifications.\n+\t// If nil, maintnotifications are in \"auto\" mode and will be enabled if the server supports it.\n+\tMaintNotificationsConfig *maintnotifications.Config\n }\n \n func (opt *Options) init() {\n@@ -178,15 +280,35 @@ func (opt *Options) init() {\n \t\t\topt.Network = \"tcp\"\n \t\t}\n \t}\n+\tif opt.Protocol < 2 {\n+\t\topt.Protocol = 3\n+\t}\n \tif opt.DialTimeout == 0 {\n \t\topt.DialTimeout = 5 * time.Second\n \t}\n+\tif opt.DialerRetries == 0 {\n+\t\topt.DialerRetries = 5\n+\t}\n+\tif opt.DialerRetryTimeout == 0 {\n+\t\topt.DialerRetryTimeout = 100 * time.Millisecond\n+\t}\n \tif opt.Dialer == nil {\n \t\topt.Dialer = NewDialer(opt)\n \t}\n \tif opt.PoolSize == 0 {\n \t\topt.PoolSize = 10 * runtime.GOMAXPROCS(0)\n \t}\n+\tif opt.MaxConcurrentDials <= 0 {\n+\t\topt.MaxConcurrentDials = opt.PoolSize\n+\t} else if opt.MaxConcurrentDials > opt.PoolSize {\n+\t\topt.MaxConcurrentDials = opt.PoolSize\n+\t}\n+\tif opt.ReadBufferSize == 0 {\n+\t\topt.ReadBufferSize = proto.DefaultBufferSize\n+\t}\n+\tif opt.WriteBufferSize == 0 {\n+\t\topt.WriteBufferSize = proto.DefaultBufferSize\n+\t}\n \tswitch opt.ReadTimeout {\n \tcase -2:\n \t\topt.ReadTimeout = -1\n@@ -214,9 +336,10 @@ func (opt *Options) init() {\n \t\topt.ConnMaxIdleTime = 30 * time.Minute\n \t}\n \n-\tif opt.MaxRetries == -1 {\n+\tswitch opt.MaxRetries {\n+\tcase -1:\n \t\topt.MaxRetries = 0\n-\t} else if opt.MaxRetries == 0 {\n+\tcase 0:\n \t\topt.MaxRetries = 3\n \t}\n \tswitch opt.MinRetryBackoff {\n@@ -231,13 +354,40 @@ func (opt *Options) init() {\n \tcase 0:\n \t\topt.MaxRetryBackoff = 512 * time.Millisecond\n \t}\n+\n+\tif opt.FailingTimeoutSeconds == 0 {\n+\t\topt.FailingTimeoutSeconds = 15\n+\t}\n+\n+\topt.MaintNotificationsConfig = opt.MaintNotificationsConfig.ApplyDefaultsWithPoolConfig(opt.PoolSize, opt.MaxActiveConns)\n+\n+\t// auto-detect endpoint type if not specified\n+\tendpointType := opt.MaintNotificationsConfig.EndpointType\n+\tif endpointType == \"\" || endpointType == maintnotifications.EndpointTypeAuto {\n+\t\t// Auto-detect endpoint type if not specified\n+\t\tendpointType = maintnotifications.DetectEndpointType(opt.Addr, opt.TLSConfig != nil)\n+\t}\n+\topt.MaintNotificationsConfig.EndpointType = endpointType\n }\n \n func (opt *Options) clone() *Options {\n \tclone := *opt\n+\n+\t// Deep clone MaintNotificationsConfig to avoid sharing between clients\n+\tif opt.MaintNotificationsConfig != nil {\n+\t\tconfigClone := *opt.MaintNotificationsConfig\n+\t\tclone.MaintNotificationsConfig = &configClone\n+\t}\n+\n \treturn &clone\n }\n \n+// NewDialer returns a function that will be used as the default dialer\n+// when none is specified in Options.Dialer.\n+func (opt *Options) NewDialer() func(context.Context, string, string) (net.Conn, error) {\n+\treturn NewDialer(opt)\n+}\n+\n // NewDialer returns a function that will be used as the default dialer\n // when none is specified in Options.Dialer.\n func NewDialer(opt *Options) func(context.Context, string, string) (net.Conn, error) {\n@@ -276,6 +426,7 @@ func NewDialer(opt *Options) func(context.Context, string, string) (net.Conn, er\n //     URL attributes (scheme, host, userinfo, resp.), query parameters using these\n //     names will be treated as unknown parameters\n //   - unknown parameter names will result in an error\n+//   - use \"skip_verify=true\" to ignore TLS certificate validation\n //\n // Examples:\n //\n@@ -483,6 +634,7 @@ func setupConnParams(u *url.URL, o *Options) (*Options, error) {\n \to.MinIdleConns = q.int(\"min_idle_conns\")\n \to.MaxIdleConns = q.int(\"max_idle_conns\")\n \to.MaxActiveConns = q.int(\"max_active_conns\")\n+\to.MaxConcurrentDials = q.int(\"max_concurrent_dials\")\n \tif q.has(\"conn_max_idle_time\") {\n \t\to.ConnMaxIdleTime = q.duration(\"conn_max_idle_time\")\n \t} else {\n@@ -496,6 +648,9 @@ func setupConnParams(u *url.URL, o *Options) (*Options, error) {\n \tif q.err != nil {\n \t\treturn nil, q.err\n \t}\n+\tif o.TLSConfig != nil && q.has(\"skip_verify\") {\n+\t\to.TLSConfig.InsecureSkipVerify = q.bool(\"skip_verify\")\n+\t}\n \n \t// any parameters left?\n \tif r := q.remaining(); len(r) > 0 {\n@@ -519,18 +674,86 @@ func getUserPassword(u *url.URL) (string, string) {\n func newConnPool(\n \topt *Options,\n \tdialer func(ctx context.Context, network, addr string) (net.Conn, error),\n-) *pool.ConnPool {\n+) (*pool.ConnPool, error) {\n+\tpoolSize, err := util.SafeIntToInt32(opt.PoolSize, \"PoolSize\")\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tminIdleConns, err := util.SafeIntToInt32(opt.MinIdleConns, \"MinIdleConns\")\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tmaxIdleConns, err := util.SafeIntToInt32(opt.MaxIdleConns, \"MaxIdleConns\")\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tmaxActiveConns, err := util.SafeIntToInt32(opt.MaxActiveConns, \"MaxActiveConns\")\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n \treturn pool.NewConnPool(&pool.Options{\n \t\tDialer: func(ctx context.Context) (net.Conn, error) {\n \t\t\treturn dialer(ctx, opt.Network, opt.Addr)\n \t\t},\n-\t\tPoolFIFO:        opt.PoolFIFO,\n-\t\tPoolSize:        opt.PoolSize,\n-\t\tPoolTimeout:     opt.PoolTimeout,\n-\t\tMinIdleConns:    opt.MinIdleConns,\n-\t\tMaxIdleConns:    opt.MaxIdleConns,\n-\t\tMaxActiveConns:  opt.MaxActiveConns,\n-\t\tConnMaxIdleTime: opt.ConnMaxIdleTime,\n-\t\tConnMaxLifetime: opt.ConnMaxLifetime,\n-\t})\n+\t\tPoolFIFO:                 opt.PoolFIFO,\n+\t\tPoolSize:                 poolSize,\n+\t\tMaxConcurrentDials:       opt.MaxConcurrentDials,\n+\t\tPoolTimeout:              opt.PoolTimeout,\n+\t\tDialTimeout:              opt.DialTimeout,\n+\t\tDialerRetries:            opt.DialerRetries,\n+\t\tDialerRetryTimeout:       opt.DialerRetryTimeout,\n+\t\tMinIdleConns:             minIdleConns,\n+\t\tMaxIdleConns:             maxIdleConns,\n+\t\tMaxActiveConns:           maxActiveConns,\n+\t\tConnMaxIdleTime:          opt.ConnMaxIdleTime,\n+\t\tConnMaxLifetime:          opt.ConnMaxLifetime,\n+\t\tReadBufferSize:           opt.ReadBufferSize,\n+\t\tWriteBufferSize:          opt.WriteBufferSize,\n+\t\tPushNotificationsEnabled: opt.Protocol == 3,\n+\t}), nil\n+}\n+\n+func newPubSubPool(opt *Options, dialer func(ctx context.Context, network, addr string) (net.Conn, error),\n+) (*pool.PubSubPool, error) {\n+\tpoolSize, err := util.SafeIntToInt32(opt.PoolSize, \"PoolSize\")\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tminIdleConns, err := util.SafeIntToInt32(opt.MinIdleConns, \"MinIdleConns\")\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tmaxIdleConns, err := util.SafeIntToInt32(opt.MaxIdleConns, \"MaxIdleConns\")\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tmaxActiveConns, err := util.SafeIntToInt32(opt.MaxActiveConns, \"MaxActiveConns\")\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\treturn pool.NewPubSubPool(&pool.Options{\n+\t\tPoolFIFO:                 opt.PoolFIFO,\n+\t\tPoolSize:                 poolSize,\n+\t\tMaxConcurrentDials:       opt.MaxConcurrentDials,\n+\t\tPoolTimeout:              opt.PoolTimeout,\n+\t\tDialTimeout:              opt.DialTimeout,\n+\t\tDialerRetries:            opt.DialerRetries,\n+\t\tDialerRetryTimeout:       opt.DialerRetryTimeout,\n+\t\tMinIdleConns:             minIdleConns,\n+\t\tMaxIdleConns:             maxIdleConns,\n+\t\tMaxActiveConns:           maxActiveConns,\n+\t\tConnMaxIdleTime:          opt.ConnMaxIdleTime,\n+\t\tConnMaxLifetime:          opt.ConnMaxLifetime,\n+\t\tReadBufferSize:           32 * 1024,\n+\t\tWriteBufferSize:          32 * 1024,\n+\t\tPushNotificationsEnabled: opt.Protocol == 3,\n+\t}, dialer), nil\n }"
    },
    {
      "sha": "7925d2c603ad2c366b2568a0512aa6ea8defabcb",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/osscluster.go",
      "status": "modified",
      "additions": 324,
      "deletions": 130,
      "changes": 454,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fosscluster.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fosscluster.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fosscluster.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -14,11 +14,18 @@ import (\n \t\"sync/atomic\"\n \t\"time\"\n \n+\t\"github.com/redis/go-redis/v9/auth\"\n \t\"github.com/redis/go-redis/v9/internal\"\n \t\"github.com/redis/go-redis/v9/internal/hashtag\"\n \t\"github.com/redis/go-redis/v9/internal/pool\"\n \t\"github.com/redis/go-redis/v9/internal/proto\"\n \t\"github.com/redis/go-redis/v9/internal/rand\"\n+\t\"github.com/redis/go-redis/v9/maintnotifications\"\n+\t\"github.com/redis/go-redis/v9/push\"\n+)\n+\n+const (\n+\tminLatencyMeasurementInterval = 10 * time.Second\n )\n \n var errClusterNoNodes = fmt.Errorf(\"redis: cluster has no nodes\")\n@@ -33,6 +40,7 @@ type ClusterOptions struct {\n \tClientName string\n \n \t// NewClient creates a cluster node client with provided name and options.\n+\t// If NewClient is set by the user, the user is responsible for handling maintnotifications upgrades and push notifications.\n \tNewClient func(opt *Options) *Client\n \n \t// The maximum number of retries before giving up. Command is retried\n@@ -62,12 +70,17 @@ type ClusterOptions struct {\n \n \tOnConnect func(ctx context.Context, cn *Conn) error\n \n-\tProtocol                   int\n-\tUsername                   string\n-\tPassword                   string\n-\tCredentialsProvider        func() (username string, password string)\n-\tCredentialsProviderContext func(ctx context.Context) (username string, password string, err error)\n-\n+\tProtocol                     int\n+\tUsername                     string\n+\tPassword                     string\n+\tCredentialsProvider          func() (username string, password string)\n+\tCredentialsProviderContext   func(ctx context.Context) (username string, password string, err error)\n+\tStreamingCredentialsProvider auth.StreamingCredentialsProvider\n+\n+\t// MaxRetries is the maximum number of retries before giving up.\n+\t// For ClusterClient, retries are disabled by default (set to -1),\n+\t// because the cluster client handles all kinds of retries internally.\n+\t// This is intentional and differs from the standalone Options default.\n \tMaxRetries      int\n \tMinRetryBackoff time.Duration\n \tMaxRetryBackoff time.Duration\n@@ -86,6 +99,20 @@ type ClusterOptions struct {\n \tConnMaxIdleTime time.Duration\n \tConnMaxLifetime time.Duration\n \n+\t// ReadBufferSize is the size of the bufio.Reader buffer for each connection.\n+\t// Larger buffers can improve performance for commands that return large responses.\n+\t// Smaller buffers can improve memory usage for larger pools.\n+\t//\n+\t// default: 32KiB (32768 bytes)\n+\tReadBufferSize int\n+\n+\t// WriteBufferSize is the size of the bufio.Writer buffer for each connection.\n+\t// Larger buffers can improve performance for large pipelines and commands with many arguments.\n+\t// Smaller buffers can improve memory usage for larger pools.\n+\t//\n+\t// default: 32KiB (32768 bytes)\n+\tWriteBufferSize int\n+\n \tTLSConfig *tls.Config\n \n \t// DisableIndentity - Disable set-lib on connect.\n@@ -104,12 +131,30 @@ type ClusterOptions struct {\n \n \t// UnstableResp3 enables Unstable mode for Redis Search module with RESP3.\n \tUnstableResp3 bool\n+\n+\t// PushNotificationProcessor is the processor for handling push notifications.\n+\t// If nil, a default processor will be created for RESP3 connections.\n+\tPushNotificationProcessor push.NotificationProcessor\n+\n+\t// FailingTimeoutSeconds is the timeout in seconds for marking a cluster node as failing.\n+\t// When a node is marked as failing, it will be avoided for this duration.\n+\t// Default is 15 seconds.\n+\tFailingTimeoutSeconds int\n+\n+\t// MaintNotificationsConfig provides custom configuration for maintnotifications upgrades.\n+\t// When MaintNotificationsConfig.Mode is not \"disabled\", the client will handle\n+\t// cluster upgrade notifications gracefully and manage connection/pool state\n+\t// transitions seamlessly. Requires Protocol: 3 (RESP3) for push notifications.\n+\t// If nil, maintnotifications upgrades are in \"auto\" mode and will be enabled if the server supports it.\n+\t// The ClusterClient does not directly work with maintnotifications, it is up to the clients in the Nodes map to work with maintnotifications.\n+\tMaintNotificationsConfig *maintnotifications.Config\n }\n \n func (opt *ClusterOptions) init() {\n-\tif opt.MaxRedirects == -1 {\n+\tswitch opt.MaxRedirects {\n+\tcase -1:\n \t\topt.MaxRedirects = 0\n-\t} else if opt.MaxRedirects == 0 {\n+\tcase 0:\n \t\topt.MaxRedirects = 3\n \t}\n \n@@ -120,6 +165,12 @@ func (opt *ClusterOptions) init() {\n \tif opt.PoolSize == 0 {\n \t\topt.PoolSize = 5 * runtime.GOMAXPROCS(0)\n \t}\n+\tif opt.ReadBufferSize == 0 {\n+\t\topt.ReadBufferSize = proto.DefaultBufferSize\n+\t}\n+\tif opt.WriteBufferSize == 0 {\n+\t\topt.WriteBufferSize = proto.DefaultBufferSize\n+\t}\n \n \tswitch opt.ReadTimeout {\n \tcase -1:\n@@ -153,6 +204,10 @@ func (opt *ClusterOptions) init() {\n \tif opt.NewClient == nil {\n \t\topt.NewClient = NewClient\n \t}\n+\n+\tif opt.FailingTimeoutSeconds == 0 {\n+\t\topt.FailingTimeoutSeconds = 15\n+\t}\n }\n \n // ParseClusterURL parses a URL into ClusterOptions that can be used to connect to Redis.\n@@ -257,6 +312,7 @@ func setupClusterQueryParams(u *url.URL, o *ClusterOptions) (*ClusterOptions, er\n \to.PoolTimeout = q.duration(\"pool_timeout\")\n \to.ConnMaxLifetime = q.duration(\"conn_max_lifetime\")\n \to.ConnMaxIdleTime = q.duration(\"conn_max_idle_time\")\n+\to.FailingTimeoutSeconds = q.int(\"failing_timeout_seconds\")\n \n \tif q.err != nil {\n \t\treturn nil, q.err\n@@ -282,16 +338,24 @@ func setupClusterQueryParams(u *url.URL, o *ClusterOptions) (*ClusterOptions, er\n }\n \n func (opt *ClusterOptions) clientOptions() *Options {\n+\t// Clone MaintNotificationsConfig to avoid sharing between cluster node clients\n+\tvar maintNotificationsConfig *maintnotifications.Config\n+\tif opt.MaintNotificationsConfig != nil {\n+\t\tconfigClone := *opt.MaintNotificationsConfig\n+\t\tmaintNotificationsConfig = &configClone\n+\t}\n+\n \treturn &Options{\n \t\tClientName: opt.ClientName,\n \t\tDialer:     opt.Dialer,\n \t\tOnConnect:  opt.OnConnect,\n \n-\t\tProtocol:                   opt.Protocol,\n-\t\tUsername:                   opt.Username,\n-\t\tPassword:                   opt.Password,\n-\t\tCredentialsProvider:        opt.CredentialsProvider,\n-\t\tCredentialsProviderContext: opt.CredentialsProviderContext,\n+\t\tProtocol:                     opt.Protocol,\n+\t\tUsername:                     opt.Username,\n+\t\tPassword:                     opt.Password,\n+\t\tCredentialsProvider:          opt.CredentialsProvider,\n+\t\tCredentialsProviderContext:   opt.CredentialsProviderContext,\n+\t\tStreamingCredentialsProvider: opt.StreamingCredentialsProvider,\n \n \t\tMaxRetries:      opt.MaxRetries,\n \t\tMinRetryBackoff: opt.MinRetryBackoff,\n@@ -302,25 +366,30 @@ func (opt *ClusterOptions) clientOptions() *Options {\n \t\tWriteTimeout:          opt.WriteTimeout,\n \t\tContextTimeoutEnabled: opt.ContextTimeoutEnabled,\n \n-\t\tPoolFIFO:         opt.PoolFIFO,\n-\t\tPoolSize:         opt.PoolSize,\n-\t\tPoolTimeout:      opt.PoolTimeout,\n-\t\tMinIdleConns:     opt.MinIdleConns,\n-\t\tMaxIdleConns:     opt.MaxIdleConns,\n-\t\tMaxActiveConns:   opt.MaxActiveConns,\n-\t\tConnMaxIdleTime:  opt.ConnMaxIdleTime,\n-\t\tConnMaxLifetime:  opt.ConnMaxLifetime,\n-\t\tDisableIdentity:  opt.DisableIdentity,\n-\t\tDisableIndentity: opt.DisableIdentity,\n-\t\tIdentitySuffix:   opt.IdentitySuffix,\n-\t\tTLSConfig:        opt.TLSConfig,\n+\t\tPoolFIFO:              opt.PoolFIFO,\n+\t\tPoolSize:              opt.PoolSize,\n+\t\tPoolTimeout:           opt.PoolTimeout,\n+\t\tMinIdleConns:          opt.MinIdleConns,\n+\t\tMaxIdleConns:          opt.MaxIdleConns,\n+\t\tMaxActiveConns:        opt.MaxActiveConns,\n+\t\tConnMaxIdleTime:       opt.ConnMaxIdleTime,\n+\t\tConnMaxLifetime:       opt.ConnMaxLifetime,\n+\t\tReadBufferSize:        opt.ReadBufferSize,\n+\t\tWriteBufferSize:       opt.WriteBufferSize,\n+\t\tDisableIdentity:       opt.DisableIdentity,\n+\t\tDisableIndentity:      opt.DisableIdentity,\n+\t\tIdentitySuffix:        opt.IdentitySuffix,\n+\t\tFailingTimeoutSeconds: opt.FailingTimeoutSeconds,\n+\t\tTLSConfig:             opt.TLSConfig,\n \t\t// If ClusterSlots is populated, then we probably have an artificial\n \t\t// cluster whose nodes are not in clustering mode (otherwise there isn't\n \t\t// much use for ClusterSlots config).  This means we cannot execute the\n \t\t// READONLY command against that node -- setting readOnly to false in such\n \t\t// situations in the options below will prevent that from happening.\n-\t\treadOnly:      opt.ReadOnly && opt.ClusterSlots == nil,\n-\t\tUnstableResp3: opt.UnstableResp3,\n+\t\treadOnly:                  opt.ReadOnly && opt.ClusterSlots == nil,\n+\t\tUnstableResp3:             opt.UnstableResp3,\n+\t\tMaintNotificationsConfig:  maintNotificationsConfig,\n+\t\tPushNotificationProcessor: opt.PushNotificationProcessor,\n \t}\n }\n \n@@ -332,6 +401,10 @@ type clusterNode struct {\n \tlatency    uint32 // atomic\n \tgeneration uint32 // atomic\n \tfailing    uint32 // atomic\n+\tloaded     uint32 // atomic\n+\n+\t// last time the latency measurement was performed for the node, stored in nanoseconds from epoch\n+\tlastLatencyMeasurement int64 // atomic\n }\n \n func newClusterNode(clOpt *ClusterOptions, addr string) *clusterNode {\n@@ -384,6 +457,7 @@ func (n *clusterNode) updateLatency() {\n \t\tlatency = float64(dur) / float64(successes)\n \t}\n \tatomic.StoreUint32(&n.latency, uint32(latency+0.5))\n+\tn.SetLastLatencyMeasurement(time.Now())\n }\n \n func (n *clusterNode) Latency() time.Duration {\n@@ -393,10 +467,11 @@ func (n *clusterNode) Latency() time.Duration {\n \n func (n *clusterNode) MarkAsFailing() {\n \tatomic.StoreUint32(&n.failing, uint32(time.Now().Unix()))\n+\tatomic.StoreUint32(&n.loaded, 0)\n }\n \n func (n *clusterNode) Failing() bool {\n-\tconst timeout = 15 // 15 seconds\n+\ttimeout := int64(n.Client.opt.FailingTimeoutSeconds)\n \n \tfailing := atomic.LoadUint32(&n.failing)\n \tif failing == 0 {\n@@ -413,6 +488,10 @@ func (n *clusterNode) Generation() uint32 {\n \treturn atomic.LoadUint32(&n.generation)\n }\n \n+func (n *clusterNode) LastLatencyMeasurement() int64 {\n+\treturn atomic.LoadInt64(&n.lastLatencyMeasurement)\n+}\n+\n func (n *clusterNode) SetGeneration(gen uint32) {\n \tfor {\n \t\tv := atomic.LoadUint32(&n.generation)\n@@ -422,6 +501,33 @@ func (n *clusterNode) SetGeneration(gen uint32) {\n \t}\n }\n \n+func (n *clusterNode) SetLastLatencyMeasurement(t time.Time) {\n+\tfor {\n+\t\tv := atomic.LoadInt64(&n.lastLatencyMeasurement)\n+\t\tif t.UnixNano() < v || atomic.CompareAndSwapInt64(&n.lastLatencyMeasurement, v, t.UnixNano()) {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+}\n+\n+func (n *clusterNode) Loading() bool {\n+\tloaded := atomic.LoadUint32(&n.loaded)\n+\tif loaded == 1 {\n+\t\treturn false\n+\t}\n+\n+\t// check if the node is loading\n+\tctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)\n+\tdefer cancel()\n+\n+\terr := n.Client.Ping(ctx).Err()\n+\tloading := err != nil && isLoadingError(err)\n+\tif !loading {\n+\t\tatomic.StoreUint32(&n.loaded, 1)\n+\t}\n+\treturn loading\n+}\n+\n //------------------------------------------------------------------------------\n \n type clusterNodes struct {\n@@ -434,13 +540,12 @@ type clusterNodes struct {\n \tclosed      bool\n \tonNewNode   []func(rdb *Client)\n \n-\t_generation uint32 // atomic\n+\tgeneration uint32 // atomic\n }\n \n func newClusterNodes(opt *ClusterOptions) *clusterNodes {\n \treturn &clusterNodes{\n-\t\topt: opt,\n-\n+\t\topt:   opt,\n \t\taddrs: opt.Addrs,\n \t\tnodes: make(map[string]*clusterNode),\n \t}\n@@ -500,21 +605,21 @@ func (c *clusterNodes) Addrs() ([]string, error) {\n }\n \n func (c *clusterNodes) NextGeneration() uint32 {\n-\treturn atomic.AddUint32(&c._generation, 1)\n+\treturn atomic.AddUint32(&c.generation, 1)\n }\n \n // GC removes unused nodes.\n func (c *clusterNodes) GC(generation uint32) {\n-\t//nolint:prealloc\n \tvar collected []*clusterNode\n \n \tc.mu.Lock()\n \n \tc.activeAddrs = c.activeAddrs[:0]\n+\tnow := time.Now()\n \tfor addr, node := range c.nodes {\n \t\tif node.Generation() >= generation {\n \t\t\tc.activeAddrs = append(c.activeAddrs, addr)\n-\t\t\tif c.opt.RouteByLatency {\n+\t\t\tif c.opt.RouteByLatency && node.LastLatencyMeasurement() < now.Add(-minLatencyMeasurementInterval).UnixNano() {\n \t\t\t\tgo node.updateLatency()\n \t\t\t}\n \t\t\tcontinue\n@@ -557,23 +662,20 @@ func (c *clusterNodes) GetOrCreate(addr string) (*clusterNode, error) {\n \t\tfn(node.Client)\n \t}\n \n-\tc.addrs = appendIfNotExists(c.addrs, addr)\n+\tc.addrs = appendIfNotExist(c.addrs, addr)\n \tc.nodes[addr] = node\n \n \treturn node, nil\n }\n \n func (c *clusterNodes) get(addr string) (*clusterNode, error) {\n-\tvar node *clusterNode\n-\tvar err error\n \tc.mu.RLock()\n+\tdefer c.mu.RUnlock()\n+\n \tif c.closed {\n-\t\terr = pool.ErrClosed\n-\t} else {\n-\t\tnode = c.nodes[addr]\n+\t\treturn nil, pool.ErrClosed\n \t}\n-\tc.mu.RUnlock()\n-\treturn node, err\n+\treturn c.nodes[addr], nil\n }\n \n func (c *clusterNodes) All() ([]*clusterNode, error) {\n@@ -604,8 +706,9 @@ func (c *clusterNodes) Random() (*clusterNode, error) {\n //------------------------------------------------------------------------------\n \n type clusterSlot struct {\n-\tstart, end int\n-\tnodes      []*clusterNode\n+\tstart int\n+\tend   int\n+\tnodes []*clusterNode\n }\n \n type clusterSlotSlice []*clusterSlot\n@@ -665,9 +768,9 @@ func newClusterState(\n \t\t\tnodes = append(nodes, node)\n \n \t\t\tif i == 0 {\n-\t\t\t\tc.Masters = appendUniqueNode(c.Masters, node)\n+\t\t\t\tc.Masters = appendIfNotExist(c.Masters, node)\n \t\t\t} else {\n-\t\t\t\tc.Slaves = appendUniqueNode(c.Slaves, node)\n+\t\t\t\tc.Slaves = appendIfNotExist(c.Slaves, node)\n \t\t\t}\n \t\t}\n \n@@ -706,12 +809,25 @@ func replaceLoopbackHost(nodeAddr, originHost string) string {\n \treturn net.JoinHostPort(originHost, nodePort)\n }\n \n+// isLoopback returns true if the host is a loopback address.\n+// For IP addresses, it uses net.IP.IsLoopback().\n+// For hostnames, it recognizes well-known loopback hostnames like \"localhost\"\n+// and Docker-specific loopback patterns like \"*.docker.internal\".\n func isLoopback(host string) bool {\n \tip := net.ParseIP(host)\n-\tif ip == nil {\n+\tif ip != nil {\n+\t\treturn ip.IsLoopback()\n+\t}\n+\n+\tif strings.ToLower(host) == \"localhost\" {\n+\t\treturn true\n+\t}\n+\n+\tif strings.HasSuffix(strings.ToLower(host), \".docker.internal\") {\n \t\treturn true\n \t}\n-\treturn ip.IsLoopback()\n+\n+\treturn false\n }\n \n func (c *clusterState) slotMasterNode(slot int) (*clusterNode, error) {\n@@ -730,7 +846,8 @@ func (c *clusterState) slotSlaveNode(slot int) (*clusterNode, error) {\n \tcase 1:\n \t\treturn nodes[0], nil\n \tcase 2:\n-\t\tif slave := nodes[1]; !slave.Failing() {\n+\t\tslave := nodes[1]\n+\t\tif !slave.Failing() && !slave.Loading() {\n \t\t\treturn slave, nil\n \t\t}\n \t\treturn nodes[0], nil\n@@ -739,7 +856,7 @@ func (c *clusterState) slotSlaveNode(slot int) (*clusterNode, error) {\n \t\tfor i := 0; i < 10; i++ {\n \t\t\tn := rand.Intn(len(nodes)-1) + 1\n \t\t\tslave = nodes[n]\n-\t\t\tif !slave.Failing() {\n+\t\t\tif !slave.Failing() && !slave.Loading() {\n \t\t\t\treturn slave, nil\n \t\t\t}\n \t\t}\n@@ -900,6 +1017,9 @@ type ClusterClient struct {\n // NewClusterClient returns a Redis Cluster client as described in\n // http://redis.io/topics/cluster-spec.\n func NewClusterClient(opt *ClusterOptions) *ClusterClient {\n+\tif opt == nil {\n+\t\tpanic(\"redis: NewClusterClient nil options\")\n+\t}\n \topt.init()\n \n \tc := &ClusterClient{\n@@ -940,21 +1060,14 @@ func (c *ClusterClient) Close() error {\n \treturn c.nodes.Close()\n }\n \n-// Do create a Cmd from the args and processes the cmd.\n-func (c *ClusterClient) Do(ctx context.Context, args ...interface{}) *Cmd {\n-\tcmd := NewCmd(ctx, args...)\n-\t_ = c.Process(ctx, cmd)\n-\treturn cmd\n-}\n-\n func (c *ClusterClient) Process(ctx context.Context, cmd Cmder) error {\n \terr := c.processHook(ctx, cmd)\n \tcmd.SetErr(err)\n \treturn err\n }\n \n func (c *ClusterClient) process(ctx context.Context, cmd Cmder) error {\n-\tslot := c.cmdSlot(ctx, cmd)\n+\tslot := c.cmdSlot(cmd, -1)\n \tvar node *clusterNode\n \tvar moved bool\n \tvar ask bool\n@@ -1229,7 +1342,7 @@ func (c *ClusterClient) loadState(ctx context.Context) (*clusterState, error) {\n \t\t\tcontinue\n \t\t}\n \n-\t\treturn newClusterState(c.nodes, slots, node.Client.opt.Addr)\n+\t\treturn newClusterState(c.nodes, slots, addr)\n \t}\n \n \t/*\n@@ -1300,9 +1413,13 @@ func (c *ClusterClient) mapCmdsByNode(ctx context.Context, cmdsMap *cmdsMap, cmd\n \t\treturn err\n \t}\n \n+\tpreferredRandomSlot := -1\n \tif c.opt.ReadOnly && c.cmdsAreReadOnly(ctx, cmds) {\n \t\tfor _, cmd := range cmds {\n-\t\t\tslot := c.cmdSlot(ctx, cmd)\n+\t\t\tslot := c.cmdSlot(cmd, preferredRandomSlot)\n+\t\t\tif preferredRandomSlot == -1 {\n+\t\t\t\tpreferredRandomSlot = slot\n+\t\t\t}\n \t\t\tnode, err := c.slotReadOnlyNode(state, slot)\n \t\t\tif err != nil {\n \t\t\t\treturn err\n@@ -1313,7 +1430,10 @@ func (c *ClusterClient) mapCmdsByNode(ctx context.Context, cmdsMap *cmdsMap, cmd\n \t}\n \n \tfor _, cmd := range cmds {\n-\t\tslot := c.cmdSlot(ctx, cmd)\n+\t\tslot := c.cmdSlot(cmd, preferredRandomSlot)\n+\t\tif preferredRandomSlot == -1 {\n+\t\t\tpreferredRandomSlot = slot\n+\t\t}\n \t\tnode, err := state.slotMasterNode(slot)\n \t\tif err != nil {\n \t\t\treturn err\n@@ -1339,7 +1459,9 @@ func (c *ClusterClient) processPipelineNode(\n \t_ = node.Client.withProcessPipelineHook(ctx, cmds, func(ctx context.Context, cmds []Cmder) error {\n \t\tcn, err := node.Client.getConn(ctx)\n \t\tif err != nil {\n-\t\t\tnode.MarkAsFailing()\n+\t\t\tif !isContextError(err) {\n+\t\t\t\tnode.MarkAsFailing()\n+\t\t\t}\n \t\t\t_ = c.mapCmdsByNode(ctx, failedCmds, cmds)\n \t\t\tsetCmdsErr(cmds, err)\n \t\t\treturn err\n@@ -1463,58 +1585,88 @@ func (c *ClusterClient) processTxPipeline(ctx context.Context, cmds []Cmder) err\n \t// Trim multi .. exec.\n \tcmds = cmds[1 : len(cmds)-1]\n \n+\tif len(cmds) == 0 {\n+\t\treturn nil\n+\t}\n+\n \tstate, err := c.state.Get(ctx)\n \tif err != nil {\n \t\tsetCmdsErr(cmds, err)\n \t\treturn err\n \t}\n \n-\tcmdsMap := c.mapCmdsBySlot(ctx, cmds)\n-\tfor slot, cmds := range cmdsMap {\n-\t\tnode, err := state.slotMasterNode(slot)\n-\t\tif err != nil {\n-\t\t\tsetCmdsErr(cmds, err)\n-\t\t\tcontinue\n+\tkeyedCmdsBySlot := c.slottedKeyedCommands(cmds)\n+\tslot := -1\n+\tswitch len(keyedCmdsBySlot) {\n+\tcase 0:\n+\t\tslot = hashtag.RandomSlot()\n+\tcase 1:\n+\t\tfor sl := range keyedCmdsBySlot {\n+\t\t\tslot = sl\n+\t\t\tbreak\n \t\t}\n+\tdefault:\n+\t\t// TxPipeline does not support cross slot transaction.\n+\t\tsetCmdsErr(cmds, ErrCrossSlot)\n+\t\treturn ErrCrossSlot\n+\t}\n \n-\t\tcmdsMap := map[*clusterNode][]Cmder{node: cmds}\n-\t\tfor attempt := 0; attempt <= c.opt.MaxRedirects; attempt++ {\n-\t\t\tif attempt > 0 {\n-\t\t\t\tif err := internal.Sleep(ctx, c.retryBackoff(attempt)); err != nil {\n-\t\t\t\t\tsetCmdsErr(cmds, err)\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n+\tnode, err := state.slotMasterNode(slot)\n+\tif err != nil {\n+\t\tsetCmdsErr(cmds, err)\n+\t\treturn err\n+\t}\n+\n+\tcmdsMap := map[*clusterNode][]Cmder{node: cmds}\n+\tfor attempt := 0; attempt <= c.opt.MaxRedirects; attempt++ {\n+\t\tif attempt > 0 {\n+\t\t\tif err := internal.Sleep(ctx, c.retryBackoff(attempt)); err != nil {\n+\t\t\t\tsetCmdsErr(cmds, err)\n+\t\t\t\treturn err\n \t\t\t}\n+\t\t}\n \n-\t\t\tfailedCmds := newCmdsMap()\n-\t\t\tvar wg sync.WaitGroup\n+\t\tfailedCmds := newCmdsMap()\n+\t\tvar wg sync.WaitGroup\n \n-\t\t\tfor node, cmds := range cmdsMap {\n-\t\t\t\twg.Add(1)\n-\t\t\t\tgo func(node *clusterNode, cmds []Cmder) {\n-\t\t\t\t\tdefer wg.Done()\n-\t\t\t\t\tc.processTxPipelineNode(ctx, node, cmds, failedCmds)\n-\t\t\t\t}(node, cmds)\n-\t\t\t}\n+\t\tfor node, cmds := range cmdsMap {\n+\t\t\twg.Add(1)\n+\t\t\tgo func(node *clusterNode, cmds []Cmder) {\n+\t\t\t\tdefer wg.Done()\n+\t\t\t\tc.processTxPipelineNode(ctx, node, cmds, failedCmds)\n+\t\t\t}(node, cmds)\n+\t\t}\n \n-\t\t\twg.Wait()\n-\t\t\tif len(failedCmds.m) == 0 {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\tcmdsMap = failedCmds.m\n+\t\twg.Wait()\n+\t\tif len(failedCmds.m) == 0 {\n+\t\t\tbreak\n \t\t}\n+\t\tcmdsMap = failedCmds.m\n \t}\n \n \treturn cmdsFirstErr(cmds)\n }\n \n-func (c *ClusterClient) mapCmdsBySlot(ctx context.Context, cmds []Cmder) map[int][]Cmder {\n-\tcmdsMap := make(map[int][]Cmder)\n+// slottedKeyedCommands returns a map of slot to commands taking into account\n+// only commands that have keys.\n+func (c *ClusterClient) slottedKeyedCommands(cmds []Cmder) map[int][]Cmder {\n+\tcmdsSlots := map[int][]Cmder{}\n+\n+\tpreferredRandomSlot := -1\n \tfor _, cmd := range cmds {\n-\t\tslot := c.cmdSlot(ctx, cmd)\n-\t\tcmdsMap[slot] = append(cmdsMap[slot], cmd)\n+\t\tif cmdFirstKeyPos(cmd) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tslot := c.cmdSlot(cmd, preferredRandomSlot)\n+\t\tif preferredRandomSlot == -1 {\n+\t\t\tpreferredRandomSlot = slot\n+\t\t}\n+\n+\t\tcmdsSlots[slot] = append(cmdsSlots[slot], cmd)\n \t}\n-\treturn cmdsMap\n+\n+\treturn cmdsSlots\n }\n \n func (c *ClusterClient) processTxPipelineNode(\n@@ -1558,7 +1710,7 @@ func (c *ClusterClient) processTxPipelineNodeConn(\n \t\ttrimmedCmds := cmds[1 : len(cmds)-1]\n \n \t\tif err := c.txPipelineReadQueued(\n-\t\t\tctx, rd, statusCmd, trimmedCmds, failedCmds,\n+\t\t\tctx, node, cn, rd, statusCmd, trimmedCmds, failedCmds,\n \t\t); err != nil {\n \t\t\tsetCmdsErr(cmds, err)\n \n@@ -1570,30 +1722,56 @@ func (c *ClusterClient) processTxPipelineNodeConn(\n \t\t\treturn err\n \t\t}\n \n-\t\treturn pipelineReadCmds(rd, trimmedCmds)\n+\t\treturn node.Client.pipelineReadCmds(ctx, cn, rd, trimmedCmds)\n \t})\n }\n \n func (c *ClusterClient) txPipelineReadQueued(\n \tctx context.Context,\n+\tnode *clusterNode,\n+\tcn *pool.Conn,\n \trd *proto.Reader,\n \tstatusCmd *StatusCmd,\n \tcmds []Cmder,\n \tfailedCmds *cmdsMap,\n ) error {\n \t// Parse queued replies.\n+\t// To be sure there are no buffered push notifications, we process them before reading the reply\n+\tif err := node.Client.processPendingPushNotificationWithReader(ctx, cn, rd); err != nil {\n+\t\t// Log the error but don't fail the command execution\n+\t\t// Push notification processing errors shouldn't break normal Redis operations\n+\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before reading reply: %v\", err)\n+\t}\n \tif err := statusCmd.readReply(rd); err != nil {\n \t\treturn err\n \t}\n \n \tfor _, cmd := range cmds {\n+\t\t// To be sure there are no buffered push notifications, we process them before reading the reply\n+\t\tif err := node.Client.processPendingPushNotificationWithReader(ctx, cn, rd); err != nil {\n+\t\t\t// Log the error but don't fail the command execution\n+\t\t\t// Push notification processing errors shouldn't break normal Redis operations\n+\t\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before reading reply: %v\", err)\n+\t\t}\n \t\terr := statusCmd.readReply(rd)\n-\t\tif err == nil || c.checkMovedErr(ctx, cmd, err, failedCmds) || isRedisError(err) {\n-\t\t\tcontinue\n+\t\tif err != nil {\n+\t\t\tif c.checkMovedErr(ctx, cmd, err, failedCmds) {\n+\t\t\t\t// will be processed later\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tcmd.SetErr(err)\n+\t\t\tif !isRedisError(err) {\n+\t\t\t\treturn err\n+\t\t\t}\n \t\t}\n-\t\treturn err\n \t}\n \n+\t// To be sure there are no buffered push notifications, we process them before reading the reply\n+\tif err := node.Client.processPendingPushNotificationWithReader(ctx, cn, rd); err != nil {\n+\t\t// Log the error but don't fail the command execution\n+\t\t// Push notification processing errors shouldn't break normal Redis operations\n+\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before reading reply: %v\", err)\n+\t}\n \t// Parse number of replies.\n \tline, err := rd.ReadLine()\n \tif err != nil {\n@@ -1699,38 +1877,64 @@ func (c *ClusterClient) Watch(ctx context.Context, fn func(*Tx) error, keys ...s\n \treturn err\n }\n \n+// maintenance notifications won't work here for now\n func (c *ClusterClient) pubSub() *PubSub {\n \tvar node *clusterNode\n \tpubsub := &PubSub{\n \t\topt: c.opt.clientOptions(),\n-\n-\t\tnewConn: func(ctx context.Context, channels []string) (*pool.Conn, error) {\n+\t\tnewConn: func(ctx context.Context, addr string, channels []string) (*pool.Conn, error) {\n \t\t\tif node != nil {\n \t\t\t\tpanic(\"node != nil\")\n \t\t\t}\n \n \t\t\tvar err error\n+\n \t\t\tif len(channels) > 0 {\n \t\t\t\tslot := hashtag.Slot(channels[0])\n-\t\t\t\tnode, err = c.slotMasterNode(ctx, slot)\n+\n+\t\t\t\t// newConn in PubSub is only used for subscription connections, so it is safe to\n+\t\t\t\t// assume that a slave node can always be used when client options specify ReadOnly.\n+\t\t\t\tif c.opt.ReadOnly {\n+\t\t\t\t\tstate, err := c.state.Get(ctx)\n+\t\t\t\t\tif err != nil {\n+\t\t\t\t\t\treturn nil, err\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tnode, err = c.slotReadOnlyNode(state, slot)\n+\t\t\t\t\tif err != nil {\n+\t\t\t\t\t\treturn nil, err\n+\t\t\t\t\t}\n+\t\t\t\t} else {\n+\t\t\t\t\tnode, err = c.slotMasterNode(ctx, slot)\n+\t\t\t\t\tif err != nil {\n+\t\t\t\t\t\treturn nil, err\n+\t\t\t\t\t}\n+\t\t\t\t}\n \t\t\t} else {\n \t\t\t\tnode, err = c.nodes.Random()\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn nil, err\n+\t\t\t\t}\n \t\t\t}\n+\t\t\tcn, err := node.Client.pubSubPool.NewConn(ctx, node.Client.opt.Network, node.Client.opt.Addr, channels)\n \t\t\tif err != nil {\n+\t\t\t\tnode = nil\n \t\t\t\treturn nil, err\n \t\t\t}\n-\n-\t\t\tcn, err := node.Client.newConn(context.TODO())\n+\t\t\t// will return nil if already initialized\n+\t\t\terr = node.Client.initConn(ctx, cn)\n \t\t\tif err != nil {\n+\t\t\t\t_ = cn.Close()\n \t\t\t\tnode = nil\n-\n \t\t\t\treturn nil, err\n \t\t\t}\n-\n+\t\t\tnode.Client.pubSubPool.TrackConn(cn)\n \t\t\treturn cn, nil\n \t\t},\n \t\tcloseConn: func(cn *pool.Conn) error {\n-\t\t\terr := node.Client.connPool.CloseConn(cn)\n+\t\t\t// Untrack connection from PubSubPool\n+\t\t\tnode.Client.pubSubPool.UntrackConn(cn)\n+\t\t\terr := cn.Close()\n \t\t\tnode = nil\n \t\t\treturn err\n \t\t},\n@@ -1829,17 +2033,20 @@ func (c *ClusterClient) cmdInfo(ctx context.Context, name string) *CommandInfo {\n \treturn info\n }\n \n-func (c *ClusterClient) cmdSlot(ctx context.Context, cmd Cmder) int {\n+func (c *ClusterClient) cmdSlot(cmd Cmder, preferredRandomSlot int) int {\n \targs := cmd.Args()\n-\tif args[0] == \"cluster\" && args[1] == \"getkeysinslot\" {\n+\tif args[0] == \"cluster\" && (args[1] == \"getkeysinslot\" || args[1] == \"countkeysinslot\") {\n \t\treturn args[2].(int)\n \t}\n \n-\treturn cmdSlot(cmd, cmdFirstKeyPos(cmd))\n+\treturn cmdSlot(cmd, cmdFirstKeyPos(cmd), preferredRandomSlot)\n }\n \n-func cmdSlot(cmd Cmder, pos int) int {\n+func cmdSlot(cmd Cmder, pos int, preferredRandomSlot int) int {\n \tif pos == 0 {\n+\t\tif preferredRandomSlot != -1 {\n+\t\t\treturn preferredRandomSlot\n+\t\t}\n \t\treturn hashtag.RandomSlot()\n \t}\n \tfirstKey := cmd.stringArg(pos)\n@@ -1909,7 +2116,7 @@ func (c *ClusterClient) MasterForKey(ctx context.Context, key string) (*Client,\n \tif err != nil {\n \t\treturn nil, err\n \t}\n-\treturn node.Client, err\n+\treturn node.Client, nil\n }\n \n func (c *ClusterClient) context(ctx context.Context) context.Context {\n@@ -1919,26 +2126,13 @@ func (c *ClusterClient) context(ctx context.Context) context.Context {\n \treturn context.Background()\n }\n \n-func appendUniqueNode(nodes []*clusterNode, node *clusterNode) []*clusterNode {\n-\tfor _, n := range nodes {\n-\t\tif n == node {\n-\t\t\treturn nodes\n-\t\t}\n-\t}\n-\treturn append(nodes, node)\n-}\n-\n-func appendIfNotExists(ss []string, es ...string) []string {\n-loop:\n-\tfor _, e := range es {\n-\t\tfor _, s := range ss {\n-\t\t\tif s == e {\n-\t\t\t\tcontinue loop\n-\t\t\t}\n+func appendIfNotExist[T comparable](vals []T, newVal T) []T {\n+\tfor _, v := range vals {\n+\t\tif v == newVal {\n+\t\t\treturn vals\n \t\t}\n-\t\tss = append(ss, e)\n \t}\n-\treturn ss\n+\treturn append(vals, newVal)\n }\n \n //------------------------------------------------------------------------------"
    },
    {
      "sha": "567bf121a318f2900955f51a8073dad17be9fb79",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/pipeline.go",
      "status": "modified",
      "additions": 21,
      "deletions": 6,
      "changes": 27,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpipeline.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpipeline.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpipeline.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -7,7 +7,7 @@ import (\n \n type pipelineExecer func(context.Context, []Cmder) error\n \n-// Pipeliner is an mechanism to realise Redis Pipeline technique.\n+// Pipeliner is a mechanism to realise Redis Pipeline technique.\n //\n // Pipelining is a technique to extremely speed up processing by packing\n // operations to batches, send them at once to Redis and read a replies in a\n@@ -23,21 +23,27 @@ type pipelineExecer func(context.Context, []Cmder) error\n type Pipeliner interface {\n \tStatefulCmdable\n \n-\t// Len is to obtain the number of commands in the pipeline that have not yet been executed.\n+\t// Len obtains the number of commands in the pipeline that have not yet been executed.\n \tLen() int\n \n \t// Do is an API for executing any command.\n \t// If a certain Redis command is not yet supported, you can use Do to execute it.\n \tDo(ctx context.Context, args ...interface{}) *Cmd\n \n-\t// Process is to put the commands to be executed into the pipeline buffer.\n+\t// Process queues the cmd for later execution.\n \tProcess(ctx context.Context, cmd Cmder) error\n \n-\t// Discard is to discard all commands in the cache that have not yet been executed.\n+\t// BatchProcess adds multiple commands to be executed into the pipeline buffer.\n+\tBatchProcess(ctx context.Context, cmd ...Cmder) error\n+\n+\t// Discard discards all commands in the pipeline buffer that have not yet been executed.\n \tDiscard()\n \n-\t// Exec is to send all the commands buffered in the pipeline to the redis-server.\n+\t// Exec sends all the commands buffered in the pipeline to the redis server.\n \tExec(ctx context.Context) ([]Cmder, error)\n+\n+\t// Cmds returns the list of queued commands.\n+\tCmds() []Cmder\n }\n \n var _ Pipeliner = (*Pipeline)(nil)\n@@ -76,7 +82,12 @@ func (c *Pipeline) Do(ctx context.Context, args ...interface{}) *Cmd {\n \n // Process queues the cmd for later execution.\n func (c *Pipeline) Process(ctx context.Context, cmd Cmder) error {\n-\tc.cmds = append(c.cmds, cmd)\n+\treturn c.BatchProcess(ctx, cmd)\n+}\n+\n+// BatchProcess queues multiple cmds for later execution.\n+func (c *Pipeline) BatchProcess(ctx context.Context, cmd ...Cmder) error {\n+\tc.cmds = append(c.cmds, cmd...)\n \treturn nil\n }\n \n@@ -119,3 +130,7 @@ func (c *Pipeline) TxPipelined(ctx context.Context, fn func(Pipeliner) error) ([\n func (c *Pipeline) TxPipeline() Pipeliner {\n \treturn c\n }\n+\n+func (c *Pipeline) Cmds() []Cmder {\n+\treturn c.cmds\n+}"
    },
    {
      "sha": "c26e7cadbde520ea467cd59553a8f1688cc1582d",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/probabilistic.go",
      "status": "modified",
      "additions": 68,
      "deletions": 64,
      "changes": 132,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fprobabilistic.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fprobabilistic.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fprobabilistic.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -319,37 +319,69 @@ func (cmd *BFInfoCmd) Result() (BFInfo, error) {\n }\n \n func (cmd *BFInfoCmd) readReply(rd *proto.Reader) (err error) {\n-\tn, err := rd.ReadMapLen()\n+\tresult := BFInfo{}\n+\n+\t// Create a mapping from key names to pointers of struct fields\n+\trespMapping := map[string]*int64{\n+\t\t\"Capacity\":                 &result.Capacity,\n+\t\t\"CAPACITY\":                 &result.Capacity,\n+\t\t\"Size\":                     &result.Size,\n+\t\t\"SIZE\":                     &result.Size,\n+\t\t\"Number of filters\":        &result.Filters,\n+\t\t\"FILTERS\":                  &result.Filters,\n+\t\t\"Number of items inserted\": &result.ItemsInserted,\n+\t\t\"ITEMS\":                    &result.ItemsInserted,\n+\t\t\"Expansion rate\":           &result.ExpansionRate,\n+\t\t\"EXPANSION\":                &result.ExpansionRate,\n+\t}\n+\n+\t// Helper function to read and assign a value based on the key\n+\treadAndAssignValue := func(key string) error {\n+\t\tfieldPtr, exists := respMapping[key]\n+\t\tif !exists {\n+\t\t\treturn fmt.Errorf(\"redis: BLOOM.INFO unexpected key %s\", key)\n+\t\t}\n+\n+\t\t// Read the integer and assign to the field via pointer dereferencing\n+\t\tval, err := rd.ReadInt()\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\t*fieldPtr = val\n+\t\treturn nil\n+\t}\n+\n+\treadType, err := rd.PeekReplyType()\n \tif err != nil {\n \t\treturn err\n \t}\n \n-\tvar key string\n-\tvar result BFInfo\n-\tfor f := 0; f < n; f++ {\n-\t\tkey, err = rd.ReadString()\n+\tif len(cmd.args) > 2 && readType == proto.RespArray {\n+\t\tn, err := rd.ReadArrayLen()\n \t\tif err != nil {\n \t\t\treturn err\n \t\t}\n-\n-\t\tswitch key {\n-\t\tcase \"Capacity\":\n-\t\t\tresult.Capacity, err = rd.ReadInt()\n-\t\tcase \"Size\":\n-\t\t\tresult.Size, err = rd.ReadInt()\n-\t\tcase \"Number of filters\":\n-\t\t\tresult.Filters, err = rd.ReadInt()\n-\t\tcase \"Number of items inserted\":\n-\t\t\tresult.ItemsInserted, err = rd.ReadInt()\n-\t\tcase \"Expansion rate\":\n-\t\t\tresult.ExpansionRate, err = rd.ReadInt()\n-\t\tdefault:\n-\t\t\treturn fmt.Errorf(\"redis: BLOOM.INFO unexpected key %s\", key)\n+\t\tif key, ok := cmd.args[2].(string); ok && n == 1 {\n+\t\t\tif err := readAndAssignValue(key); err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t} else {\n+\t\t\treturn fmt.Errorf(\"redis: BLOOM.INFO invalid argument key type\")\n \t\t}\n-\n+\t} else {\n+\t\tn, err := rd.ReadMapLen()\n \t\tif err != nil {\n \t\t\treturn err\n \t\t}\n+\t\tfor i := 0; i < n; i++ {\n+\t\t\tkey, err := rd.ReadString()\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tif err := readAndAssignValue(key); err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t}\n \t}\n \n \tcmd.val = result\n@@ -1084,18 +1116,14 @@ func (c cmdable) TopKListWithCount(ctx context.Context, key string) *MapStringIn\n // Returns OK on success or an error if the operation could not be completed.\n // For more information - https://redis.io/commands/tdigest.add/\n func (c cmdable) TDigestAdd(ctx context.Context, key string, elements ...float64) *StatusCmd {\n-\targs := make([]interface{}, 2, 2+len(elements))\n+\targs := make([]interface{}, 2+len(elements))\n \targs[0] = \"TDIGEST.ADD\"\n \targs[1] = key\n \n-\t// Convert floatSlice to []interface{}\n-\tinterfaceSlice := make([]interface{}, len(elements))\n \tfor i, v := range elements {\n-\t\tinterfaceSlice[i] = v\n+\t\targs[2+i] = v\n \t}\n \n-\targs = append(args, interfaceSlice...)\n-\n \tcmd := NewStatusCmd(ctx, args...)\n \t_ = c(ctx, cmd)\n \treturn cmd\n@@ -1106,18 +1134,14 @@ func (c cmdable) TDigestAdd(ctx context.Context, key string, elements ...float64\n // Returns an array of floats representing the values at the specified ranks or an error if the operation could not be completed.\n // For more information - https://redis.io/commands/tdigest.byrank/\n func (c cmdable) TDigestByRank(ctx context.Context, key string, rank ...uint64) *FloatSliceCmd {\n-\targs := make([]interface{}, 2, 2+len(rank))\n+\targs := make([]interface{}, 2+len(rank))\n \targs[0] = \"TDIGEST.BYRANK\"\n \targs[1] = key\n \n-\t// Convert uint slice to []interface{}\n-\tinterfaceSlice := make([]interface{}, len(rank))\n-\tfor i, v := range rank {\n-\t\tinterfaceSlice[i] = v\n+\tfor i, r := range rank {\n+\t\targs[2+i] = r\n \t}\n \n-\targs = append(args, interfaceSlice...)\n-\n \tcmd := NewFloatSliceCmd(ctx, args...)\n \t_ = c(ctx, cmd)\n \treturn cmd\n@@ -1128,18 +1152,14 @@ func (c cmdable) TDigestByRank(ctx context.Context, key string, rank ...uint64)\n // Returns an array of floats representing the values at the specified ranks or an error if the operation could not be completed.\n // For more information - https://redis.io/commands/tdigest.byrevrank/\n func (c cmdable) TDigestByRevRank(ctx context.Context, key string, rank ...uint64) *FloatSliceCmd {\n-\targs := make([]interface{}, 2, 2+len(rank))\n+\targs := make([]interface{}, 2+len(rank))\n \targs[0] = \"TDIGEST.BYREVRANK\"\n \targs[1] = key\n \n-\t// Convert uint slice to []interface{}\n-\tinterfaceSlice := make([]interface{}, len(rank))\n-\tfor i, v := range rank {\n-\t\tinterfaceSlice[i] = v\n+\tfor i, r := range rank {\n+\t\targs[2+i] = r\n \t}\n \n-\targs = append(args, interfaceSlice...)\n-\n \tcmd := NewFloatSliceCmd(ctx, args...)\n \t_ = c(ctx, cmd)\n \treturn cmd\n@@ -1150,18 +1170,14 @@ func (c cmdable) TDigestByRevRank(ctx context.Context, key string, rank ...uint6\n // Returns an array of floats representing the CDF values for each element or an error if the operation could not be completed.\n // For more information - https://redis.io/commands/tdigest.cdf/\n func (c cmdable) TDigestCDF(ctx context.Context, key string, elements ...float64) *FloatSliceCmd {\n-\targs := make([]interface{}, 2, 2+len(elements))\n+\targs := make([]interface{}, 2+len(elements))\n \targs[0] = \"TDIGEST.CDF\"\n \targs[1] = key\n \n-\t// Convert floatSlice to []interface{}\n-\tinterfaceSlice := make([]interface{}, len(elements))\n \tfor i, v := range elements {\n-\t\tinterfaceSlice[i] = v\n+\t\targs[2+i] = v\n \t}\n \n-\targs = append(args, interfaceSlice...)\n-\n \tcmd := NewFloatSliceCmd(ctx, args...)\n \t_ = c(ctx, cmd)\n \treturn cmd\n@@ -1344,18 +1360,14 @@ func (c cmdable) TDigestMin(ctx context.Context, key string) *FloatCmd {\n // Returns an array of floats representing the quantile values for each element or an error if the operation could not be completed.\n // For more information - https://redis.io/commands/tdigest.quantile/\n func (c cmdable) TDigestQuantile(ctx context.Context, key string, elements ...float64) *FloatSliceCmd {\n-\targs := make([]interface{}, 2, 2+len(elements))\n+\targs := make([]interface{}, 2+len(elements))\n \targs[0] = \"TDIGEST.QUANTILE\"\n \targs[1] = key\n \n-\t// Convert floatSlice to []interface{}\n-\tinterfaceSlice := make([]interface{}, len(elements))\n \tfor i, v := range elements {\n-\t\tinterfaceSlice[i] = v\n+\t\targs[2+i] = v\n \t}\n \n-\targs = append(args, interfaceSlice...)\n-\n \tcmd := NewFloatSliceCmd(ctx, args...)\n \t_ = c(ctx, cmd)\n \treturn cmd\n@@ -1366,18 +1378,14 @@ func (c cmdable) TDigestQuantile(ctx context.Context, key string, elements ...fl\n // Returns an array of integers representing the rank values for each element or an error if the operation could not be completed.\n // For more information - https://redis.io/commands/tdigest.rank/\n func (c cmdable) TDigestRank(ctx context.Context, key string, values ...float64) *IntSliceCmd {\n-\targs := make([]interface{}, 2, 2+len(values))\n+\targs := make([]interface{}, 2+len(values))\n \targs[0] = \"TDIGEST.RANK\"\n \targs[1] = key\n \n-\t// Convert floatSlice to []interface{}\n-\tinterfaceSlice := make([]interface{}, len(values))\n \tfor i, v := range values {\n-\t\tinterfaceSlice[i] = v\n+\t\targs[i+2] = v\n \t}\n \n-\targs = append(args, interfaceSlice...)\n-\n \tcmd := NewIntSliceCmd(ctx, args...)\n \t_ = c(ctx, cmd)\n \treturn cmd\n@@ -1399,18 +1407,14 @@ func (c cmdable) TDigestReset(ctx context.Context, key string) *StatusCmd {\n // Returns an array of integers representing the reverse rank values for each element or an error if the operation could not be completed.\n // For more information - https://redis.io/commands/tdigest.revrank/\n func (c cmdable) TDigestRevRank(ctx context.Context, key string, values ...float64) *IntSliceCmd {\n-\targs := make([]interface{}, 2, 2+len(values))\n+\targs := make([]interface{}, 2+len(values))\n \targs[0] = \"TDIGEST.REVRANK\"\n \targs[1] = key\n \n-\t// Convert floatSlice to []interface{}\n-\tinterfaceSlice := make([]interface{}, len(values))\n \tfor i, v := range values {\n-\t\tinterfaceSlice[i] = v\n+\t\targs[2+i] = v\n \t}\n \n-\targs = append(args, interfaceSlice...)\n-\n \tcmd := NewIntSliceCmd(ctx, args...)\n \t_ = c(ctx, cmd)\n \treturn cmd"
    },
    {
      "sha": "959a5c45b1a817b234187c9ad4b6f3d0eebeb14b",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/pubsub.go",
      "status": "modified",
      "additions": 78,
      "deletions": 8,
      "changes": 86,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpubsub.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpubsub.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpubsub.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -10,6 +10,7 @@ import (\n \t\"github.com/redis/go-redis/v9/internal\"\n \t\"github.com/redis/go-redis/v9/internal/pool\"\n \t\"github.com/redis/go-redis/v9/internal/proto\"\n+\t\"github.com/redis/go-redis/v9/push\"\n )\n \n // PubSub implements Pub/Sub commands as described in\n@@ -21,7 +22,7 @@ import (\n type PubSub struct {\n \topt *Options\n \n-\tnewConn   func(ctx context.Context, channels []string) (*pool.Conn, error)\n+\tnewConn   func(ctx context.Context, addr string, channels []string) (*pool.Conn, error)\n \tcloseConn func(*pool.Conn) error\n \n \tmu        sync.Mutex\n@@ -38,13 +39,22 @@ type PubSub struct {\n \tchOnce sync.Once\n \tmsgCh  *channel\n \tallCh  *channel\n+\n+\t// Push notification processor for handling generic push notifications\n+\tpushProcessor push.NotificationProcessor\n+\n+\t// Cleanup callback for maintenanceNotifications upgrade tracking\n+\tonClose func()\n }\n \n func (c *PubSub) init() {\n \tc.exit = make(chan struct{})\n }\n \n func (c *PubSub) String() string {\n+\tc.mu.Lock()\n+\tdefer c.mu.Unlock()\n+\n \tchannels := mapKeys(c.channels)\n \tchannels = append(channels, mapKeys(c.patterns)...)\n \tchannels = append(channels, mapKeys(c.schannels)...)\n@@ -66,10 +76,18 @@ func (c *PubSub) conn(ctx context.Context, newChannels []string) (*pool.Conn, er\n \t\treturn c.cn, nil\n \t}\n \n+\tif c.opt.Addr == \"\" {\n+\t\t// TODO(maintenanceNotifications):\n+\t\t// this is probably cluster client\n+\t\t// c.newConn will ignore the addr argument\n+\t\t// will be changed when we have maintenanceNotifications upgrades for cluster clients\n+\t\tc.opt.Addr = internal.RedisNull\n+\t}\n+\n \tchannels := mapKeys(c.channels)\n \tchannels = append(channels, newChannels...)\n \n-\tcn, err := c.newConn(ctx, channels)\n+\tcn, err := c.newConn(ctx, c.opt.Addr, channels)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -150,12 +168,31 @@ func (c *PubSub) releaseConn(ctx context.Context, cn *pool.Conn, err error, allo\n \tif c.cn != cn {\n \t\treturn\n \t}\n+\n+\tif !cn.IsUsable() || cn.ShouldHandoff() {\n+\t\tc.reconnect(ctx, fmt.Errorf(\"pubsub: connection is not usable\"))\n+\t}\n+\n \tif isBadConn(err, allowTimeout, c.opt.Addr) {\n \t\tc.reconnect(ctx, err)\n \t}\n }\n \n func (c *PubSub) reconnect(ctx context.Context, reason error) {\n+\tif c.cn != nil && c.cn.ShouldHandoff() {\n+\t\tnewEndpoint := c.cn.GetHandoffEndpoint()\n+\t\t// If new endpoint is NULL, use the original address\n+\t\tif newEndpoint == internal.RedisNull {\n+\t\t\tnewEndpoint = c.opt.Addr\n+\t\t}\n+\n+\t\tif newEndpoint != \"\" {\n+\t\t\t// Update the address in the options\n+\t\t\toldAddr := c.cn.RemoteAddr().String()\n+\t\t\tc.opt.Addr = newEndpoint\n+\t\t\tinternal.Logger.Printf(ctx, \"pubsub: reconnecting to new endpoint %s (was %s)\", newEndpoint, oldAddr)\n+\t\t}\n+\t}\n \t_ = c.closeTheCn(reason)\n \t_, _ = c.conn(ctx, nil)\n }\n@@ -164,9 +201,6 @@ func (c *PubSub) closeTheCn(reason error) error {\n \tif c.cn == nil {\n \t\treturn nil\n \t}\n-\tif !c.closed {\n-\t\tinternal.Logger.Printf(c.getContext(), \"redis: discarding bad PubSub connection: %s\", reason)\n-\t}\n \terr := c.closeConn(c.cn)\n \tc.cn = nil\n \treturn err\n@@ -182,6 +216,11 @@ func (c *PubSub) Close() error {\n \tc.closed = true\n \tclose(c.exit)\n \n+\t// Call cleanup callback if set\n+\tif c.onClose != nil {\n+\t\tc.onClose()\n+\t}\n+\n \treturn c.closeTheCn(pool.ErrClosed)\n }\n \n@@ -426,16 +465,20 @@ func (c *PubSub) ReceiveTimeout(ctx context.Context, timeout time.Duration) (int\n \t}\n \n \t// Don't hold the lock to allow subscriptions and pings.\n-\n \tcn, err := c.connWithLock(ctx)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n \n-\terr = cn.WithReader(context.Background(), timeout, func(rd *proto.Reader) error {\n+\terr = cn.WithReader(ctx, timeout, func(rd *proto.Reader) error {\n+\t\t// To be sure there are no buffered push notifications, we process them before reading the reply\n+\t\tif err := c.processPendingPushNotificationWithReader(ctx, cn, rd); err != nil {\n+\t\t\t// Log the error but don't fail the command execution\n+\t\t\t// Push notification processing errors shouldn't break normal Redis operations\n+\t\t\tinternal.Logger.Printf(ctx, \"push: conn[%d] error processing pending notifications before reading reply: %v\", cn.GetID(), err)\n+\t\t}\n \t\treturn c.cmd.readReply(rd)\n \t})\n-\n \tc.releaseConnWithLock(ctx, cn, err, timeout > 0)\n \n \tif err != nil {\n@@ -448,6 +491,12 @@ func (c *PubSub) ReceiveTimeout(ctx context.Context, timeout time.Duration) (int\n // Receive returns a message as a Subscription, Message, Pong or error.\n // See PubSub example for details. This is low-level API and in most cases\n // Channel should be used instead.\n+// Receive returns a message as a Subscription, Message, Pong, or an error.\n+// See PubSub example for details. This is a low-level API and in most cases\n+// Channel should be used instead.\n+// This method blocks until a message is received or an error occurs.\n+// It may return early with an error if the context is canceled, the connection fails,\n+// or other internal errors occur.\n func (c *PubSub) Receive(ctx context.Context) (interface{}, error) {\n \treturn c.ReceiveTimeout(ctx, 0)\n }\n@@ -529,6 +578,27 @@ func (c *PubSub) ChannelWithSubscriptions(opts ...ChannelOption) <-chan interfac\n \treturn c.allCh.allCh\n }\n \n+func (c *PubSub) processPendingPushNotificationWithReader(ctx context.Context, cn *pool.Conn, rd *proto.Reader) error {\n+\t// Only process push notifications for RESP3 connections with a processor\n+\tif c.opt.Protocol != 3 || c.pushProcessor == nil {\n+\t\treturn nil\n+\t}\n+\n+\t// Create handler context with client, connection pool, and connection information\n+\thandlerCtx := c.pushNotificationHandlerContext(cn)\n+\treturn c.pushProcessor.ProcessPendingNotifications(ctx, handlerCtx, rd)\n+}\n+\n+func (c *PubSub) pushNotificationHandlerContext(cn *pool.Conn) push.NotificationHandlerContext {\n+\t// PubSub doesn't have a client or connection pool, so we pass nil for those\n+\t// PubSub connections are blocking\n+\treturn push.NotificationHandlerContext{\n+\t\tPubSub:     c,\n+\t\tConn:       cn,\n+\t\tIsBlocking: true,\n+\t}\n+}\n+\n type ChannelOption func(c *channel)\n \n // WithChannelSize specifies the Go chan size that is used to buffer incoming messages."
    },
    {
      "sha": "c10c98aa8612b58e6fd03d4861c8253f4671250f",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/push/errors.go",
      "status": "added",
      "additions": 176,
      "deletions": 0,
      "changes": 176,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Ferrors.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Ferrors.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Ferrors.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,176 @@\n+package push\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+)\n+\n+// Push notification error definitions\n+// This file contains all error types and messages used by the push notification system\n+\n+// Error reason constants\n+const (\n+\t// HandlerReasons\n+\tReasonHandlerNil       = \"handler cannot be nil\"\n+\tReasonHandlerExists    = \"cannot overwrite existing handler\"\n+\tReasonHandlerProtected = \"handler is protected\"\n+\n+\t// ProcessorReasons\n+\tReasonPushNotificationsDisabled = \"push notifications are disabled\"\n+)\n+\n+// ProcessorType represents the type of processor involved in the error\n+// defined as a custom type for better readability and easier maintenance\n+type ProcessorType string\n+\n+const (\n+\t// ProcessorTypes\n+\tProcessorTypeProcessor     = ProcessorType(\"processor\")\n+\tProcessorTypeVoidProcessor = ProcessorType(\"void_processor\")\n+\tProcessorTypeCustom        = ProcessorType(\"custom\")\n+)\n+\n+// ProcessorOperation represents the operation being performed by the processor\n+// defined as a custom type for better readability and easier maintenance\n+type ProcessorOperation string\n+\n+const (\n+\t// ProcessorOperations\n+\tProcessorOperationProcess    = ProcessorOperation(\"process\")\n+\tProcessorOperationRegister   = ProcessorOperation(\"register\")\n+\tProcessorOperationUnregister = ProcessorOperation(\"unregister\")\n+\tProcessorOperationUnknown    = ProcessorOperation(\"unknown\")\n+)\n+\n+// Common error variables for reuse\n+var (\n+\t// ErrHandlerNil is returned when attempting to register a nil handler\n+\tErrHandlerNil = errors.New(ReasonHandlerNil)\n+)\n+\n+// Registry errors\n+\n+// ErrHandlerExists creates an error for when attempting to overwrite an existing handler\n+func ErrHandlerExists(pushNotificationName string) error {\n+\treturn NewHandlerError(ProcessorOperationRegister, pushNotificationName, ReasonHandlerExists, nil)\n+}\n+\n+// ErrProtectedHandler creates an error for when attempting to unregister a protected handler\n+func ErrProtectedHandler(pushNotificationName string) error {\n+\treturn NewHandlerError(ProcessorOperationUnregister, pushNotificationName, ReasonHandlerProtected, nil)\n+}\n+\n+// VoidProcessor errors\n+\n+// ErrVoidProcessorRegister creates an error for when attempting to register a handler on void processor\n+func ErrVoidProcessorRegister(pushNotificationName string) error {\n+\treturn NewProcessorError(ProcessorTypeVoidProcessor, ProcessorOperationRegister, pushNotificationName, ReasonPushNotificationsDisabled, nil)\n+}\n+\n+// ErrVoidProcessorUnregister creates an error for when attempting to unregister a handler on void processor\n+func ErrVoidProcessorUnregister(pushNotificationName string) error {\n+\treturn NewProcessorError(ProcessorTypeVoidProcessor, ProcessorOperationUnregister, pushNotificationName, ReasonPushNotificationsDisabled, nil)\n+}\n+\n+// Error type definitions for advanced error handling\n+\n+// HandlerError represents errors related to handler operations\n+type HandlerError struct {\n+\tOperation            ProcessorOperation\n+\tPushNotificationName string\n+\tReason               string\n+\tErr                  error\n+}\n+\n+func (e *HandlerError) Error() string {\n+\tif e.Err != nil {\n+\t\treturn fmt.Sprintf(\"handler %s failed for '%s': %s (%v)\", e.Operation, e.PushNotificationName, e.Reason, e.Err)\n+\t}\n+\treturn fmt.Sprintf(\"handler %s failed for '%s': %s\", e.Operation, e.PushNotificationName, e.Reason)\n+}\n+\n+func (e *HandlerError) Unwrap() error {\n+\treturn e.Err\n+}\n+\n+// NewHandlerError creates a new HandlerError\n+func NewHandlerError(operation ProcessorOperation, pushNotificationName, reason string, err error) *HandlerError {\n+\treturn &HandlerError{\n+\t\tOperation:            operation,\n+\t\tPushNotificationName: pushNotificationName,\n+\t\tReason:               reason,\n+\t\tErr:                  err,\n+\t}\n+}\n+\n+// ProcessorError represents errors related to processor operations\n+type ProcessorError struct {\n+\tProcessorType        ProcessorType      // \"processor\", \"void_processor\"\n+\tOperation            ProcessorOperation // \"process\", \"register\", \"unregister\"\n+\tPushNotificationName string             // Name of the push notification involved\n+\tReason               string\n+\tErr                  error\n+}\n+\n+func (e *ProcessorError) Error() string {\n+\tnotifInfo := \"\"\n+\tif e.PushNotificationName != \"\" {\n+\t\tnotifInfo = fmt.Sprintf(\" for '%s'\", e.PushNotificationName)\n+\t}\n+\tif e.Err != nil {\n+\t\treturn fmt.Sprintf(\"%s %s failed%s: %s (%v)\", e.ProcessorType, e.Operation, notifInfo, e.Reason, e.Err)\n+\t}\n+\treturn fmt.Sprintf(\"%s %s failed%s: %s\", e.ProcessorType, e.Operation, notifInfo, e.Reason)\n+}\n+\n+func (e *ProcessorError) Unwrap() error {\n+\treturn e.Err\n+}\n+\n+// NewProcessorError creates a new ProcessorError\n+func NewProcessorError(processorType ProcessorType, operation ProcessorOperation, pushNotificationName, reason string, err error) *ProcessorError {\n+\treturn &ProcessorError{\n+\t\tProcessorType:        processorType,\n+\t\tOperation:            operation,\n+\t\tPushNotificationName: pushNotificationName,\n+\t\tReason:               reason,\n+\t\tErr:                  err,\n+\t}\n+}\n+\n+// Helper functions for common error scenarios\n+\n+// IsHandlerNilError checks if an error is due to a nil handler\n+func IsHandlerNilError(err error) bool {\n+\treturn errors.Is(err, ErrHandlerNil)\n+}\n+\n+// IsHandlerExistsError checks if an error is due to attempting to overwrite an existing handler.\n+// This function works correctly even when the error is wrapped.\n+func IsHandlerExistsError(err error) bool {\n+\tvar handlerErr *HandlerError\n+\tif errors.As(err, &handlerErr) {\n+\t\treturn handlerErr.Operation == ProcessorOperationRegister && handlerErr.Reason == ReasonHandlerExists\n+\t}\n+\treturn false\n+}\n+\n+// IsProtectedHandlerError checks if an error is due to attempting to unregister a protected handler.\n+// This function works correctly even when the error is wrapped.\n+func IsProtectedHandlerError(err error) bool {\n+\tvar handlerErr *HandlerError\n+\tif errors.As(err, &handlerErr) {\n+\t\treturn handlerErr.Operation == ProcessorOperationUnregister && handlerErr.Reason == ReasonHandlerProtected\n+\t}\n+\treturn false\n+}\n+\n+// IsVoidProcessorError checks if an error is due to void processor operations.\n+// This function works correctly even when the error is wrapped.\n+func IsVoidProcessorError(err error) bool {\n+\tvar procErr *ProcessorError\n+\tif errors.As(err, &procErr) {\n+\t\treturn procErr.ProcessorType == ProcessorTypeVoidProcessor && procErr.Reason == ReasonPushNotificationsDisabled\n+\t}\n+\treturn false\n+}"
    },
    {
      "sha": "815edce378444fa5e896c65ffbca0c89895ebbc4",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/push/handler.go",
      "status": "added",
      "additions": 14,
      "deletions": 0,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fhandler.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fhandler.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fhandler.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,14 @@\n+package push\n+\n+import (\n+\t\"context\"\n+)\n+\n+// NotificationHandler defines the interface for push notification handlers.\n+type NotificationHandler interface {\n+\t// HandlePushNotification processes a push notification with context information.\n+\t// The handlerCtx provides information about the client, connection pool, and connection\n+\t// on which the notification was received, allowing handlers to make informed decisions.\n+\t// Returns an error if the notification could not be handled.\n+\tHandlePushNotification(ctx context.Context, handlerCtx NotificationHandlerContext, notification []interface{}) error\n+}"
    },
    {
      "sha": "c39e186b0daeebae5b1fe72cca5aead6bc38881c",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/push/handler_context.go",
      "status": "added",
      "additions": 44,
      "deletions": 0,
      "changes": 44,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fhandler_context.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fhandler_context.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fhandler_context.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,44 @@\n+package push\n+\n+// No imports needed for this file\n+\n+// NotificationHandlerContext provides context information about where a push notification was received.\n+// This struct allows handlers to make informed decisions based on the source of the notification\n+// with strongly typed access to different client types using concrete types.\n+type NotificationHandlerContext struct {\n+\t// Client is the Redis client instance that received the notification.\n+\t// It is interface to both allow for future expansion and to avoid\n+\t// circular dependencies. The developer is responsible for type assertion.\n+\t// It can be one of the following types:\n+\t// - *redis.baseClient\n+\t// - *redis.Client\n+\t// - *redis.ClusterClient\n+\t// - *redis.Conn\n+\tClient interface{}\n+\n+\t// ConnPool is the connection pool from which the connection was obtained.\n+\t// It is interface to both allow for future expansion and to avoid\n+\t// circular dependencies. The developer is responsible for type assertion.\n+\t// It can be one of the following types:\n+\t// - *pool.ConnPool\n+\t// - *pool.SingleConnPool\n+\t// - *pool.StickyConnPool\n+\tConnPool interface{}\n+\n+\t// PubSub is the PubSub instance that received the notification.\n+\t// It is interface to both allow for future expansion and to avoid\n+\t// circular dependencies. The developer is responsible for type assertion.\n+\t// It can be one of the following types:\n+\t// - *redis.PubSub\n+\tPubSub interface{}\n+\n+\t// Conn is the specific connection on which the notification was received.\n+\t// It is interface to both allow for future expansion and to avoid\n+\t// circular dependencies. The developer is responsible for type assertion.\n+\t// It can be one of the following types:\n+\t// - *pool.Conn\n+\tConn interface{}\n+\n+\t// IsBlocking indicates if the notification was received on a blocking connection.\n+\tIsBlocking bool\n+}"
    },
    {
      "sha": "b8112ddc83d74c34ccfa017da1fde11eb161b53e",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/push/processor.go",
      "status": "added",
      "additions": 203,
      "deletions": 0,
      "changes": 203,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fprocessor.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fprocessor.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fprocessor.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,203 @@\n+package push\n+\n+import (\n+\t\"context\"\n+\n+\t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/proto\"\n+)\n+\n+// NotificationProcessor defines the interface for push notification processors.\n+type NotificationProcessor interface {\n+\t// GetHandler returns the handler for a specific push notification name.\n+\tGetHandler(pushNotificationName string) NotificationHandler\n+\t// ProcessPendingNotifications checks for and processes any pending push notifications.\n+\t// To be used when it is known that there are notifications on the socket.\n+\t// It will try to read from the socket and if it is empty - it may block.\n+\tProcessPendingNotifications(ctx context.Context, handlerCtx NotificationHandlerContext, rd *proto.Reader) error\n+\t// RegisterHandler registers a handler for a specific push notification name.\n+\tRegisterHandler(pushNotificationName string, handler NotificationHandler, protected bool) error\n+\t// UnregisterHandler removes a handler for a specific push notification name.\n+\tUnregisterHandler(pushNotificationName string) error\n+}\n+\n+// Processor handles push notifications with a registry of handlers\n+type Processor struct {\n+\tregistry *Registry\n+}\n+\n+// NewProcessor creates a new push notification processor\n+func NewProcessor() *Processor {\n+\treturn &Processor{\n+\t\tregistry: NewRegistry(),\n+\t}\n+}\n+\n+// GetHandler returns the handler for a specific push notification name\n+func (p *Processor) GetHandler(pushNotificationName string) NotificationHandler {\n+\treturn p.registry.GetHandler(pushNotificationName)\n+}\n+\n+// RegisterHandler registers a handler for a specific push notification name\n+func (p *Processor) RegisterHandler(pushNotificationName string, handler NotificationHandler, protected bool) error {\n+\treturn p.registry.RegisterHandler(pushNotificationName, handler, protected)\n+}\n+\n+// UnregisterHandler removes a handler for a specific push notification name\n+func (p *Processor) UnregisterHandler(pushNotificationName string) error {\n+\treturn p.registry.UnregisterHandler(pushNotificationName)\n+}\n+\n+// ProcessPendingNotifications checks for and processes any pending push notifications\n+// This method should be called by the client in WithReader before reading the reply\n+// It will try to read from the socket and if it is empty - it may block.\n+func (p *Processor) ProcessPendingNotifications(ctx context.Context, handlerCtx NotificationHandlerContext, rd *proto.Reader) error {\n+\tif rd == nil {\n+\t\treturn nil\n+\t}\n+\n+\tfor {\n+\t\t// Check if there's data available to read\n+\t\treplyType, err := rd.PeekReplyType()\n+\t\tif err != nil {\n+\t\t\t// No more data available or error reading\n+\t\t\t// if timeout, it will be handled by the caller\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// Only process push notifications (arrays starting with >)\n+\t\tif replyType != proto.RespPush {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// see if we should skip this notification\n+\t\tnotificationName, err := rd.PeekPushNotificationName()\n+\t\tif err != nil {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tif willHandleNotificationInClient(notificationName) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// Read the push notification\n+\t\treply, err := rd.ReadReply()\n+\t\tif err != nil {\n+\t\t\tinternal.Logger.Printf(ctx, \"push: error reading push notification: %v\", err)\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// Convert to slice of interfaces\n+\t\tnotification, ok := reply.([]interface{})\n+\t\tif !ok {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// Handle the notification directly\n+\t\tif len(notification) > 0 {\n+\t\t\t// Extract the notification type (first element)\n+\t\t\tif notificationType, ok := notification[0].(string); ok {\n+\t\t\t\t// Get the handler for this notification type\n+\t\t\t\tif handler := p.registry.GetHandler(notificationType); handler != nil {\n+\t\t\t\t\t// Handle the notification\n+\t\t\t\t\terr := handler.HandlePushNotification(ctx, handlerCtx, notification)\n+\t\t\t\t\tif err != nil {\n+\t\t\t\t\t\tinternal.Logger.Printf(ctx, \"push: error handling push notification: %v\", err)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn nil\n+}\n+\n+// VoidProcessor discards all push notifications without processing them\n+type VoidProcessor struct{}\n+\n+// NewVoidProcessor creates a new void push notification processor\n+func NewVoidProcessor() *VoidProcessor {\n+\treturn &VoidProcessor{}\n+}\n+\n+// GetHandler returns nil for void processor since it doesn't maintain handlers\n+func (v *VoidProcessor) GetHandler(_ string) NotificationHandler {\n+\treturn nil\n+}\n+\n+// RegisterHandler returns an error for void processor since it doesn't maintain handlers\n+func (v *VoidProcessor) RegisterHandler(pushNotificationName string, _ NotificationHandler, _ bool) error {\n+\treturn ErrVoidProcessorRegister(pushNotificationName)\n+}\n+\n+// UnregisterHandler returns an error for void processor since it doesn't maintain handlers\n+func (v *VoidProcessor) UnregisterHandler(pushNotificationName string) error {\n+\treturn ErrVoidProcessorUnregister(pushNotificationName)\n+}\n+\n+// ProcessPendingNotifications for VoidProcessor does nothing since push notifications\n+// are only available in RESP3 and this processor is used for RESP2 connections.\n+// This avoids unnecessary buffer scanning overhead.\n+// It does however read and discard all push notifications from the buffer to avoid\n+// them being interpreted as a reply.\n+// This method should be called by the client in WithReader before reading the reply\n+// to be sure there are no buffered push notifications.\n+// It will try to read from the socket and if it is empty - it may block.\n+func (v *VoidProcessor) ProcessPendingNotifications(_ context.Context, handlerCtx NotificationHandlerContext, rd *proto.Reader) error {\n+\t// read and discard all push notifications\n+\tif rd == nil {\n+\t\treturn nil\n+\t}\n+\n+\tfor {\n+\t\t// Check if there's data available to read\n+\t\treplyType, err := rd.PeekReplyType()\n+\t\tif err != nil {\n+\t\t\t// No more data available or error reading\n+\t\t\t// if timeout, it will be handled by the caller\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// Only process push notifications (arrays starting with >)\n+\t\tif replyType != proto.RespPush {\n+\t\t\tbreak\n+\t\t}\n+\t\t// see if we should skip this notification\n+\t\tnotificationName, err := rd.PeekPushNotificationName()\n+\t\tif err != nil {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tif willHandleNotificationInClient(notificationName) {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// Read the push notification\n+\t\t_, err = rd.ReadReply()\n+\t\tif err != nil {\n+\t\t\tinternal.Logger.Printf(context.Background(), \"push: error reading push notification: %v\", err)\n+\t\t\treturn nil\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// willHandleNotificationInClient checks if a notification type should be ignored by the push notification\n+// processor and handled by other specialized systems instead (pub/sub, streams, keyspace, etc.).\n+func willHandleNotificationInClient(notificationType string) bool {\n+\tswitch notificationType {\n+\t// Pub/Sub notifications - handled by pub/sub system\n+\tcase \"message\", // Regular pub/sub message\n+\t\t\"pmessage\",     // Pattern pub/sub message\n+\t\t\"subscribe\",    // Subscription confirmation\n+\t\t\"unsubscribe\",  // Unsubscription confirmation\n+\t\t\"psubscribe\",   // Pattern subscription confirmation\n+\t\t\"punsubscribe\", // Pattern unsubscription confirmation\n+\t\t\"smessage\",     // Sharded pub/sub message (Redis 7.0+)\n+\t\t\"ssubscribe\",   // Sharded subscription confirmation\n+\t\t\"sunsubscribe\": // Sharded unsubscription confirmation\n+\t\treturn true\n+\tdefault:\n+\t\treturn false\n+\t}\n+}"
    },
    {
      "sha": "e6adeaa456cb9b60a4b621fed2b1cfb79c75f23b",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/push/push.go",
      "status": "added",
      "additions": 7,
      "deletions": 0,
      "changes": 7,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fpush.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fpush.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fpush.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,7 @@\n+// Package push provides push notifications for Redis.\n+// This is an EXPERIMENTAL API for handling push notifications from Redis.\n+// It is not yet stable and may change in the future.\n+// Although this is in a public package, in its current form public use is not advised.\n+// Pending push notifications should be processed before executing any readReply from the connection\n+// as per RESP3 specification push notifications can be sent at any time.\n+package push"
    },
    {
      "sha": "a265ae92f98bcdb7c25f1383eaa33549727e5f98",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/push/registry.go",
      "status": "added",
      "additions": 61,
      "deletions": 0,
      "changes": 61,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fregistry.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fregistry.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush%2Fregistry.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,61 @@\n+package push\n+\n+import (\n+\t\"sync\"\n+)\n+\n+// Registry manages push notification handlers\n+type Registry struct {\n+\tmu        sync.RWMutex\n+\thandlers  map[string]NotificationHandler\n+\tprotected map[string]bool\n+}\n+\n+// NewRegistry creates a new push notification registry\n+func NewRegistry() *Registry {\n+\treturn &Registry{\n+\t\thandlers:  make(map[string]NotificationHandler),\n+\t\tprotected: make(map[string]bool),\n+\t}\n+}\n+\n+// RegisterHandler registers a handler for a specific push notification name\n+func (r *Registry) RegisterHandler(pushNotificationName string, handler NotificationHandler, protected bool) error {\n+\tif handler == nil {\n+\t\treturn ErrHandlerNil\n+\t}\n+\n+\tr.mu.Lock()\n+\tdefer r.mu.Unlock()\n+\n+\t// Check if handler already exists\n+\tif _, exists := r.protected[pushNotificationName]; exists {\n+\t\treturn ErrHandlerExists(pushNotificationName)\n+\t}\n+\n+\tr.handlers[pushNotificationName] = handler\n+\tr.protected[pushNotificationName] = protected\n+\treturn nil\n+}\n+\n+// GetHandler returns the handler for a specific push notification name\n+func (r *Registry) GetHandler(pushNotificationName string) NotificationHandler {\n+\tr.mu.RLock()\n+\tdefer r.mu.RUnlock()\n+\treturn r.handlers[pushNotificationName]\n+}\n+\n+// UnregisterHandler removes a handler for a specific push notification name\n+func (r *Registry) UnregisterHandler(pushNotificationName string) error {\n+\tr.mu.Lock()\n+\tdefer r.mu.Unlock()\n+\n+\t// Check if handler is protected\n+\tif protected, exists := r.protected[pushNotificationName]; exists && protected {\n+\t\treturn ErrProtectedHandler(pushNotificationName)\n+\t}\n+\n+\tdelete(r.handlers, pushNotificationName)\n+\tdelete(r.protected, pushNotificationName)\n+\treturn nil\n+}"
    },
    {
      "sha": "572955fecbbb1ae927cfa1925b74770daf7597d5",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/push_notifications.go",
      "status": "added",
      "additions": 21,
      "deletions": 0,
      "changes": 21,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush_notifications.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush_notifications.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fpush_notifications.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,21 @@\n+package redis\n+\n+import (\n+\t\"github.com/redis/go-redis/v9/push\"\n+)\n+\n+// NewPushNotificationProcessor creates a new push notification processor\n+// This processor maintains a registry of handlers and processes push notifications\n+// It is used for RESP3 connections where push notifications are available\n+func NewPushNotificationProcessor() push.NotificationProcessor {\n+\treturn push.NewProcessor()\n+}\n+\n+// NewVoidPushNotificationProcessor creates a new void push notification processor\n+// This processor does not maintain any handlers and always returns nil for all operations\n+// It is used for RESP2 connections where push notifications are not available\n+// It can also be used to disable push notifications for RESP3 connections, where\n+// it will discard all push notifications without processing them\n+func NewVoidPushNotificationProcessor() push.NotificationProcessor {\n+\treturn push.NewVoidProcessor()\n+}"
    },
    {
      "sha": "a6a710677981e7b4a844f724a52c30b9cdf6d9ee",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/redis.go",
      "status": "modified",
      "additions": 657,
      "deletions": 93,
      "changes": 750,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fredis.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fredis.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fredis.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -9,10 +9,14 @@ import (\n \t\"sync/atomic\"\n \t\"time\"\n \n+\t\"github.com/redis/go-redis/v9/auth\"\n \t\"github.com/redis/go-redis/v9/internal\"\n+\t\"github.com/redis/go-redis/v9/internal/auth/streaming\"\n \t\"github.com/redis/go-redis/v9/internal/hscan\"\n \t\"github.com/redis/go-redis/v9/internal/pool\"\n \t\"github.com/redis/go-redis/v9/internal/proto\"\n+\t\"github.com/redis/go-redis/v9/maintnotifications\"\n+\t\"github.com/redis/go-redis/v9/push\"\n )\n \n // Scanner internal/hscan.Scanner exposed interface.\n@@ -22,10 +26,16 @@ type Scanner = hscan.Scanner\n const Nil = proto.Nil\n \n // SetLogger set custom log\n+// Use with VoidLogger to disable logging.\n func SetLogger(logger internal.Logging) {\n \tinternal.Logger = logger\n }\n \n+// SetLogLevel sets the log level for the library.\n+func SetLogLevel(logLevel internal.LogLevelT) {\n+\tinternal.LogLevel = logLevel\n+}\n+\n //------------------------------------------------------------------------------\n \n type Hook interface {\n@@ -201,15 +211,39 @@ func (hs *hooksMixin) processTxPipelineHook(ctx context.Context, cmds []Cmder) e\n //------------------------------------------------------------------------------\n \n type baseClient struct {\n-\topt      *Options\n-\tconnPool pool.Pooler\n+\topt        *Options\n+\toptLock    sync.RWMutex\n+\tconnPool   pool.Pooler\n+\tpubSubPool *pool.PubSubPool\n+\thooksMixin\n \n \tonClose func() error // hook called when client is closed\n+\n+\t// Push notification processing\n+\tpushProcessor push.NotificationProcessor\n+\n+\t// Maintenance notifications manager\n+\tmaintNotificationsManager     *maintnotifications.Manager\n+\tmaintNotificationsManagerLock sync.RWMutex\n+\n+\t// streamingCredentialsManager is used to manage streaming credentials\n+\tstreamingCredentialsManager *streaming.Manager\n }\n \n func (c *baseClient) clone() *baseClient {\n-\tclone := *c\n-\treturn &clone\n+\tc.maintNotificationsManagerLock.RLock()\n+\tmaintNotificationsManager := c.maintNotificationsManager\n+\tc.maintNotificationsManagerLock.RUnlock()\n+\n+\tclone := &baseClient{\n+\t\topt:                         c.opt,\n+\t\tconnPool:                    c.connPool,\n+\t\tonClose:                     c.onClose,\n+\t\tpushProcessor:               c.pushProcessor,\n+\t\tmaintNotificationsManager:   maintNotificationsManager,\n+\t\tstreamingCredentialsManager: c.streamingCredentialsManager,\n+\t}\n+\treturn clone\n }\n \n func (c *baseClient) withTimeout(timeout time.Duration) *baseClient {\n@@ -227,21 +261,6 @@ func (c *baseClient) String() string {\n \treturn fmt.Sprintf(\"Redis<%s db:%d>\", c.getAddr(), c.opt.DB)\n }\n \n-func (c *baseClient) newConn(ctx context.Context) (*pool.Conn, error) {\n-\tcn, err := c.connPool.NewConn(ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\terr = c.initConn(ctx, cn)\n-\tif err != nil {\n-\t\t_ = c.connPool.CloseConn(cn)\n-\t\treturn nil, err\n-\t}\n-\n-\treturn cn, nil\n-}\n-\n func (c *baseClient) getConn(ctx context.Context) (*pool.Conn, error) {\n \tif c.opt.Limiter != nil {\n \t\terr := c.opt.Limiter.Allow()\n@@ -267,7 +286,7 @@ func (c *baseClient) _getConn(ctx context.Context) (*pool.Conn, error) {\n \t\treturn nil, err\n \t}\n \n-\tif cn.Inited {\n+\tif cn.IsInited() {\n \t\treturn cn, nil\n \t}\n \n@@ -279,59 +298,222 @@ func (c *baseClient) _getConn(ctx context.Context) (*pool.Conn, error) {\n \t\treturn nil, err\n \t}\n \n+\t// initConn will transition to IDLE state, so we need to acquire it\n+\t// before returning it to the user.\n+\tif !cn.TryAcquire() {\n+\t\treturn nil, fmt.Errorf(\"redis: connection is not usable\")\n+\t}\n+\n \treturn cn, nil\n }\n \n+func (c *baseClient) reAuthConnection() func(poolCn *pool.Conn, credentials auth.Credentials) error {\n+\treturn func(poolCn *pool.Conn, credentials auth.Credentials) error {\n+\t\tvar err error\n+\t\tusername, password := credentials.BasicAuth()\n+\n+\t\t// Use background context - timeout is handled by ReadTimeout in WithReader/WithWriter\n+\t\tctx := context.Background()\n+\n+\t\tconnPool := pool.NewSingleConnPool(c.connPool, poolCn)\n+\n+\t\t// Pass hooks so that reauth commands are recorded/traced\n+\t\tcn := newConn(c.opt, connPool, &c.hooksMixin)\n+\n+\t\tif username != \"\" {\n+\t\t\terr = cn.AuthACL(ctx, username, password).Err()\n+\t\t} else {\n+\t\t\terr = cn.Auth(ctx, password).Err()\n+\t\t}\n+\n+\t\treturn err\n+\t}\n+}\n+func (c *baseClient) onAuthenticationErr() func(poolCn *pool.Conn, err error) {\n+\treturn func(poolCn *pool.Conn, err error) {\n+\t\tif err != nil {\n+\t\t\tif isBadConn(err, false, c.opt.Addr) {\n+\t\t\t\t// Close the connection to force a reconnection.\n+\t\t\t\terr := c.connPool.CloseConn(poolCn)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\tinternal.Logger.Printf(context.Background(), \"redis: failed to close connection: %v\", err)\n+\t\t\t\t\t// try to close the network connection directly\n+\t\t\t\t\t// so that no resource is leaked\n+\t\t\t\t\terr := poolCn.Close()\n+\t\t\t\t\tif err != nil {\n+\t\t\t\t\t\tinternal.Logger.Printf(context.Background(), \"redis: failed to close network connection: %v\", err)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tinternal.Logger.Printf(context.Background(), \"redis: re-authentication failed: %v\", err)\n+\t\t}\n+\t}\n+}\n+\n+func (c *baseClient) wrappedOnClose(newOnClose func() error) func() error {\n+\tonClose := c.onClose\n+\treturn func() error {\n+\t\tvar firstErr error\n+\t\terr := newOnClose()\n+\t\t// Even if we have an error we would like to execute the onClose hook\n+\t\t// if it exists. We will return the first error that occurred.\n+\t\t// This is to keep error handling consistent with the rest of the code.\n+\t\tif err != nil {\n+\t\t\tfirstErr = err\n+\t\t}\n+\t\tif onClose != nil {\n+\t\t\terr = onClose()\n+\t\t\tif err != nil && firstErr == nil {\n+\t\t\t\tfirstErr = err\n+\t\t\t}\n+\t\t}\n+\t\treturn firstErr\n+\t}\n+}\n+\n func (c *baseClient) initConn(ctx context.Context, cn *pool.Conn) error {\n-\tif cn.Inited {\n+\t// This function is called in two scenarios:\n+\t// 1. First-time init: Connection is in CREATED state (from pool.Get())\n+\t//    - We need to transition CREATED  INITIALIZING and do the initialization\n+\t//    - If another goroutine is already initializing, we WAIT for it to finish\n+\t// 2. Re-initialization: Connection is in INITIALIZING state (from SetNetConnAndInitConn())\n+\t//    - We're already in INITIALIZING, so just proceed with initialization\n+\n+\tcurrentState := cn.GetStateMachine().GetState()\n+\n+\t// Fast path: Check if already initialized (IDLE or IN_USE)\n+\tif currentState == pool.StateIdle || currentState == pool.StateInUse {\n \t\treturn nil\n \t}\n-\tcn.Inited = true\n \n-\tvar err error\n-\tusername, password := c.opt.Username, c.opt.Password\n-\tif c.opt.CredentialsProviderContext != nil {\n-\t\tif username, password, err = c.opt.CredentialsProviderContext(ctx); err != nil {\n+\t// If in CREATED state, try to transition to INITIALIZING\n+\tif currentState == pool.StateCreated {\n+\t\tfinalState, err := cn.GetStateMachine().TryTransition([]pool.ConnState{pool.StateCreated}, pool.StateInitializing)\n+\t\tif err != nil {\n+\t\t\t// Another goroutine is initializing or connection is in unexpected state\n+\t\t\t// Check what state we're in now\n+\t\t\tif finalState == pool.StateIdle || finalState == pool.StateInUse {\n+\t\t\t\t// Already initialized by another goroutine\n+\t\t\t\treturn nil\n+\t\t\t}\n+\n+\t\t\tif finalState == pool.StateInitializing {\n+\t\t\t\t// Another goroutine is initializing - WAIT for it to complete\n+\t\t\t\t// Use a context with timeout = min(remaining command timeout, DialTimeout)\n+\t\t\t\t// This prevents waiting too long while respecting the caller's deadline\n+\t\t\t\tvar waitCtx context.Context\n+\t\t\t\tvar cancel context.CancelFunc\n+\t\t\t\tdialTimeout := c.opt.DialTimeout\n+\n+\t\t\t\tif cmdDeadline, hasCmdDeadline := ctx.Deadline(); hasCmdDeadline {\n+\t\t\t\t\t// Calculate remaining time until command deadline\n+\t\t\t\t\tremainingTime := time.Until(cmdDeadline)\n+\t\t\t\t\t// Use the minimum of remaining time and DialTimeout\n+\t\t\t\t\tif remainingTime < dialTimeout {\n+\t\t\t\t\t\t// Command deadline is sooner, use it\n+\t\t\t\t\t\twaitCtx = ctx\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t// DialTimeout is shorter, cap the wait at DialTimeout\n+\t\t\t\t\t\twaitCtx, cancel = context.WithTimeout(ctx, dialTimeout)\n+\t\t\t\t\t}\n+\t\t\t\t} else {\n+\t\t\t\t\t// No command deadline, use DialTimeout to prevent waiting indefinitely\n+\t\t\t\t\twaitCtx, cancel = context.WithTimeout(ctx, dialTimeout)\n+\t\t\t\t}\n+\t\t\t\tif cancel != nil {\n+\t\t\t\t\tdefer cancel()\n+\t\t\t\t}\n+\n+\t\t\t\tfinalState, err := cn.GetStateMachine().AwaitAndTransition(\n+\t\t\t\t\twaitCtx,\n+\t\t\t\t\t[]pool.ConnState{pool.StateIdle, pool.StateInUse},\n+\t\t\t\t\tpool.StateIdle, // Target is IDLE (but we're already there, so this is a no-op)\n+\t\t\t\t)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn err\n+\t\t\t\t}\n+\t\t\t\t// Verify we're now initialized\n+\t\t\t\tif finalState == pool.StateIdle || finalState == pool.StateInUse {\n+\t\t\t\t\treturn nil\n+\t\t\t\t}\n+\t\t\t\t// Unexpected state after waiting\n+\t\t\t\treturn fmt.Errorf(\"connection in unexpected state after initialization: %s\", finalState)\n+\t\t\t}\n+\n+\t\t\t// Unexpected state (CLOSED, UNUSABLE, etc.)\n \t\t\treturn err\n \t\t}\n-\t} else if c.opt.CredentialsProvider != nil {\n-\t\tusername, password = c.opt.CredentialsProvider()\n \t}\n \n+\t// At this point, we're in INITIALIZING state and we own the initialization\n+\t// If we fail, we must transition to CLOSED\n+\tvar initErr error\n \tconnPool := pool.NewSingleConnPool(c.connPool, cn)\n-\tconn := newConn(c.opt, connPool)\n+\tconn := newConn(c.opt, connPool, &c.hooksMixin)\n+\n+\tusername, password := \"\", \"\"\n+\tif c.opt.StreamingCredentialsProvider != nil {\n+\t\tcredListener, initErr := c.streamingCredentialsManager.Listener(\n+\t\t\tcn,\n+\t\t\tc.reAuthConnection(),\n+\t\t\tc.onAuthenticationErr(),\n+\t\t)\n+\t\tif initErr != nil {\n+\t\t\tcn.GetStateMachine().Transition(pool.StateClosed)\n+\t\t\treturn fmt.Errorf(\"failed to create credentials listener: %w\", initErr)\n+\t\t}\n \n-\tvar auth bool\n-\tprotocol := c.opt.Protocol\n-\t// By default, use RESP3 in current version.\n-\tif protocol < 2 {\n-\t\tprotocol = 3\n+\t\tcredentials, unsubscribeFromCredentialsProvider, initErr := c.opt.StreamingCredentialsProvider.\n+\t\t\tSubscribe(credListener)\n+\t\tif initErr != nil {\n+\t\t\tcn.GetStateMachine().Transition(pool.StateClosed)\n+\t\t\treturn fmt.Errorf(\"failed to subscribe to streaming credentials: %w\", initErr)\n+\t\t}\n+\n+\t\tc.onClose = c.wrappedOnClose(unsubscribeFromCredentialsProvider)\n+\t\tcn.SetOnClose(unsubscribeFromCredentialsProvider)\n+\n+\t\tusername, password = credentials.BasicAuth()\n+\t} else if c.opt.CredentialsProviderContext != nil {\n+\t\tusername, password, initErr = c.opt.CredentialsProviderContext(ctx)\n+\t\tif initErr != nil {\n+\t\t\tcn.GetStateMachine().Transition(pool.StateClosed)\n+\t\t\treturn fmt.Errorf(\"failed to get credentials from context provider: %w\", initErr)\n+\t\t}\n+\t} else if c.opt.CredentialsProvider != nil {\n+\t\tusername, password = c.opt.CredentialsProvider()\n+\t} else if c.opt.Username != \"\" || c.opt.Password != \"\" {\n+\t\tusername, password = c.opt.Username, c.opt.Password\n \t}\n \n \t// for redis-server versions that do not support the HELLO command,\n \t// RESP2 will continue to be used.\n-\tif err = conn.Hello(ctx, protocol, username, password, \"\").Err(); err == nil {\n-\t\tauth = true\n-\t} else if !isRedisError(err) {\n+\tif initErr = conn.Hello(ctx, c.opt.Protocol, username, password, c.opt.ClientName).Err(); initErr == nil {\n+\t\t// Authentication successful with HELLO command\n+\t} else if !isRedisError(initErr) {\n \t\t// When the server responds with the RESP protocol and the result is not a normal\n \t\t// execution result of the HELLO command, we consider it to be an indication that\n \t\t// the server does not support the HELLO command.\n \t\t// The server may be a redis-server that does not support the HELLO command,\n \t\t// or it could be DragonflyDB or a third-party redis-proxy. They all respond\n \t\t// with different error string results for unsupported commands, making it\n \t\t// difficult to rely on error strings to determine all results.\n-\t\treturn err\n-\t}\n-\n-\t_, err = conn.Pipelined(ctx, func(pipe Pipeliner) error {\n-\t\tif !auth && password != \"\" {\n-\t\t\tif username != \"\" {\n-\t\t\t\tpipe.AuthACL(ctx, username, password)\n-\t\t\t} else {\n-\t\t\t\tpipe.Auth(ctx, password)\n-\t\t\t}\n+\t\tcn.GetStateMachine().Transition(pool.StateClosed)\n+\t\treturn initErr\n+\t} else if password != \"\" {\n+\t\t// Try legacy AUTH command if HELLO failed\n+\t\tif username != \"\" {\n+\t\t\tinitErr = conn.AuthACL(ctx, username, password).Err()\n+\t\t} else {\n+\t\t\tinitErr = conn.Auth(ctx, password).Err()\n \t\t}\n+\t\tif initErr != nil {\n+\t\t\tcn.GetStateMachine().Transition(pool.StateClosed)\n+\t\t\treturn fmt.Errorf(\"failed to authenticate: %w\", initErr)\n+\t\t}\n+\t}\n \n+\t_, initErr = conn.Pipelined(ctx, func(pipe Pipeliner) error {\n \t\tif c.opt.DB > 0 {\n \t\t\tpipe.Select(ctx, c.opt.DB)\n \t\t}\n@@ -346,8 +528,58 @@ func (c *baseClient) initConn(ctx context.Context, cn *pool.Conn) error {\n \n \t\treturn nil\n \t})\n-\tif err != nil {\n-\t\treturn err\n+\tif initErr != nil {\n+\t\tcn.GetStateMachine().Transition(pool.StateClosed)\n+\t\treturn fmt.Errorf(\"failed to initialize connection options: %w\", initErr)\n+\t}\n+\n+\t// Enable maintnotifications if maintnotifications are configured\n+\tc.optLock.RLock()\n+\tmaintNotifEnabled := c.opt.MaintNotificationsConfig != nil && c.opt.MaintNotificationsConfig.Mode != maintnotifications.ModeDisabled\n+\tprotocol := c.opt.Protocol\n+\tendpointType := c.opt.MaintNotificationsConfig.EndpointType\n+\tc.optLock.RUnlock()\n+\tvar maintNotifHandshakeErr error\n+\tif maintNotifEnabled && protocol == 3 {\n+\t\tmaintNotifHandshakeErr = conn.ClientMaintNotifications(\n+\t\t\tctx,\n+\t\t\ttrue,\n+\t\t\tendpointType.String(),\n+\t\t).Err()\n+\t\tif maintNotifHandshakeErr != nil {\n+\t\t\tif !isRedisError(maintNotifHandshakeErr) {\n+\t\t\t\t// if not redis error, fail the connection\n+\t\t\t\tcn.GetStateMachine().Transition(pool.StateClosed)\n+\t\t\t\treturn maintNotifHandshakeErr\n+\t\t\t}\n+\t\t\tc.optLock.Lock()\n+\t\t\t// handshake failed - check and modify config atomically\n+\t\t\tswitch c.opt.MaintNotificationsConfig.Mode {\n+\t\t\tcase maintnotifications.ModeEnabled:\n+\t\t\t\t// enabled mode, fail the connection\n+\t\t\t\tc.optLock.Unlock()\n+\t\t\t\tcn.GetStateMachine().Transition(pool.StateClosed)\n+\t\t\t\treturn fmt.Errorf(\"failed to enable maintnotifications: %w\", maintNotifHandshakeErr)\n+\t\t\tdefault: // will handle auto and any other\n+\t\t\t\t// Disabling logging here as it's too noisy.\n+\t\t\t\t// TODO: Enable when we have a better logging solution for log levels\n+\t\t\t\t// internal.Logger.Printf(ctx, \"auto mode fallback: maintnotifications disabled due to handshake error: %v\", maintNotifHandshakeErr)\n+\t\t\t\tc.opt.MaintNotificationsConfig.Mode = maintnotifications.ModeDisabled\n+\t\t\t\tc.optLock.Unlock()\n+\t\t\t\t// auto mode, disable maintnotifications and continue\n+\t\t\t\tif initErr := c.disableMaintNotificationsUpgrades(); initErr != nil {\n+\t\t\t\t\t// Log error but continue - auto mode should be resilient\n+\t\t\t\t\tinternal.Logger.Printf(ctx, \"failed to disable maintnotifications in auto mode: %v\", initErr)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// handshake was executed successfully\n+\t\t\t// to make sure that the handshake will be executed on other connections as well if it was successfully\n+\t\t\t// executed on this connection, we will force the handshake to be executed on all connections\n+\t\t\tc.optLock.Lock()\n+\t\t\tc.opt.MaintNotificationsConfig.Mode = maintnotifications.ModeEnabled\n+\t\t\tc.optLock.Unlock()\n+\t\t}\n \t}\n \n \tif !c.opt.DisableIdentity && !c.opt.DisableIndentity {\n@@ -361,14 +593,33 @@ func (c *baseClient) initConn(ctx context.Context, cn *pool.Conn) error {\n \t\tp.ClientSetInfo(ctx, WithLibraryVersion(libVer))\n \t\t// Handle network errors (e.g. timeouts) in CLIENT SETINFO to avoid\n \t\t// out of order responses later on.\n-\t\tif _, err = p.Exec(ctx); err != nil && !isRedisError(err) {\n-\t\t\treturn err\n+\t\tif _, initErr = p.Exec(ctx); initErr != nil && !isRedisError(initErr) {\n+\t\t\tcn.GetStateMachine().Transition(pool.StateClosed)\n+\t\t\treturn initErr\n \t\t}\n \t}\n \n+\t// Set the connection initialization function for potential reconnections\n+\t// This must be set before transitioning to IDLE so that handoff/reauth can use it\n+\tcn.SetInitConnFunc(c.createInitConnFunc())\n+\n+\t// Initialization succeeded - transition to IDLE state\n+\t// This marks the connection as initialized and ready for use\n+\t// NOTE: The connection is still owned by the calling goroutine at this point\n+\t// and won't be available to other goroutines until it's Put() back into the pool\n+\tcn.GetStateMachine().Transition(pool.StateIdle)\n+\n+\t// Call OnConnect hook if configured\n+\t// The connection is in IDLE state but still owned by this goroutine\n+\t// If OnConnect needs to send commands, it can use the connection safely\n \tif c.opt.OnConnect != nil {\n-\t\treturn c.opt.OnConnect(ctx, conn)\n+\t\tif initErr = c.opt.OnConnect(ctx, conn); initErr != nil {\n+\t\t\t// OnConnect failed - transition to closed\n+\t\t\tcn.GetStateMachine().Transition(pool.StateClosed)\n+\t\t\treturn initErr\n+\t\t}\n \t}\n+\n \treturn nil\n }\n \n@@ -380,6 +631,10 @@ func (c *baseClient) releaseConn(ctx context.Context, cn *pool.Conn, err error)\n \tif isBadConn(err, false, c.opt.Addr) {\n \t\tc.connPool.Remove(ctx, cn, err)\n \t} else {\n+\t\t// process any pending push notifications before returning the connection to the pool\n+\t\tif err := c.processPushNotifications(ctx, cn); err != nil {\n+\t\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before releasing connection: %v\", err)\n+\t\t}\n \t\tc.connPool.Put(ctx, cn)\n \t}\n }\n@@ -421,16 +676,16 @@ func (c *baseClient) process(ctx context.Context, cmd Cmder) error {\n \treturn lastErr\n }\n \n-func (c *baseClient) assertUnstableCommand(cmd Cmder) bool {\n+func (c *baseClient) assertUnstableCommand(cmd Cmder) (bool, error) {\n \tswitch cmd.(type) {\n \tcase *AggregateCmd, *FTInfoCmd, *FTSpellCheckCmd, *FTSearchCmd, *FTSynDumpCmd:\n \t\tif c.opt.UnstableResp3 {\n-\t\t\treturn true\n+\t\t\treturn true, nil\n \t\t} else {\n-\t\t\tpanic(\"RESP3 responses for this command are disabled because they may still change. Please set the flag UnstableResp3 .  See the [README](https://github.com/redis/go-redis/blob/master/README.md) and the release notes for guidance.\")\n+\t\t\treturn false, fmt.Errorf(\"RESP3 responses for this command are disabled because they may still change. Please set the flag UnstableResp3. See the README and the release notes for guidance\")\n \t\t}\n \tdefault:\n-\t\treturn false\n+\t\treturn false, nil\n \t}\n }\n \n@@ -443,6 +698,11 @@ func (c *baseClient) _process(ctx context.Context, cmd Cmder, attempt int) (bool\n \n \tretryTimeout := uint32(0)\n \tif err := c.withConn(ctx, func(ctx context.Context, cn *pool.Conn) error {\n+\t\t// Process any pending push notifications before executing the command\n+\t\tif err := c.processPushNotifications(ctx, cn); err != nil {\n+\t\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before command: %v\", err)\n+\t\t}\n+\n \t\tif err := cn.WithWriter(c.context(ctx), c.opt.WriteTimeout, func(wr *proto.Writer) error {\n \t\t\treturn writeCmd(wr, cmd)\n \t\t}); err != nil {\n@@ -451,10 +711,22 @@ func (c *baseClient) _process(ctx context.Context, cmd Cmder, attempt int) (bool\n \t\t}\n \t\treadReplyFunc := cmd.readReply\n \t\t// Apply unstable RESP3 search module.\n-\t\tif c.opt.Protocol != 2 && c.assertUnstableCommand(cmd) {\n-\t\t\treadReplyFunc = cmd.readRawReply\n+\t\tif c.opt.Protocol != 2 {\n+\t\t\tuseRawReply, err := c.assertUnstableCommand(cmd)\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tif useRawReply {\n+\t\t\t\treadReplyFunc = cmd.readRawReply\n+\t\t\t}\n \t\t}\n-\t\tif err := cn.WithReader(c.context(ctx), c.cmdTimeout(cmd), readReplyFunc); err != nil {\n+\t\tif err := cn.WithReader(c.context(ctx), c.cmdTimeout(cmd), func(rd *proto.Reader) error {\n+\t\t\t// To be sure there are no buffered push notifications, we process them before reading the reply\n+\t\t\tif err := c.processPendingPushNotificationWithReader(ctx, cn, rd); err != nil {\n+\t\t\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before reading reply: %v\", err)\n+\t\t\t}\n+\t\t\treturn readReplyFunc(rd)\n+\t\t}); err != nil {\n \t\t\tif cmd.readTimeout() == nil {\n \t\t\t\tatomic.StoreUint32(&retryTimeout, 1)\n \t\t\t} else {\n@@ -487,19 +759,86 @@ func (c *baseClient) cmdTimeout(cmd Cmder) time.Duration {\n \treturn c.opt.ReadTimeout\n }\n \n+// context returns the context for the current connection.\n+// If the context timeout is enabled, it returns the original context.\n+// Otherwise, it returns a new background context.\n+func (c *baseClient) context(ctx context.Context) context.Context {\n+\tif c.opt.ContextTimeoutEnabled {\n+\t\treturn ctx\n+\t}\n+\treturn context.Background()\n+}\n+\n+// createInitConnFunc creates a connection initialization function that can be used for reconnections.\n+func (c *baseClient) createInitConnFunc() func(context.Context, *pool.Conn) error {\n+\treturn func(ctx context.Context, cn *pool.Conn) error {\n+\t\treturn c.initConn(ctx, cn)\n+\t}\n+}\n+\n+// enableMaintNotificationsUpgrades initializes the maintnotifications upgrade manager and pool hook.\n+// This function is called during client initialization.\n+// will register push notification handlers for all maintenance upgrade events.\n+// will start background workers for handoff processing in the pool hook.\n+func (c *baseClient) enableMaintNotificationsUpgrades() error {\n+\t// Create client adapter\n+\tclientAdapterInstance := newClientAdapter(c)\n+\n+\t// Create maintnotifications manager directly\n+\tmanager, err := maintnotifications.NewManager(clientAdapterInstance, c.connPool, c.opt.MaintNotificationsConfig)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\t// Set the manager reference and initialize pool hook\n+\tc.maintNotificationsManagerLock.Lock()\n+\tc.maintNotificationsManager = manager\n+\tc.maintNotificationsManagerLock.Unlock()\n+\n+\t// Initialize pool hook (safe to call without lock since manager is now set)\n+\tmanager.InitPoolHook(c.dialHook)\n+\treturn nil\n+}\n+\n+func (c *baseClient) disableMaintNotificationsUpgrades() error {\n+\tc.maintNotificationsManagerLock.Lock()\n+\tdefer c.maintNotificationsManagerLock.Unlock()\n+\n+\t// Close the maintnotifications manager\n+\tif c.maintNotificationsManager != nil {\n+\t\t// Closing the manager will also shutdown the pool hook\n+\t\t// and remove it from the pool\n+\t\tc.maintNotificationsManager.Close()\n+\t\tc.maintNotificationsManager = nil\n+\t}\n+\treturn nil\n+}\n+\n // Close closes the client, releasing any open resources.\n //\n // It is rare to Close a Client, as the Client is meant to be\n // long-lived and shared between many goroutines.\n func (c *baseClient) Close() error {\n \tvar firstErr error\n+\n+\t// Close maintnotifications manager first\n+\tif err := c.disableMaintNotificationsUpgrades(); err != nil {\n+\t\tfirstErr = err\n+\t}\n+\n \tif c.onClose != nil {\n-\t\tif err := c.onClose(); err != nil {\n+\t\tif err := c.onClose(); err != nil && firstErr == nil {\n \t\t\tfirstErr = err\n \t\t}\n \t}\n-\tif err := c.connPool.Close(); err != nil && firstErr == nil {\n-\t\tfirstErr = err\n+\tif c.connPool != nil {\n+\t\tif err := c.connPool.Close(); err != nil && firstErr == nil {\n+\t\t\tfirstErr = err\n+\t\t}\n+\t}\n+\tif c.pubSubPool != nil {\n+\t\tif err := c.pubSubPool.Close(); err != nil && firstErr == nil {\n+\t\t\tfirstErr = err\n+\t\t}\n \t}\n \treturn firstErr\n }\n@@ -539,11 +878,19 @@ func (c *baseClient) generalProcessPipeline(\n \t\t// Enable retries by default to retry dial errors returned by withConn.\n \t\tcanRetry := true\n \t\tlastErr = c.withConn(ctx, func(ctx context.Context, cn *pool.Conn) error {\n+\t\t\t// Process any pending push notifications before executing the pipeline\n+\t\t\tif err := c.processPushNotifications(ctx, cn); err != nil {\n+\t\t\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before processing pipeline: %v\", err)\n+\t\t\t}\n \t\t\tvar err error\n \t\t\tcanRetry, err = p(ctx, cn, cmds)\n \t\t\treturn err\n \t\t})\n \t\tif lastErr == nil || !canRetry || !shouldRetry(lastErr, true) {\n+\t\t\t// The error should be set here only when failing to obtain the conn.\n+\t\t\tif !isRedisError(lastErr) {\n+\t\t\t\tsetCmdsErr(cmds, lastErr)\n+\t\t\t}\n \t\t\treturn lastErr\n \t\t}\n \t}\n@@ -553,6 +900,11 @@ func (c *baseClient) generalProcessPipeline(\n func (c *baseClient) pipelineProcessCmds(\n \tctx context.Context, cn *pool.Conn, cmds []Cmder,\n ) (bool, error) {\n+\t// Process any pending push notifications before executing the pipeline\n+\tif err := c.processPushNotifications(ctx, cn); err != nil {\n+\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before writing pipeline: %v\", err)\n+\t}\n+\n \tif err := cn.WithWriter(c.context(ctx), c.opt.WriteTimeout, func(wr *proto.Writer) error {\n \t\treturn writeCmds(wr, cmds)\n \t}); err != nil {\n@@ -561,16 +913,21 @@ func (c *baseClient) pipelineProcessCmds(\n \t}\n \n \tif err := cn.WithReader(c.context(ctx), c.opt.ReadTimeout, func(rd *proto.Reader) error {\n-\t\treturn pipelineReadCmds(rd, cmds)\n+\t\t// read all replies\n+\t\treturn c.pipelineReadCmds(ctx, cn, rd, cmds)\n \t}); err != nil {\n \t\treturn true, err\n \t}\n \n \treturn false, nil\n }\n \n-func pipelineReadCmds(rd *proto.Reader, cmds []Cmder) error {\n+func (c *baseClient) pipelineReadCmds(ctx context.Context, cn *pool.Conn, rd *proto.Reader, cmds []Cmder) error {\n \tfor i, cmd := range cmds {\n+\t\t// To be sure there are no buffered push notifications, we process them before reading the reply\n+\t\tif err := c.processPendingPushNotificationWithReader(ctx, cn, rd); err != nil {\n+\t\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before reading reply: %v\", err)\n+\t\t}\n \t\terr := cmd.readReply(rd)\n \t\tcmd.SetErr(err)\n \t\tif err != nil && !isRedisError(err) {\n@@ -585,6 +942,11 @@ func pipelineReadCmds(rd *proto.Reader, cmds []Cmder) error {\n func (c *baseClient) txPipelineProcessCmds(\n \tctx context.Context, cn *pool.Conn, cmds []Cmder,\n ) (bool, error) {\n+\t// Process any pending push notifications before executing the transaction pipeline\n+\tif err := c.processPushNotifications(ctx, cn); err != nil {\n+\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before transaction: %v\", err)\n+\t}\n+\n \tif err := cn.WithWriter(c.context(ctx), c.opt.WriteTimeout, func(wr *proto.Writer) error {\n \t\treturn writeCmds(wr, cmds)\n \t}); err != nil {\n@@ -597,32 +959,50 @@ func (c *baseClient) txPipelineProcessCmds(\n \t\t// Trim multi and exec.\n \t\ttrimmedCmds := cmds[1 : len(cmds)-1]\n \n-\t\tif err := txPipelineReadQueued(rd, statusCmd, trimmedCmds); err != nil {\n+\t\tif err := c.txPipelineReadQueued(ctx, cn, rd, statusCmd, trimmedCmds); err != nil {\n \t\t\tsetCmdsErr(cmds, err)\n \t\t\treturn err\n \t\t}\n \n-\t\treturn pipelineReadCmds(rd, trimmedCmds)\n+\t\t// Read replies.\n+\t\treturn c.pipelineReadCmds(ctx, cn, rd, trimmedCmds)\n \t}); err != nil {\n \t\treturn false, err\n \t}\n \n \treturn false, nil\n }\n \n-func txPipelineReadQueued(rd *proto.Reader, statusCmd *StatusCmd, cmds []Cmder) error {\n+// txPipelineReadQueued reads queued replies from the Redis server.\n+// It returns an error if the server returns an error or if the number of replies does not match the number of commands.\n+func (c *baseClient) txPipelineReadQueued(ctx context.Context, cn *pool.Conn, rd *proto.Reader, statusCmd *StatusCmd, cmds []Cmder) error {\n+\t// To be sure there are no buffered push notifications, we process them before reading the reply\n+\tif err := c.processPendingPushNotificationWithReader(ctx, cn, rd); err != nil {\n+\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before reading reply: %v\", err)\n+\t}\n \t// Parse +OK.\n \tif err := statusCmd.readReply(rd); err != nil {\n \t\treturn err\n \t}\n \n \t// Parse +QUEUED.\n-\tfor range cmds {\n-\t\tif err := statusCmd.readReply(rd); err != nil && !isRedisError(err) {\n-\t\t\treturn err\n+\tfor _, cmd := range cmds {\n+\t\t// To be sure there are no buffered push notifications, we process them before reading the reply\n+\t\tif err := c.processPendingPushNotificationWithReader(ctx, cn, rd); err != nil {\n+\t\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before reading reply: %v\", err)\n+\t\t}\n+\t\tif err := statusCmd.readReply(rd); err != nil {\n+\t\t\tcmd.SetErr(err)\n+\t\t\tif !isRedisError(err) {\n+\t\t\t\treturn err\n+\t\t\t}\n \t\t}\n \t}\n \n+\t// To be sure there are no buffered push notifications, we process them before reading the reply\n+\tif err := c.processPendingPushNotificationWithReader(ctx, cn, rd); err != nil {\n+\t\tinternal.Logger.Printf(ctx, \"push: error processing pending notifications before reading reply: %v\", err)\n+\t}\n \t// Parse number of replies.\n \tline, err := rd.ReadLine()\n \tif err != nil {\n@@ -639,13 +1019,6 @@ func txPipelineReadQueued(rd *proto.Reader, statusCmd *StatusCmd, cmds []Cmder)\n \treturn nil\n }\n \n-func (c *baseClient) context(ctx context.Context) context.Context {\n-\tif c.opt.ContextTimeoutEnabled {\n-\t\treturn ctx\n-\t}\n-\treturn context.Background()\n-}\n-\n //------------------------------------------------------------------------------\n \n // Client is a Redis client representing a pool of zero or more underlying connections.\n@@ -656,20 +1029,68 @@ func (c *baseClient) context(ctx context.Context) context.Context {\n type Client struct {\n \t*baseClient\n \tcmdable\n-\thooksMixin\n }\n \n // NewClient returns a client to the Redis Server specified by Options.\n func NewClient(opt *Options) *Client {\n+\tif opt == nil {\n+\t\tpanic(\"redis: NewClient nil options\")\n+\t}\n+\t// clone to not share options with the caller\n+\topt = opt.clone()\n \topt.init()\n \n+\t// Push notifications are always enabled for RESP3 (cannot be disabled)\n+\n \tc := Client{\n \t\tbaseClient: &baseClient{\n \t\t\topt: opt,\n \t\t},\n \t}\n \tc.init()\n-\tc.connPool = newConnPool(opt, c.dialHook)\n+\n+\t// Initialize push notification processor using shared helper\n+\t// Use void processor for RESP2 connections (push notifications not available)\n+\tc.pushProcessor = initializePushProcessor(opt)\n+\t// set opt push processor for child clients\n+\tc.opt.PushNotificationProcessor = c.pushProcessor\n+\n+\t// Create connection pools\n+\tvar err error\n+\tc.connPool, err = newConnPool(opt, c.dialHook)\n+\tif err != nil {\n+\t\tpanic(fmt.Errorf(\"redis: failed to create connection pool: %w\", err))\n+\t}\n+\tc.pubSubPool, err = newPubSubPool(opt, c.dialHook)\n+\tif err != nil {\n+\t\tpanic(fmt.Errorf(\"redis: failed to create pubsub pool: %w\", err))\n+\t}\n+\n+\tif opt.StreamingCredentialsProvider != nil {\n+\t\tc.streamingCredentialsManager = streaming.NewManager(c.connPool, c.opt.PoolTimeout)\n+\t\tc.connPool.AddPoolHook(c.streamingCredentialsManager.PoolHook())\n+\t}\n+\n+\t// Initialize maintnotifications first if enabled and protocol is RESP3\n+\tif opt.MaintNotificationsConfig != nil && opt.MaintNotificationsConfig.Mode != maintnotifications.ModeDisabled && opt.Protocol == 3 {\n+\t\terr := c.enableMaintNotificationsUpgrades()\n+\t\tif err != nil {\n+\t\t\tinternal.Logger.Printf(context.Background(), \"failed to initialize maintnotifications: %v\", err)\n+\t\t\tif opt.MaintNotificationsConfig.Mode == maintnotifications.ModeEnabled {\n+\t\t\t\t/*\n+\t\t\t\t\tDesign decision: panic here to fail fast if maintnotifications cannot be enabled when explicitly requested.\n+\t\t\t\t\tWe choose to panic instead of returning an error to avoid breaking the existing client API, which does not expect\n+\t\t\t\t\tan error from NewClient. This ensures that misconfiguration or critical initialization failures are surfaced\n+\t\t\t\t\timmediately, rather than allowing the client to continue in a partially initialized or inconsistent state.\n+\t\t\t\t\tClients relying on maintnotifications should be aware that initialization errors will cause a panic, and should\n+\t\t\t\t\thandle this accordingly (e.g., via recover or by validating configuration before calling NewClient).\n+\t\t\t\t\tThis approach is only used when MaintNotificationsConfig.Mode is MaintNotificationsEnabled, indicating that maintnotifications\n+\t\t\t\t\tupgrades are required for correct operation. In other modes, initialization failures are logged but do not panic.\n+\t\t\t\t*/\n+\t\t\t\tpanic(fmt.Errorf(\"failed to enable maintnotifications: %w\", err))\n+\t\t\t}\n+\t\t}\n+\t}\n \n \treturn &c\n }\n@@ -692,14 +1113,7 @@ func (c *Client) WithTimeout(timeout time.Duration) *Client {\n }\n \n func (c *Client) Conn() *Conn {\n-\treturn newConn(c.opt, pool.NewStickyConnPool(c.connPool))\n-}\n-\n-// Do create a Cmd from the args and processes the cmd.\n-func (c *Client) Do(ctx context.Context, args ...interface{}) *Cmd {\n-\tcmd := NewCmd(ctx, args...)\n-\t_ = c.Process(ctx, cmd)\n-\treturn cmd\n+\treturn newConn(c.opt, pool.NewStickyConnPool(c.connPool), &c.hooksMixin)\n }\n \n func (c *Client) Process(ctx context.Context, cmd Cmder) error {\n@@ -713,11 +1127,51 @@ func (c *Client) Options() *Options {\n \treturn c.opt\n }\n \n+// GetMaintNotificationsManager returns the maintnotifications manager instance for monitoring and control.\n+// Returns nil if maintnotifications are not enabled.\n+func (c *Client) GetMaintNotificationsManager() *maintnotifications.Manager {\n+\tc.maintNotificationsManagerLock.RLock()\n+\tdefer c.maintNotificationsManagerLock.RUnlock()\n+\treturn c.maintNotificationsManager\n+}\n+\n+// initializePushProcessor initializes the push notification processor for any client type.\n+// This is a shared helper to avoid duplication across NewClient, NewFailoverClient, and NewSentinelClient.\n+func initializePushProcessor(opt *Options) push.NotificationProcessor {\n+\t// Always use custom processor if provided\n+\tif opt.PushNotificationProcessor != nil {\n+\t\treturn opt.PushNotificationProcessor\n+\t}\n+\n+\t// Push notifications are always enabled for RESP3, disabled for RESP2\n+\tif opt.Protocol == 3 {\n+\t\t// Create default processor for RESP3 connections\n+\t\treturn NewPushNotificationProcessor()\n+\t}\n+\n+\t// Create void processor for RESP2 connections (push notifications not available)\n+\treturn NewVoidPushNotificationProcessor()\n+}\n+\n+// RegisterPushNotificationHandler registers a handler for a specific push notification name.\n+// Returns an error if a handler is already registered for this push notification name.\n+// If protected is true, the handler cannot be unregistered.\n+func (c *Client) RegisterPushNotificationHandler(pushNotificationName string, handler push.NotificationHandler, protected bool) error {\n+\treturn c.pushProcessor.RegisterHandler(pushNotificationName, handler, protected)\n+}\n+\n+// GetPushNotificationHandler returns the handler for a specific push notification name.\n+// Returns nil if no handler is registered for the given name.\n+func (c *Client) GetPushNotificationHandler(pushNotificationName string) push.NotificationHandler {\n+\treturn c.pushProcessor.GetHandler(pushNotificationName)\n+}\n+\n type PoolStats pool.Stats\n \n // PoolStats returns connection pool stats.\n func (c *Client) PoolStats() *PoolStats {\n \tstats := c.connPool.Stats()\n+\tstats.PubSubStats = *(c.pubSubPool.Stats())\n \treturn (*PoolStats)(stats)\n }\n \n@@ -752,13 +1206,31 @@ func (c *Client) TxPipeline() Pipeliner {\n func (c *Client) pubSub() *PubSub {\n \tpubsub := &PubSub{\n \t\topt: c.opt,\n-\n-\t\tnewConn: func(ctx context.Context, channels []string) (*pool.Conn, error) {\n-\t\t\treturn c.newConn(ctx)\n+\t\tnewConn: func(ctx context.Context, addr string, channels []string) (*pool.Conn, error) {\n+\t\t\tcn, err := c.pubSubPool.NewConn(ctx, c.opt.Network, addr, channels)\n+\t\t\tif err != nil {\n+\t\t\t\treturn nil, err\n+\t\t\t}\n+\t\t\t// will return nil if already initialized\n+\t\t\terr = c.initConn(ctx, cn)\n+\t\t\tif err != nil {\n+\t\t\t\t_ = cn.Close()\n+\t\t\t\treturn nil, err\n+\t\t\t}\n+\t\t\t// Track connection in PubSubPool\n+\t\t\tc.pubSubPool.TrackConn(cn)\n+\t\t\treturn cn, nil\n \t\t},\n-\t\tcloseConn: c.connPool.CloseConn,\n+\t\tcloseConn: func(cn *pool.Conn) error {\n+\t\t\t// Untrack connection from PubSubPool\n+\t\t\tc.pubSubPool.UntrackConn(cn)\n+\t\t\t_ = cn.Close()\n+\t\t\treturn nil\n+\t\t},\n+\t\tpushProcessor: c.pushProcessor,\n \t}\n \tpubsub.init()\n+\n \treturn pubsub\n }\n \n@@ -825,17 +1297,27 @@ type Conn struct {\n \tbaseClient\n \tcmdable\n \tstatefulCmdable\n-\thooksMixin\n }\n \n-func newConn(opt *Options, connPool pool.Pooler) *Conn {\n+// newConn is a helper func to create a new Conn instance.\n+// the Conn instance is not thread-safe and should not be shared between goroutines.\n+// the parentHooks will be cloned, no need to clone before passing it.\n+func newConn(opt *Options, connPool pool.Pooler, parentHooks *hooksMixin) *Conn {\n \tc := Conn{\n \t\tbaseClient: baseClient{\n \t\t\topt:      opt,\n \t\t\tconnPool: connPool,\n \t\t},\n \t}\n \n+\tif parentHooks != nil {\n+\t\tc.hooksMixin = parentHooks.clone()\n+\t}\n+\n+\t// Initialize push notification processor using shared helper\n+\t// Use void processor for RESP2 connections (push notifications not available)\n+\tc.pushProcessor = initializePushProcessor(opt)\n+\n \tc.cmdable = c.Process\n \tc.statefulCmdable = c.Process\n \tc.initHooks(hooks{\n@@ -854,6 +1336,13 @@ func (c *Conn) Process(ctx context.Context, cmd Cmder) error {\n \treturn err\n }\n \n+// RegisterPushNotificationHandler registers a handler for a specific push notification name.\n+// Returns an error if a handler is already registered for this push notification name.\n+// If protected is true, the handler cannot be unregistered.\n+func (c *Conn) RegisterPushNotificationHandler(pushNotificationName string, handler push.NotificationHandler, protected bool) error {\n+\treturn c.pushProcessor.RegisterHandler(pushNotificationName, handler, protected)\n+}\n+\n func (c *Conn) Pipelined(ctx context.Context, fn func(Pipeliner) error) ([]Cmder, error) {\n \treturn c.Pipeline().Pipelined(ctx, fn)\n }\n@@ -881,3 +1370,78 @@ func (c *Conn) TxPipeline() Pipeliner {\n \tpipe.init()\n \treturn &pipe\n }\n+\n+// processPushNotifications processes all pending push notifications on a connection\n+// This ensures that cluster topology changes are handled immediately before the connection is used\n+// This method should be called by the client before using WithReader for command execution\n+//\n+// Performance optimization: Skip the expensive MaybeHasData() syscall if a health check\n+// was performed recently (within 5 seconds). The health check already verified the connection\n+// is healthy and checked for unexpected data (push notifications).\n+func (c *baseClient) processPushNotifications(ctx context.Context, cn *pool.Conn) error {\n+\t// Only process push notifications for RESP3 connections with a processor\n+\tif c.opt.Protocol != 3 || c.pushProcessor == nil {\n+\t\treturn nil\n+\t}\n+\n+\t// Performance optimization: Skip MaybeHasData() syscall if health check was recent\n+\t// If the connection was health-checked within the last 5 seconds, we can skip the\n+\t// expensive syscall since the health check already verified no unexpected data.\n+\t// This is safe because:\n+\t// 0. lastHealthCheckNs is set in pool/conn.go:putConn() after a successful health check\n+\t// 1. Health check (connCheck) uses the same syscall (Recvfrom with MSG_PEEK)\n+\t// 2. If push notifications arrived, they would have been detected by health check\n+\t// 3. 5 seconds is short enough that connection state is still fresh\n+\t// 4. Push notifications will be processed by the next WithReader call\n+\t// used it is set on getConn, so we should use another timer (lastPutAt?)\n+\tlastHealthCheckNs := cn.LastPutAtNs()\n+\tif lastHealthCheckNs > 0 {\n+\t\t// Use pool's cached time to avoid expensive time.Now() syscall\n+\t\tnowNs := pool.GetCachedTimeNs()\n+\t\tif nowNs-lastHealthCheckNs < int64(5*time.Second) {\n+\t\t\t// Recent health check confirmed no unexpected data, skip the syscall\n+\t\t\treturn nil\n+\t\t}\n+\t}\n+\n+\t// Check if there is any data to read before processing\n+\t// This is an optimization on UNIX systems where MaybeHasData is a syscall\n+\t// On Windows, MaybeHasData always returns true, so this check is a no-op\n+\tif !cn.MaybeHasData() {\n+\t\treturn nil\n+\t}\n+\n+\t// Use WithReader to access the reader and process push notifications\n+\t// This is critical for maintnotifications to work properly\n+\t// NOTE: almost no timeouts are set for this read, so it should not block\n+\t// longer than necessary, 10us should be plenty of time to read if there are any push notifications\n+\t// on the socket.\n+\treturn cn.WithReader(ctx, 10*time.Microsecond, func(rd *proto.Reader) error {\n+\t\t// Create handler context with client, connection pool, and connection information\n+\t\thandlerCtx := c.pushNotificationHandlerContext(cn)\n+\t\treturn c.pushProcessor.ProcessPendingNotifications(ctx, handlerCtx, rd)\n+\t})\n+}\n+\n+// processPendingPushNotificationWithReader processes all pending push notifications on a connection\n+// This method should be called by the client in WithReader before reading the reply\n+func (c *baseClient) processPendingPushNotificationWithReader(ctx context.Context, cn *pool.Conn, rd *proto.Reader) error {\n+\t// if we have the reader, we don't need to check for data on the socket, we are waiting\n+\t// for either a reply or a push notification, so we can block until we get a reply or reach the timeout\n+\tif c.opt.Protocol != 3 || c.pushProcessor == nil {\n+\t\treturn nil\n+\t}\n+\n+\t// Create handler context with client, connection pool, and connection information\n+\thandlerCtx := c.pushNotificationHandlerContext(cn)\n+\treturn c.pushProcessor.ProcessPendingNotifications(ctx, handlerCtx, rd)\n+}\n+\n+// pushNotificationHandlerContext creates a handler context for push notification processing\n+func (c *baseClient) pushNotificationHandlerContext(cn *pool.Conn) push.NotificationHandlerContext {\n+\treturn push.NotificationHandlerContext{\n+\t\tClient:   c,\n+\t\tConnPool: c.connPool,\n+\t\tConn:     cn, // Wrap in adapter for easier interface access\n+\t}\n+}"
    },
    {
      "sha": "3e0d0a13484ee1a71d2ad18536a82bc072a244e9",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/result.go",
      "status": "modified",
      "additions": 8,
      "deletions": 0,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fresult.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fresult.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fresult.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -82,6 +82,14 @@ func NewBoolSliceResult(val []bool, err error) *BoolSliceCmd {\n \treturn &cmd\n }\n \n+// NewFloatSliceResult returns a FloatSliceCmd initialised with val and err for testing.\n+func NewFloatSliceResult(val []float64, err error) *FloatSliceCmd {\n+\tvar cmd FloatSliceCmd\n+\tcmd.val = val\n+\tcmd.SetErr(err)\n+\treturn &cmd\n+}\n+\n // NewMapStringStringResult returns a MapStringStringCmd initialised with val and err for testing.\n func NewMapStringStringResult(val map[string]string, err error) *MapStringStringCmd {\n \tvar cmd MapStringStringCmd"
    },
    {
      "sha": "3381460abddf001f24aac9f6c357f26167ab17a1",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/ring.go",
      "status": "modified",
      "additions": 128,
      "deletions": 31,
      "changes": 159,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fring.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fring.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fring.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -13,15 +13,23 @@ import (\n \n \t\"github.com/cespare/xxhash/v2\"\n \t\"github.com/dgryski/go-rendezvous\" //nolint\n+\t\"github.com/redis/go-redis/v9/auth\"\n \n \t\"github.com/redis/go-redis/v9/internal\"\n \t\"github.com/redis/go-redis/v9/internal/hashtag\"\n \t\"github.com/redis/go-redis/v9/internal/pool\"\n+\t\"github.com/redis/go-redis/v9/internal/proto\"\n \t\"github.com/redis/go-redis/v9/internal/rand\"\n )\n \n var errRingShardsDown = errors.New(\"redis: all ring shards are down\")\n \n+// defaultHeartbeatFn is the default function used to check the shard liveness\n+var defaultHeartbeatFn = func(ctx context.Context, client *Client) bool {\n+\terr := client.Ping(ctx).Err()\n+\treturn err == nil || err == pool.ErrPoolTimeout\n+}\n+\n //------------------------------------------------------------------------------\n \n type ConsistentHash interface {\n@@ -54,10 +62,14 @@ type RingOptions struct {\n \t// ClientName will execute the `CLIENT SETNAME ClientName` command for each conn.\n \tClientName string\n \n-\t// Frequency of PING commands sent to check shards availability.\n+\t// Frequency of executing HeartbeatFn to check shards availability.\n \t// Shard is considered down after 3 subsequent failed checks.\n \tHeartbeatFrequency time.Duration\n \n+\t// A function used to check the shard liveness\n+\t// if not set, defaults to defaultHeartbeatFn\n+\tHeartbeatFn func(ctx context.Context, client *Client) bool\n+\n \t// NewConsistentHash returns a consistent hash that is used\n \t// to distribute keys across the shards.\n \t//\n@@ -73,7 +85,24 @@ type RingOptions struct {\n \tProtocol int\n \tUsername string\n \tPassword string\n-\tDB       int\n+\t// CredentialsProvider allows the username and password to be updated\n+\t// before reconnecting. It should return the current username and password.\n+\tCredentialsProvider func() (username string, password string)\n+\n+\t// CredentialsProviderContext is an enhanced parameter of CredentialsProvider,\n+\t// done to maintain API compatibility. In the future,\n+\t// there might be a merge between CredentialsProviderContext and CredentialsProvider.\n+\t// There will be a conflict between them; if CredentialsProviderContext exists, we will ignore CredentialsProvider.\n+\tCredentialsProviderContext func(ctx context.Context) (username string, password string, err error)\n+\n+\t// StreamingCredentialsProvider is used to retrieve the credentials\n+\t// for the connection from an external source. Those credentials may change\n+\t// during the connection lifetime. This is useful for managed identity\n+\t// scenarios where the credentials are retrieved from an external source.\n+\t//\n+\t// Currently, this is a placeholder for the future implementation.\n+\tStreamingCredentialsProvider auth.StreamingCredentialsProvider\n+\tDB                           int\n \n \tMaxRetries      int\n \tMinRetryBackoff time.Duration\n@@ -95,6 +124,20 @@ type RingOptions struct {\n \tConnMaxIdleTime time.Duration\n \tConnMaxLifetime time.Duration\n \n+\t// ReadBufferSize is the size of the bufio.Reader buffer for each connection.\n+\t// Larger buffers can improve performance for commands that return large responses.\n+\t// Smaller buffers can improve memory usage for larger pools.\n+\t//\n+\t// default: 32KiB (32768 bytes)\n+\tReadBufferSize int\n+\n+\t// WriteBufferSize is the size of the bufio.Writer buffer for each connection.\n+\t// Larger buffers can improve performance for large pipelines and commands with many arguments.\n+\t// Smaller buffers can improve memory usage for larger pools.\n+\t//\n+\t// default: 32KiB (32768 bytes)\n+\tWriteBufferSize int\n+\n \tTLSConfig *tls.Config\n \tLimiter   Limiter\n \n@@ -124,13 +167,18 @@ func (opt *RingOptions) init() {\n \t\topt.HeartbeatFrequency = 500 * time.Millisecond\n \t}\n \n+\tif opt.HeartbeatFn == nil {\n+\t\topt.HeartbeatFn = defaultHeartbeatFn\n+\t}\n+\n \tif opt.NewConsistentHash == nil {\n \t\topt.NewConsistentHash = newRendezvous\n \t}\n \n-\tif opt.MaxRetries == -1 {\n+\tswitch opt.MaxRetries {\n+\tcase -1:\n \t\topt.MaxRetries = 0\n-\t} else if opt.MaxRetries == 0 {\n+\tcase 0:\n \t\topt.MaxRetries = 3\n \t}\n \tswitch opt.MinRetryBackoff {\n@@ -145,6 +193,13 @@ func (opt *RingOptions) init() {\n \tcase 0:\n \t\topt.MaxRetryBackoff = 512 * time.Millisecond\n \t}\n+\n+\tif opt.ReadBufferSize == 0 {\n+\t\topt.ReadBufferSize = proto.DefaultBufferSize\n+\t}\n+\tif opt.WriteBufferSize == 0 {\n+\t\topt.WriteBufferSize = proto.DefaultBufferSize\n+\t}\n }\n \n func (opt *RingOptions) clientOptions() *Options {\n@@ -153,10 +208,13 @@ func (opt *RingOptions) clientOptions() *Options {\n \t\tDialer:     opt.Dialer,\n \t\tOnConnect:  opt.OnConnect,\n \n-\t\tProtocol: opt.Protocol,\n-\t\tUsername: opt.Username,\n-\t\tPassword: opt.Password,\n-\t\tDB:       opt.DB,\n+\t\tProtocol:                     opt.Protocol,\n+\t\tUsername:                     opt.Username,\n+\t\tPassword:                     opt.Password,\n+\t\tCredentialsProvider:          opt.CredentialsProvider,\n+\t\tCredentialsProviderContext:   opt.CredentialsProviderContext,\n+\t\tStreamingCredentialsProvider: opt.StreamingCredentialsProvider,\n+\t\tDB:                           opt.DB,\n \n \t\tMaxRetries: -1,\n \n@@ -173,6 +231,8 @@ func (opt *RingOptions) clientOptions() *Options {\n \t\tMaxActiveConns:  opt.MaxActiveConns,\n \t\tConnMaxIdleTime: opt.ConnMaxIdleTime,\n \t\tConnMaxLifetime: opt.ConnMaxLifetime,\n+\t\tReadBufferSize:  opt.ReadBufferSize,\n+\t\tWriteBufferSize: opt.WriteBufferSize,\n \n \t\tTLSConfig: opt.TLSConfig,\n \t\tLimiter:   opt.Limiter,\n@@ -348,16 +408,16 @@ func (c *ringSharding) newRingShards(\n \treturn\n }\n \n+// Warning: External exposure of `c.shards.list` may cause data races.\n+// So keep internal or implement deep copy if exposed.\n func (c *ringSharding) List() []*ringShard {\n-\tvar list []*ringShard\n-\n \tc.mu.RLock()\n-\tif !c.closed {\n-\t\tlist = c.shards.list\n-\t}\n-\tc.mu.RUnlock()\n+\tdefer c.mu.RUnlock()\n \n-\treturn list\n+\tif c.closed {\n+\t\treturn nil\n+\t}\n+\treturn c.shards.list\n }\n \n func (c *ringSharding) Hash(key string) string {\n@@ -404,7 +464,12 @@ func (c *ringSharding) GetByName(shardName string) (*ringShard, error) {\n \tc.mu.RLock()\n \tdefer c.mu.RUnlock()\n \n-\treturn c.shards.m[shardName], nil\n+\tshard, ok := c.shards.m[shardName]\n+\tif !ok {\n+\t\treturn nil, errors.New(\"redis: the shard is not in the ring\")\n+\t}\n+\n+\treturn shard, nil\n }\n \n func (c *ringSharding) Random() (*ringShard, error) {\n@@ -421,9 +486,9 @@ func (c *ringSharding) Heartbeat(ctx context.Context, frequency time.Duration) {\n \t\tcase <-ticker.C:\n \t\t\tvar rebalance bool\n \n+\t\t\t// note: `c.List()` return a shadow copy of `[]*ringShard`.\n \t\t\tfor _, shard := range c.List() {\n-\t\t\t\terr := shard.Client.Ping(ctx).Err()\n-\t\t\t\tisUp := err == nil || err == pool.ErrPoolTimeout\n+\t\t\t\tisUp := c.opt.HeartbeatFn(ctx, shard.Client)\n \t\t\t\tif shard.Vote(isUp) {\n \t\t\t\t\tinternal.Logger.Printf(ctx, \"ring shard state changed: %s\", shard)\n \t\t\t\t\trebalance = true\n@@ -521,6 +586,9 @@ type Ring struct {\n }\n \n func NewRing(opt *RingOptions) *Ring {\n+\tif opt == nil {\n+\t\tpanic(\"redis: NewRing nil options\")\n+\t}\n \topt.init()\n \n \thbCtx, hbCancel := context.WithCancel(context.Background())\n@@ -553,13 +621,6 @@ func (c *Ring) SetAddrs(addrs map[string]string) {\n \tc.sharding.SetAddrs(addrs)\n }\n \n-// Do create a Cmd from the args and processes the cmd.\n-func (c *Ring) Do(ctx context.Context, args ...interface{}) *Cmd {\n-\tcmd := NewCmd(ctx, args...)\n-\t_ = c.Process(ctx, cmd)\n-\treturn cmd\n-}\n-\n func (c *Ring) Process(ctx context.Context, cmd Cmder) error {\n \terr := c.processHook(ctx, cmd)\n \tcmd.SetErr(err)\n@@ -577,6 +638,7 @@ func (c *Ring) retryBackoff(attempt int) time.Duration {\n \n // PoolStats returns accumulated connection pool stats.\n func (c *Ring) PoolStats() *PoolStats {\n+\t// note: `c.List()` return a shadow copy of `[]*ringShard`.\n \tshards := c.sharding.List()\n \tvar acc PoolStats\n \tfor _, shard := range shards {\n@@ -646,6 +708,7 @@ func (c *Ring) ForEachShard(\n \tctx context.Context,\n \tfn func(ctx context.Context, client *Client) error,\n ) error {\n+\t// note: `c.List()` return a shadow copy of `[]*ringShard`.\n \tshards := c.sharding.List()\n \tvar wg sync.WaitGroup\n \terrCh := make(chan error, 1)\n@@ -677,6 +740,7 @@ func (c *Ring) ForEachShard(\n }\n \n func (c *Ring) cmdsInfo(ctx context.Context) (map[string]*CommandInfo, error) {\n+\t// note: `c.List()` return a shadow copy of `[]*ringShard`.\n \tshards := c.sharding.List()\n \tvar firstErr error\n \tfor _, shard := range shards {\n@@ -694,7 +758,7 @@ func (c *Ring) cmdsInfo(ctx context.Context) (map[string]*CommandInfo, error) {\n \treturn nil, firstErr\n }\n \n-func (c *Ring) cmdShard(ctx context.Context, cmd Cmder) (*ringShard, error) {\n+func (c *Ring) cmdShard(cmd Cmder) (*ringShard, error) {\n \tpos := cmdFirstKeyPos(cmd)\n \tif pos == 0 {\n \t\treturn c.sharding.Random()\n@@ -712,7 +776,7 @@ func (c *Ring) process(ctx context.Context, cmd Cmder) error {\n \t\t\t}\n \t\t}\n \n-\t\tshard, err := c.cmdShard(ctx, cmd)\n+\t\tshard, err := c.cmdShard(cmd)\n \t\tif err != nil {\n \t\t\treturn err\n \t\t}\n@@ -771,6 +835,8 @@ func (c *Ring) generalProcessPipeline(\n \t}\n \n \tvar wg sync.WaitGroup\n+\terrs := make(chan error, len(cmdsMap))\n+\n \tfor hash, cmds := range cmdsMap {\n \t\twg.Add(1)\n \t\tgo func(hash string, cmds []Cmder) {\n@@ -783,16 +849,24 @@ func (c *Ring) generalProcessPipeline(\n \t\t\t\treturn\n \t\t\t}\n \n+\t\t\thook := shard.Client.processPipelineHook\n \t\t\tif tx {\n \t\t\t\tcmds = wrapMultiExec(ctx, cmds)\n-\t\t\t\t_ = shard.Client.processTxPipelineHook(ctx, cmds)\n-\t\t\t} else {\n-\t\t\t\t_ = shard.Client.processPipelineHook(ctx, cmds)\n+\t\t\t\thook = shard.Client.processTxPipelineHook\n+\t\t\t}\n+\n+\t\t\tif err = hook(ctx, cmds); err != nil {\n+\t\t\t\terrs <- err\n \t\t\t}\n \t\t}(hash, cmds)\n \t}\n \n \twg.Wait()\n+\tclose(errs)\n+\n+\tif err := <-errs; err != nil {\n+\t\treturn err\n+\t}\n \treturn cmdsFirstErr(cmds)\n }\n \n@@ -805,7 +879,7 @@ func (c *Ring) Watch(ctx context.Context, fn func(*Tx) error, keys ...string) er\n \n \tfor _, key := range keys {\n \t\tif key != \"\" {\n-\t\t\tshard, err := c.sharding.GetByKey(hashtag.Key(key))\n+\t\t\tshard, err := c.sharding.GetByKey(key)\n \t\t\tif err != nil {\n \t\t\t\treturn err\n \t\t\t}\n@@ -839,3 +913,26 @@ func (c *Ring) Close() error {\n \n \treturn c.sharding.Close()\n }\n+\n+// GetShardClients returns a list of all shard clients in the ring.\n+// This can be used to create dedicated connections (e.g., PubSub) for each shard.\n+func (c *Ring) GetShardClients() []*Client {\n+\tshards := c.sharding.List()\n+\tclients := make([]*Client, 0, len(shards))\n+\tfor _, shard := range shards {\n+\t\tif shard.IsUp() {\n+\t\t\tclients = append(clients, shard.Client)\n+\t\t}\n+\t}\n+\treturn clients\n+}\n+\n+// GetShardClientForKey returns the shard client that would handle the given key.\n+// This can be used to determine which shard a particular key/channel would be routed to.\n+func (c *Ring) GetShardClientForKey(key string) (*Client, error) {\n+\tshard, err := c.sharding.GetByKey(key)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\treturn shard.Client, nil\n+}"
    },
    {
      "sha": "91f0634041e355b3ed1326ffd1d0379666499861",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/search_builders.go",
      "status": "added",
      "additions": 825,
      "deletions": 0,
      "changes": 825,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsearch_builders.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsearch_builders.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsearch_builders.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2",
      "patch": "@@ -0,0 +1,825 @@\n+package redis\n+\n+import (\n+\t\"context\"\n+)\n+\n+// ----------------------\n+// Search Module Builders\n+// ----------------------\n+\n+// SearchBuilder provides a fluent API for FT.SEARCH\n+// (see original FTSearchOptions for all options).\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+type SearchBuilder struct {\n+\tc       *Client\n+\tctx     context.Context\n+\tindex   string\n+\tquery   string\n+\toptions *FTSearchOptions\n+}\n+\n+// NewSearchBuilder creates a new SearchBuilder for FT.SEARCH commands.\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+func (c *Client) NewSearchBuilder(ctx context.Context, index, query string) *SearchBuilder {\n+\tb := &SearchBuilder{c: c, ctx: ctx, index: index, query: query, options: &FTSearchOptions{LimitOffset: -1}}\n+\treturn b\n+}\n+\n+// WithScores includes WITHSCORES.\n+func (b *SearchBuilder) WithScores() *SearchBuilder {\n+\tb.options.WithScores = true\n+\treturn b\n+}\n+\n+// NoContent includes NOCONTENT.\n+func (b *SearchBuilder) NoContent() *SearchBuilder { b.options.NoContent = true; return b }\n+\n+// Verbatim includes VERBATIM.\n+func (b *SearchBuilder) Verbatim() *SearchBuilder { b.options.Verbatim = true; return b }\n+\n+// NoStopWords includes NOSTOPWORDS.\n+func (b *SearchBuilder) NoStopWords() *SearchBuilder { b.options.NoStopWords = true; return b }\n+\n+// WithPayloads includes WITHPAYLOADS.\n+func (b *SearchBuilder) WithPayloads() *SearchBuilder {\n+\tb.options.WithPayloads = true\n+\treturn b\n+}\n+\n+// WithSortKeys includes WITHSORTKEYS.\n+func (b *SearchBuilder) WithSortKeys() *SearchBuilder {\n+\tb.options.WithSortKeys = true\n+\treturn b\n+}\n+\n+// Filter adds a FILTER clause: FILTER <field> <min> <max>.\n+func (b *SearchBuilder) Filter(field string, min, max interface{}) *SearchBuilder {\n+\tb.options.Filters = append(b.options.Filters, FTSearchFilter{\n+\t\tFieldName: field,\n+\t\tMin:       min,\n+\t\tMax:       max,\n+\t})\n+\treturn b\n+}\n+\n+// GeoFilter adds a GEOFILTER clause: GEOFILTER <field> <lon> <lat> <radius> <unit>.\n+func (b *SearchBuilder) GeoFilter(field string, lon, lat, radius float64, unit string) *SearchBuilder {\n+\tb.options.GeoFilter = append(b.options.GeoFilter, FTSearchGeoFilter{\n+\t\tFieldName: field,\n+\t\tLongitude: lon,\n+\t\tLatitude:  lat,\n+\t\tRadius:    radius,\n+\t\tUnit:      unit,\n+\t})\n+\treturn b\n+}\n+\n+// InKeys restricts the search to the given keys.\n+func (b *SearchBuilder) InKeys(keys ...interface{}) *SearchBuilder {\n+\tb.options.InKeys = append(b.options.InKeys, keys...)\n+\treturn b\n+}\n+\n+// InFields restricts the search to the given fields.\n+func (b *SearchBuilder) InFields(fields ...interface{}) *SearchBuilder {\n+\tb.options.InFields = append(b.options.InFields, fields...)\n+\treturn b\n+}\n+\n+// ReturnFields adds simple RETURN <n> <field>...\n+func (b *SearchBuilder) ReturnFields(fields ...string) *SearchBuilder {\n+\tfor _, f := range fields {\n+\t\tb.options.Return = append(b.options.Return, FTSearchReturn{FieldName: f})\n+\t}\n+\treturn b\n+}\n+\n+// ReturnAs adds RETURN <field> AS <alias>.\n+func (b *SearchBuilder) ReturnAs(field, alias string) *SearchBuilder {\n+\tb.options.Return = append(b.options.Return, FTSearchReturn{FieldName: field, As: alias})\n+\treturn b\n+}\n+\n+// Slop adds SLOP <n>.\n+func (b *SearchBuilder) Slop(slop int) *SearchBuilder {\n+\tb.options.Slop = slop\n+\treturn b\n+}\n+\n+// Timeout adds TIMEOUT <ms>.\n+func (b *SearchBuilder) Timeout(timeout int) *SearchBuilder {\n+\tb.options.Timeout = timeout\n+\treturn b\n+}\n+\n+// InOrder includes INORDER.\n+func (b *SearchBuilder) InOrder() *SearchBuilder {\n+\tb.options.InOrder = true\n+\treturn b\n+}\n+\n+// Language sets LANGUAGE <lang>.\n+func (b *SearchBuilder) Language(lang string) *SearchBuilder {\n+\tb.options.Language = lang\n+\treturn b\n+}\n+\n+// Expander sets EXPANDER <expander>.\n+func (b *SearchBuilder) Expander(expander string) *SearchBuilder {\n+\tb.options.Expander = expander\n+\treturn b\n+}\n+\n+// Scorer sets SCORER <scorer>.\n+func (b *SearchBuilder) Scorer(scorer string) *SearchBuilder {\n+\tb.options.Scorer = scorer\n+\treturn b\n+}\n+\n+// ExplainScore includes EXPLAINSCORE.\n+func (b *SearchBuilder) ExplainScore() *SearchBuilder {\n+\tb.options.ExplainScore = true\n+\treturn b\n+}\n+\n+// Payload sets PAYLOAD <payload>.\n+func (b *SearchBuilder) Payload(payload string) *SearchBuilder {\n+\tb.options.Payload = payload\n+\treturn b\n+}\n+\n+// SortBy adds SORTBY <field> ASC|DESC.\n+func (b *SearchBuilder) SortBy(field string, asc bool) *SearchBuilder {\n+\tb.options.SortBy = append(b.options.SortBy, FTSearchSortBy{\n+\t\tFieldName: field,\n+\t\tAsc:       asc,\n+\t\tDesc:      !asc,\n+\t})\n+\treturn b\n+}\n+\n+// WithSortByCount includes WITHCOUNT (when used with SortBy).\n+func (b *SearchBuilder) WithSortByCount() *SearchBuilder {\n+\tb.options.SortByWithCount = true\n+\treturn b\n+}\n+\n+// Param adds a single PARAMS <k> <v>.\n+func (b *SearchBuilder) Param(key string, value interface{}) *SearchBuilder {\n+\tif b.options.Params == nil {\n+\t\tb.options.Params = make(map[string]interface{}, 1)\n+\t}\n+\tb.options.Params[key] = value\n+\treturn b\n+}\n+\n+// ParamsMap adds multiple PARAMS at once.\n+func (b *SearchBuilder) ParamsMap(p map[string]interface{}) *SearchBuilder {\n+\tif b.options.Params == nil {\n+\t\tb.options.Params = make(map[string]interface{}, len(p))\n+\t}\n+\tfor k, v := range p {\n+\t\tb.options.Params[k] = v\n+\t}\n+\treturn b\n+}\n+\n+// Dialect sets DIALECT <version>.\n+func (b *SearchBuilder) Dialect(version int) *SearchBuilder {\n+\tb.options.DialectVersion = version\n+\treturn b\n+}\n+\n+// Limit sets OFFSET and COUNT. CountOnly uses LIMIT 0 0.\n+func (b *SearchBuilder) Limit(offset, count int) *SearchBuilder {\n+\tb.options.LimitOffset = offset\n+\tb.options.Limit = count\n+\treturn b\n+}\n+func (b *SearchBuilder) CountOnly() *SearchBuilder { b.options.CountOnly = true; return b }\n+\n+// Run executes FT.SEARCH and returns a typed result.\n+func (b *SearchBuilder) Run() (FTSearchResult, error) {\n+\tcmd := b.c.FTSearchWithArgs(b.ctx, b.index, b.query, b.options)\n+\treturn cmd.Result()\n+}\n+\n+// ----------------------\n+// AggregateBuilder for FT.AGGREGATE\n+// ----------------------\n+\n+type AggregateBuilder struct {\n+\tc       *Client\n+\tctx     context.Context\n+\tindex   string\n+\tquery   string\n+\toptions *FTAggregateOptions\n+}\n+\n+// NewAggregateBuilder creates a new AggregateBuilder for FT.AGGREGATE commands.\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+func (c *Client) NewAggregateBuilder(ctx context.Context, index, query string) *AggregateBuilder {\n+\treturn &AggregateBuilder{c: c, ctx: ctx, index: index, query: query, options: &FTAggregateOptions{LimitOffset: -1}}\n+}\n+\n+// Verbatim includes VERBATIM.\n+func (b *AggregateBuilder) Verbatim() *AggregateBuilder { b.options.Verbatim = true; return b }\n+\n+// AddScores includes ADDSCORES.\n+func (b *AggregateBuilder) AddScores() *AggregateBuilder { b.options.AddScores = true; return b }\n+\n+// Scorer sets SCORER <scorer>.\n+func (b *AggregateBuilder) Scorer(s string) *AggregateBuilder {\n+\tb.options.Scorer = s\n+\treturn b\n+}\n+\n+// LoadAll includes LOAD * (mutually exclusive with Load).\n+func (b *AggregateBuilder) LoadAll() *AggregateBuilder {\n+\tb.options.LoadAll = true\n+\treturn b\n+}\n+\n+// Load adds LOAD <n> <field> [AS alias]...\n+// You can call it multiple times for multiple fields.\n+func (b *AggregateBuilder) Load(field string, alias ...string) *AggregateBuilder {\n+\t// each Load entry becomes one element in options.Load\n+\tl := FTAggregateLoad{Field: field}\n+\tif len(alias) > 0 {\n+\t\tl.As = alias[0]\n+\t}\n+\tb.options.Load = append(b.options.Load, l)\n+\treturn b\n+}\n+\n+// Timeout sets TIMEOUT <ms>.\n+func (b *AggregateBuilder) Timeout(ms int) *AggregateBuilder {\n+\tb.options.Timeout = ms\n+\treturn b\n+}\n+\n+// Apply adds APPLY <field> [AS alias].\n+func (b *AggregateBuilder) Apply(field string, alias ...string) *AggregateBuilder {\n+\ta := FTAggregateApply{Field: field}\n+\tif len(alias) > 0 {\n+\t\ta.As = alias[0]\n+\t}\n+\tb.options.Apply = append(b.options.Apply, a)\n+\treturn b\n+}\n+\n+// GroupBy starts a new GROUPBY <fields...> clause.\n+func (b *AggregateBuilder) GroupBy(fields ...interface{}) *AggregateBuilder {\n+\tb.options.GroupBy = append(b.options.GroupBy, FTAggregateGroupBy{\n+\t\tFields: fields,\n+\t})\n+\treturn b\n+}\n+\n+// Reduce adds a REDUCE <fn> [<#args> <args...>] clause to the *last* GROUPBY.\n+func (b *AggregateBuilder) Reduce(fn SearchAggregator, args ...interface{}) *AggregateBuilder {\n+\tif len(b.options.GroupBy) == 0 {\n+\t\t// no GROUPBY yet  nothing to attach to\n+\t\treturn b\n+\t}\n+\tidx := len(b.options.GroupBy) - 1\n+\tb.options.GroupBy[idx].Reduce = append(b.options.GroupBy[idx].Reduce, FTAggregateReducer{\n+\t\tReducer: fn,\n+\t\tArgs:    args,\n+\t})\n+\treturn b\n+}\n+\n+// ReduceAs does the same but also sets an alias: REDUCE <fn>  AS <alias>\n+func (b *AggregateBuilder) ReduceAs(fn SearchAggregator, alias string, args ...interface{}) *AggregateBuilder {\n+\tif len(b.options.GroupBy) == 0 {\n+\t\treturn b\n+\t}\n+\tidx := len(b.options.GroupBy) - 1\n+\tb.options.GroupBy[idx].Reduce = append(b.options.GroupBy[idx].Reduce, FTAggregateReducer{\n+\t\tReducer: fn,\n+\t\tArgs:    args,\n+\t\tAs:      alias,\n+\t})\n+\treturn b\n+}\n+\n+// SortBy adds SORTBY <field> ASC|DESC.\n+func (b *AggregateBuilder) SortBy(field string, asc bool) *AggregateBuilder {\n+\tsb := FTAggregateSortBy{FieldName: field, Asc: asc, Desc: !asc}\n+\tb.options.SortBy = append(b.options.SortBy, sb)\n+\treturn b\n+}\n+\n+// SortByMax sets MAX <n> (only if SortBy was called).\n+func (b *AggregateBuilder) SortByMax(max int) *AggregateBuilder {\n+\tb.options.SortByMax = max\n+\treturn b\n+}\n+\n+// Filter sets FILTER <expr>.\n+func (b *AggregateBuilder) Filter(expr string) *AggregateBuilder {\n+\tb.options.Filter = expr\n+\treturn b\n+}\n+\n+// WithCursor enables WITHCURSOR [COUNT <n>] [MAXIDLE <ms>].\n+func (b *AggregateBuilder) WithCursor(count, maxIdle int) *AggregateBuilder {\n+\tb.options.WithCursor = true\n+\tif b.options.WithCursorOptions == nil {\n+\t\tb.options.WithCursorOptions = &FTAggregateWithCursor{}\n+\t}\n+\tb.options.WithCursorOptions.Count = count\n+\tb.options.WithCursorOptions.MaxIdle = maxIdle\n+\treturn b\n+}\n+\n+// Params adds PARAMS <k v> pairs.\n+func (b *AggregateBuilder) Params(p map[string]interface{}) *AggregateBuilder {\n+\tif b.options.Params == nil {\n+\t\tb.options.Params = make(map[string]interface{}, len(p))\n+\t}\n+\tfor k, v := range p {\n+\t\tb.options.Params[k] = v\n+\t}\n+\treturn b\n+}\n+\n+// Dialect sets DIALECT <version>.\n+func (b *AggregateBuilder) Dialect(version int) *AggregateBuilder {\n+\tb.options.DialectVersion = version\n+\treturn b\n+}\n+\n+// Run executes FT.AGGREGATE and returns a typed result.\n+func (b *AggregateBuilder) Run() (*FTAggregateResult, error) {\n+\tcmd := b.c.FTAggregateWithArgs(b.ctx, b.index, b.query, b.options)\n+\treturn cmd.Result()\n+}\n+\n+// ----------------------\n+// CreateIndexBuilder for FT.CREATE\n+// ----------------------\n+// CreateIndexBuilder is builder for FT.CREATE\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+type CreateIndexBuilder struct {\n+\tc       *Client\n+\tctx     context.Context\n+\tindex   string\n+\toptions *FTCreateOptions\n+\tschema  []*FieldSchema\n+}\n+\n+// NewCreateIndexBuilder creates a new CreateIndexBuilder for FT.CREATE commands.\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+func (c *Client) NewCreateIndexBuilder(ctx context.Context, index string) *CreateIndexBuilder {\n+\treturn &CreateIndexBuilder{c: c, ctx: ctx, index: index, options: &FTCreateOptions{}}\n+}\n+\n+// OnHash sets ON HASH.\n+func (b *CreateIndexBuilder) OnHash() *CreateIndexBuilder { b.options.OnHash = true; return b }\n+\n+// OnJSON sets ON JSON.\n+func (b *CreateIndexBuilder) OnJSON() *CreateIndexBuilder { b.options.OnJSON = true; return b }\n+\n+// Prefix sets PREFIX.\n+func (b *CreateIndexBuilder) Prefix(prefixes ...interface{}) *CreateIndexBuilder {\n+\tb.options.Prefix = prefixes\n+\treturn b\n+}\n+\n+// Filter sets FILTER.\n+func (b *CreateIndexBuilder) Filter(filter string) *CreateIndexBuilder {\n+\tb.options.Filter = filter\n+\treturn b\n+}\n+\n+// DefaultLanguage sets LANGUAGE.\n+func (b *CreateIndexBuilder) DefaultLanguage(lang string) *CreateIndexBuilder {\n+\tb.options.DefaultLanguage = lang\n+\treturn b\n+}\n+\n+// LanguageField sets LANGUAGE_FIELD.\n+func (b *CreateIndexBuilder) LanguageField(field string) *CreateIndexBuilder {\n+\tb.options.LanguageField = field\n+\treturn b\n+}\n+\n+// Score sets SCORE.\n+func (b *CreateIndexBuilder) Score(score float64) *CreateIndexBuilder {\n+\tb.options.Score = score\n+\treturn b\n+}\n+\n+// ScoreField sets SCORE_FIELD.\n+func (b *CreateIndexBuilder) ScoreField(field string) *CreateIndexBuilder {\n+\tb.options.ScoreField = field\n+\treturn b\n+}\n+\n+// PayloadField sets PAYLOAD_FIELD.\n+func (b *CreateIndexBuilder) PayloadField(field string) *CreateIndexBuilder {\n+\tb.options.PayloadField = field\n+\treturn b\n+}\n+\n+// NoOffsets includes NOOFFSETS.\n+func (b *CreateIndexBuilder) NoOffsets() *CreateIndexBuilder { b.options.NoOffsets = true; return b }\n+\n+// Temporary sets TEMPORARY seconds.\n+func (b *CreateIndexBuilder) Temporary(sec int) *CreateIndexBuilder {\n+\tb.options.Temporary = sec\n+\treturn b\n+}\n+\n+// NoHL includes NOHL.\n+func (b *CreateIndexBuilder) NoHL() *CreateIndexBuilder { b.options.NoHL = true; return b }\n+\n+// NoFields includes NOFIELDS.\n+func (b *CreateIndexBuilder) NoFields() *CreateIndexBuilder { b.options.NoFields = true; return b }\n+\n+// NoFreqs includes NOFREQS.\n+func (b *CreateIndexBuilder) NoFreqs() *CreateIndexBuilder { b.options.NoFreqs = true; return b }\n+\n+// StopWords sets STOPWORDS.\n+func (b *CreateIndexBuilder) StopWords(words ...interface{}) *CreateIndexBuilder {\n+\tb.options.StopWords = words\n+\treturn b\n+}\n+\n+// SkipInitialScan includes SKIPINITIALSCAN.\n+func (b *CreateIndexBuilder) SkipInitialScan() *CreateIndexBuilder {\n+\tb.options.SkipInitialScan = true\n+\treturn b\n+}\n+\n+// Schema adds a FieldSchema.\n+func (b *CreateIndexBuilder) Schema(field *FieldSchema) *CreateIndexBuilder {\n+\tb.schema = append(b.schema, field)\n+\treturn b\n+}\n+\n+// Run executes FT.CREATE and returns the status.\n+func (b *CreateIndexBuilder) Run() (string, error) {\n+\tcmd := b.c.FTCreate(b.ctx, b.index, b.options, b.schema...)\n+\treturn cmd.Result()\n+}\n+\n+// ----------------------\n+// DropIndexBuilder for FT.DROPINDEX\n+// ----------------------\n+// DropIndexBuilder is a builder for FT.DROPINDEX\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+type DropIndexBuilder struct {\n+\tc       *Client\n+\tctx     context.Context\n+\tindex   string\n+\toptions *FTDropIndexOptions\n+}\n+\n+// NewDropIndexBuilder creates a new DropIndexBuilder for FT.DROPINDEX commands.\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+func (c *Client) NewDropIndexBuilder(ctx context.Context, index string) *DropIndexBuilder {\n+\treturn &DropIndexBuilder{c: c, ctx: ctx, index: index}\n+}\n+\n+// DeleteRuncs includes DD.\n+func (b *DropIndexBuilder) DeleteDocs() *DropIndexBuilder { b.options.DeleteDocs = true; return b }\n+\n+// Run executes FT.DROPINDEX.\n+func (b *DropIndexBuilder) Run() (string, error) {\n+\tcmd := b.c.FTDropIndexWithArgs(b.ctx, b.index, b.options)\n+\treturn cmd.Result()\n+}\n+\n+// ----------------------\n+// AliasBuilder for FT.ALIAS* commands\n+// ----------------------\n+// AliasBuilder is builder for FT.ALIAS* commands\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+type AliasBuilder struct {\n+\tc      *Client\n+\tctx    context.Context\n+\talias  string\n+\tindex  string\n+\taction string // add|del|update\n+}\n+\n+// NewAliasBuilder creates a new AliasBuilder for FT.ALIAS* commands.\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+func (c *Client) NewAliasBuilder(ctx context.Context, alias string) *AliasBuilder {\n+\treturn &AliasBuilder{c: c, ctx: ctx, alias: alias}\n+}\n+\n+// Action sets the action for the alias builder.\n+func (b *AliasBuilder) Action(action string) *AliasBuilder {\n+\tb.action = action\n+\treturn b\n+}\n+\n+// Add sets the action to \"add\" and requires an index.\n+func (b *AliasBuilder) Add(index string) *AliasBuilder {\n+\tb.action = \"add\"\n+\tb.index = index\n+\treturn b\n+}\n+\n+// Del sets the action to \"del\".\n+func (b *AliasBuilder) Del() *AliasBuilder {\n+\tb.action = \"del\"\n+\treturn b\n+}\n+\n+// Update sets the action to \"update\" and requires an index.\n+func (b *AliasBuilder) Update(index string) *AliasBuilder {\n+\tb.action = \"update\"\n+\tb.index = index\n+\treturn b\n+}\n+\n+// Run executes the configured alias command.\n+func (b *AliasBuilder) Run() (string, error) {\n+\tswitch b.action {\n+\tcase \"add\":\n+\t\tcmd := b.c.FTAliasAdd(b.ctx, b.index, b.alias)\n+\t\treturn cmd.Result()\n+\tcase \"del\":\n+\t\tcmd := b.c.FTAliasDel(b.ctx, b.alias)\n+\t\treturn cmd.Result()\n+\tcase \"update\":\n+\t\tcmd := b.c.FTAliasUpdate(b.ctx, b.index, b.alias)\n+\t\treturn cmd.Result()\n+\t}\n+\treturn \"\", nil\n+}\n+\n+// ----------------------\n+// ExplainBuilder for FT.EXPLAIN\n+// ----------------------\n+// ExplainBuilder is builder for FT.EXPLAIN\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+type ExplainBuilder struct {\n+\tc       *Client\n+\tctx     context.Context\n+\tindex   string\n+\tquery   string\n+\toptions *FTExplainOptions\n+}\n+\n+// NewExplainBuilder creates a new ExplainBuilder for FT.EXPLAIN commands.\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+func (c *Client) NewExplainBuilder(ctx context.Context, index, query string) *ExplainBuilder {\n+\treturn &ExplainBuilder{c: c, ctx: ctx, index: index, query: query, options: &FTExplainOptions{}}\n+}\n+\n+// Dialect sets dialect for EXPLAINCLI.\n+func (b *ExplainBuilder) Dialect(d string) *ExplainBuilder { b.options.Dialect = d; return b }\n+\n+// Run executes FT.EXPLAIN and returns the plan.\n+func (b *ExplainBuilder) Run() (string, error) {\n+\tcmd := b.c.FTExplainWithArgs(b.ctx, b.index, b.query, b.options)\n+\treturn cmd.Result()\n+}\n+\n+// ----------------------\n+// InfoBuilder for FT.INFO\n+// ----------------------\n+\n+type FTInfoBuilder struct {\n+\tc     *Client\n+\tctx   context.Context\n+\tindex string\n+}\n+\n+// NewSearchInfoBuilder creates a new FTInfoBuilder for FT.INFO commands.\n+func (c *Client) NewSearchInfoBuilder(ctx context.Context, index string) *FTInfoBuilder {\n+\treturn &FTInfoBuilder{c: c, ctx: ctx, index: index}\n+}\n+\n+// Run executes FT.INFO and returns detailed info.\n+func (b *FTInfoBuilder) Run() (FTInfoResult, error) {\n+\tcmd := b.c.FTInfo(b.ctx, b.index)\n+\treturn cmd.Result()\n+}\n+\n+// ----------------------\n+// SpellCheckBuilder for FT.SPELLCHECK\n+// ----------------------\n+// SpellCheckBuilder is builder for FT.SPELLCHECK\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+type SpellCheckBuilder struct {\n+\tc       *Client\n+\tctx     context.Context\n+\tindex   string\n+\tquery   string\n+\toptions *FTSpellCheckOptions\n+}\n+\n+// NewSpellCheckBuilder creates a new SpellCheckBuilder for FT.SPELLCHECK commands.\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+func (c *Client) NewSpellCheckBuilder(ctx context.Context, index, query string) *SpellCheckBuilder {\n+\treturn &SpellCheckBuilder{c: c, ctx: ctx, index: index, query: query, options: &FTSpellCheckOptions{}}\n+}\n+\n+// Distance sets MAXDISTANCE.\n+func (b *SpellCheckBuilder) Distance(d int) *SpellCheckBuilder { b.options.Distance = d; return b }\n+\n+// Terms sets INCLUDE or EXCLUDE terms.\n+func (b *SpellCheckBuilder) Terms(include bool, dictionary string, terms ...interface{}) *SpellCheckBuilder {\n+\tif b.options.Terms == nil {\n+\t\tb.options.Terms = &FTSpellCheckTerms{}\n+\t}\n+\tif include {\n+\t\tb.options.Terms.Inclusion = \"INCLUDE\"\n+\t} else {\n+\t\tb.options.Terms.Inclusion = \"EXCLUDE\"\n+\t}\n+\tb.options.Terms.Dictionary = dictionary\n+\tb.options.Terms.Terms = terms\n+\treturn b\n+}\n+\n+// Dialect sets dialect version.\n+func (b *SpellCheckBuilder) Dialect(d int) *SpellCheckBuilder { b.options.Dialect = d; return b }\n+\n+// Run executes FT.SPELLCHECK and returns suggestions.\n+func (b *SpellCheckBuilder) Run() ([]SpellCheckResult, error) {\n+\tcmd := b.c.FTSpellCheckWithArgs(b.ctx, b.index, b.query, b.options)\n+\treturn cmd.Result()\n+}\n+\n+// ----------------------\n+// DictBuilder for FT.DICT* commands\n+// ----------------------\n+// DictBuilder is builder for FT.DICT* commands\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+type DictBuilder struct {\n+\tc      *Client\n+\tctx    context.Context\n+\tdict   string\n+\tterms  []interface{}\n+\taction string // add|del|dump\n+}\n+\n+// NewDictBuilder creates a new DictBuilder for FT.DICT* commands.\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+func (c *Client) NewDictBuilder(ctx context.Context, dict string) *DictBuilder {\n+\treturn &DictBuilder{c: c, ctx: ctx, dict: dict}\n+}\n+\n+// Action sets the action for the dictionary builder.\n+func (b *DictBuilder) Action(action string) *DictBuilder {\n+\tb.action = action\n+\treturn b\n+}\n+\n+// Add sets the action to \"add\" and requires terms.\n+func (b *DictBuilder) Add(terms ...interface{}) *DictBuilder {\n+\tb.action = \"add\"\n+\tb.terms = terms\n+\treturn b\n+}\n+\n+// Del sets the action to \"del\" and requires terms.\n+func (b *DictBuilder) Del(terms ...interface{}) *DictBuilder {\n+\tb.action = \"del\"\n+\tb.terms = terms\n+\treturn b\n+}\n+\n+// Dump sets the action to \"dump\".\n+func (b *DictBuilder) Dump() *DictBuilder {\n+\tb.action = \"dump\"\n+\treturn b\n+}\n+\n+// Run executes the configured dictionary command.\n+func (b *DictBuilder) Run() (interface{}, error) {\n+\tswitch b.action {\n+\tcase \"add\":\n+\t\tcmd := b.c.FTDictAdd(b.ctx, b.dict, b.terms...)\n+\t\treturn cmd.Result()\n+\tcase \"del\":\n+\t\tcmd := b.c.FTDictDel(b.ctx, b.dict, b.terms...)\n+\t\treturn cmd.Result()\n+\tcase \"dump\":\n+\t\tcmd := b.c.FTDictDump(b.ctx, b.dict)\n+\t\treturn cmd.Result()\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ----------------------\n+// TagValsBuilder for FT.TAGVALS\n+// ----------------------\n+// TagValsBuilder is builder for FT.TAGVALS\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+type TagValsBuilder struct {\n+\tc     *Client\n+\tctx   context.Context\n+\tindex string\n+\tfield string\n+}\n+\n+// NewTagValsBuilder creates a new TagValsBuilder for FT.TAGVALS commands.\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+func (c *Client) NewTagValsBuilder(ctx context.Context, index, field string) *TagValsBuilder {\n+\treturn &TagValsBuilder{c: c, ctx: ctx, index: index, field: field}\n+}\n+\n+// Run executes FT.TAGVALS and returns tag values.\n+func (b *TagValsBuilder) Run() ([]string, error) {\n+\tcmd := b.c.FTTagVals(b.ctx, b.index, b.field)\n+\treturn cmd.Result()\n+}\n+\n+// ----------------------\n+// CursorBuilder for FT.CURSOR*\n+// ----------------------\n+// CursorBuilder is builder for FT.CURSOR* commands\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+type CursorBuilder struct {\n+\tc        *Client\n+\tctx      context.Context\n+\tindex    string\n+\tcursorId int64\n+\tcount    int\n+\taction   string // read|del\n+}\n+\n+// NewCursorBuilder creates a new CursorBuilder for FT.CURSOR* commands.\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+func (c *Client) NewCursorBuilder(ctx context.Context, index string, cursorId int64) *CursorBuilder {\n+\treturn &CursorBuilder{c: c, ctx: ctx, index: index, cursorId: cursorId}\n+}\n+\n+// Action sets the action for the cursor builder.\n+func (b *CursorBuilder) Action(action string) *CursorBuilder {\n+\tb.action = action\n+\treturn b\n+}\n+\n+// Read sets the action to \"read\".\n+func (b *CursorBuilder) Read() *CursorBuilder {\n+\tb.action = \"read\"\n+\treturn b\n+}\n+\n+// Del sets the action to \"del\".\n+func (b *CursorBuilder) Del() *CursorBuilder {\n+\tb.action = \"del\"\n+\treturn b\n+}\n+\n+// Count for READ.\n+func (b *CursorBuilder) Count(count int) *CursorBuilder { b.count = count; return b }\n+\n+// Run executes the cursor command.\n+func (b *CursorBuilder) Run() (interface{}, error) {\n+\tswitch b.action {\n+\tcase \"read\":\n+\t\tcmd := b.c.FTCursorRead(b.ctx, b.index, int(b.cursorId), b.count)\n+\t\treturn cmd.Result()\n+\tcase \"del\":\n+\t\tcmd := b.c.FTCursorDel(b.ctx, b.index, int(b.cursorId))\n+\t\treturn cmd.Result()\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ----------------------\n+// SynUpdateBuilder for FT.SYNUPDATE\n+// ----------------------\n+// SyncUpdateBuilder is builder for FT.SYNCUPDATE\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+type SynUpdateBuilder struct {\n+\tc       *Client\n+\tctx     context.Context\n+\tindex   string\n+\tgroupId interface{}\n+\toptions *FTSynUpdateOptions\n+\tterms   []interface{}\n+}\n+\n+// NewSynUpdateBuilder creates a new SynUpdateBuilder for FT.SYNUPDATE commands.\n+// EXPERIMENTAL: this API is subject to change, use with caution.\n+func (c *Client) NewSynUpdateBuilder(ctx context.Context, index string, groupId interface{}) *SynUpdateBuilder {\n+\treturn &SynUpdateBuilder{c: c, ctx: ctx, index: index, groupId: groupId, options: &FTSynUpdateOptions{}}\n+}\n+\n+// SkipInitialScan includes SKIPINITIALSCAN.\n+func (b *SynUpdateBuilder) SkipInitialScan() *SynUpdateBuilder {\n+\tb.options.SkipInitialScan = true\n+\treturn b\n+}\n+\n+// Terms adds synonyms to the group.\n+func (b *SynUpdateBuilder) Terms(terms ...interface{}) *SynUpdateBuilder { b.terms = terms; return b }\n+\n+// Run executes FT.SYNUPDATE.\n+func (b *SynUpdateBuilder) Run() (string, error) {\n+\tcmd := b.c.FTSynUpdateWithArgs(b.ctx, b.index, b.groupId, b.options, b.terms)\n+\treturn cmd.Result()\n+}"
    },
    {
      "sha": "9018b3ded5325e1344ca743b0a3ea4fb48391711",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/search_commands.go",
      "status": "modified",
      "additions": 0,
      "deletions": 0,
      "changes": 0,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsearch_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsearch_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsearch_commands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "663f7b1ad9bddfed03b94f452dcc13e498814f9a",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/sentinel.go",
      "status": "modified",
      "additions": 402,
      "deletions": 51,
      "changes": 453,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsentinel.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsentinel.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsentinel.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "79efa6e4052444bcc3d438ad8d01fcd818714577",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/set_commands.go",
      "status": "modified",
      "additions": 13,
      "deletions": 7,
      "changes": 20,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fset_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fset_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fset_commands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "7827babc88b352cbd08ce5137d6bc628088a819d",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/sortedset_commands.go",
      "status": "modified",
      "additions": 17,
      "deletions": 8,
      "changes": 25,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsortedset_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsortedset_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fsortedset_commands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "5573e48b966024c54b7666be34bb58844be6eef7",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/stream_commands.go",
      "status": "modified",
      "additions": 75,
      "deletions": 0,
      "changes": 75,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fstream_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fstream_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fstream_commands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "f3c33f4cba78125e0fa35a9be0fb869e68d2b5d3",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/string_commands.go",
      "status": "modified",
      "additions": 446,
      "deletions": 1,
      "changes": 447,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fstring_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fstring_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fstring_commands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "40bc1d6618dbc95dbf33b86481bfef5e4c359b22",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/tx.go",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Ftx.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Ftx.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Ftx.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "1dc9764dc7ddd9a440e2fa2a5dbe5f7286a58448",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/universal.go",
      "status": "modified",
      "additions": 115,
      "deletions": 29,
      "changes": 144,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Funiversal.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Funiversal.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Funiversal.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "8f99de0730610a61adb536cd4fc3e65f4e1a91b2",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/vectorset_commands.go",
      "status": "added",
      "additions": 358,
      "deletions": 0,
      "changes": 358,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fvectorset_commands.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fvectorset_commands.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fvectorset_commands.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "126fa10b0c4254409d57bd33ec381424057155cd",
      "filename": "backend/vendor/github.com/redis/go-redis/v9/version.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fversion.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fversion.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fredis%2Fgo-redis%2Fv9%2Fversion.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "9b3903cd2f6f0b3b303c6d39330a913751c988bd",
      "filename": "backend/vendor/github.com/slack-go/slack/.gitignore",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2F.gitignore",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2F.gitignore",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2F.gitignore?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "0ef0f87ec0748c34e5541c741c70d3f2d6c20f64",
      "filename": "backend/vendor/github.com/slack-go/slack/.golangci.yml",
      "status": "modified",
      "additions": 25,
      "deletions": 4,
      "changes": 29,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2F.golangci.yml",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2F.golangci.yml",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2F.golangci.yml?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "32da687bb0f135003ffd63753c1f8fa339da2c37",
      "filename": "backend/vendor/github.com/slack-go/slack/CHANGELOG.md",
      "status": "removed",
      "additions": 0,
      "deletions": 103,
      "changes": 103,
      "blob_url": "https://github.com/umputun/remark42/blob/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FCHANGELOG.md",
      "raw_url": "https://github.com/umputun/remark42/raw/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FCHANGELOG.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FCHANGELOG.md?ref=b4511427905c02e55730d7cb27036f5796decaa9"
    },
    {
      "sha": "8b92d35316fda45779ef291a903a63a8c804cb41",
      "filename": "backend/vendor/github.com/slack-go/slack/CONTRIBUTING.md",
      "status": "added",
      "additions": 40,
      "deletions": 0,
      "changes": 40,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FCONTRIBUTING.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FCONTRIBUTING.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FCONTRIBUTING.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "127ae253c8417fbfad5ba1bf07b107e82921657c",
      "filename": "backend/vendor/github.com/slack-go/slack/README.md",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FREADME.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FREADME.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FREADME.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "8607960b86d7986a3be011ddb606a28c76dad099",
      "filename": "backend/vendor/github.com/slack-go/slack/TODO.txt",
      "status": "removed",
      "additions": 0,
      "deletions": 3,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FTODO.txt",
      "raw_url": "https://github.com/umputun/remark42/raw/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FTODO.txt",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2FTODO.txt?ref=b4511427905c02e55730d7cb27036f5796decaa9"
    },
    {
      "sha": "6f76568ba2dfa28f765673538f6ea548dd0ecec2",
      "filename": "backend/vendor/github.com/slack-go/slack/admin_conversations.go",
      "status": "added",
      "additions": 85,
      "deletions": 0,
      "changes": 85,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fadmin_conversations.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fadmin_conversations.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fadmin_conversations.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "8432f89b3dbf4dc69d94dab8b522ae4abfa5ec79",
      "filename": "backend/vendor/github.com/slack-go/slack/assistant.go",
      "status": "added",
      "additions": 157,
      "deletions": 0,
      "changes": 157,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fassistant.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fassistant.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fassistant.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "972f59ea6e267016306d374ddff2a8bae54a87a9",
      "filename": "backend/vendor/github.com/slack-go/slack/auth.go",
      "status": "modified",
      "additions": 7,
      "deletions": 2,
      "changes": 9,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fauth.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fauth.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fauth.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "7c4f993085f7b00ddefed25a7866a3dcb111e992",
      "filename": "backend/vendor/github.com/slack-go/slack/block.go",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "819c0ef0d75bc221a6b2fb4c656371c268cb8c3a",
      "filename": "backend/vendor/github.com/slack-go/slack/block_action.go",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_action.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_action.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_action.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "c81dcb8be172366a1e3ee8e19f8bb8a0fc9f8f65",
      "filename": "backend/vendor/github.com/slack-go/slack/block_call.go",
      "status": "modified",
      "additions": 6,
      "deletions": 1,
      "changes": 7,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_call.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_call.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_call.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "879ee61ee2442ee2eafc3b13f884d20ce949c908",
      "filename": "backend/vendor/github.com/slack-go/slack/block_context.go",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_context.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_context.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_context.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "7df9b107b2daf798edfd3e80b0e95d099372d222",
      "filename": "backend/vendor/github.com/slack-go/slack/block_conv.go",
      "status": "modified",
      "additions": 2,
      "deletions": 0,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_conv.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_conv.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_conv.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "e10d7b053717572296cf48c40d014781fac445a5",
      "filename": "backend/vendor/github.com/slack-go/slack/block_divider.go",
      "status": "modified",
      "additions": 5,
      "deletions": 1,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_divider.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_divider.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_divider.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "2b32d331e0cb7f6433cb0ad304b834f456b99e2e",
      "filename": "backend/vendor/github.com/slack-go/slack/block_element.go",
      "status": "modified",
      "additions": 17,
      "deletions": 3,
      "changes": 20,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_element.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_element.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_element.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "2f669b02acf51f274237b0416cfb581ac833a119",
      "filename": "backend/vendor/github.com/slack-go/slack/block_file.go",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_file.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_file.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_file.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "3afb4c95fe249bf119213de4d5169e423bdf2b0d",
      "filename": "backend/vendor/github.com/slack-go/slack/block_header.go",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_header.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_header.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_header.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "2a914e5a00006f33a07af555d05e11dd49253475",
      "filename": "backend/vendor/github.com/slack-go/slack/block_image.go",
      "status": "modified",
      "additions": 17,
      "deletions": 0,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_image.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_image.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_image.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "f74eda6d23cae9bf98659175ee2ca5361f4a07e7",
      "filename": "backend/vendor/github.com/slack-go/slack/block_input.go",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_input.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_input.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_input.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "e22a0d16d5821165041efc1431a84e1bef540c6b",
      "filename": "backend/vendor/github.com/slack-go/slack/block_markdown.go",
      "status": "added",
      "additions": 34,
      "deletions": 0,
      "changes": 34,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_markdown.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_markdown.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_markdown.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "fd73b6c4a4ca22a275ae0496dc657ba7fb05afd3",
      "filename": "backend/vendor/github.com/slack-go/slack/block_object.go",
      "status": "modified",
      "additions": 22,
      "deletions": 6,
      "changes": 28,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_object.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_object.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_object.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "73233d23ae6e740132dbde923022b95de3244e64",
      "filename": "backend/vendor/github.com/slack-go/slack/block_rich_text.go",
      "status": "modified",
      "additions": 19,
      "deletions": 4,
      "changes": 23,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_rich_text.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_rich_text.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_rich_text.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "0d2352a037dc8defce4002db1e52ba06e939348a",
      "filename": "backend/vendor/github.com/slack-go/slack/block_section.go",
      "status": "modified",
      "additions": 15,
      "deletions": 0,
      "changes": 15,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_section.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_section.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_section.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "7a49a2c8d1bb10ee55fdfd7ed16d74f82f846d48",
      "filename": "backend/vendor/github.com/slack-go/slack/block_unknown.go",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_unknown.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_unknown.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_unknown.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "4d6739c4644a499ecb319cdcbda06d052df0fb1b",
      "filename": "backend/vendor/github.com/slack-go/slack/block_video.go",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_video.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_video.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fblock_video.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "85c4848ba4db8d0028f6fbe452510b6ed7751b4a",
      "filename": "backend/vendor/github.com/slack-go/slack/chat.go",
      "status": "modified",
      "additions": 33,
      "deletions": 5,
      "changes": 38,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fchat.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fchat.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fchat.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "33eb0ff9be28ba69332cbabaa79170dcf3f9b737",
      "filename": "backend/vendor/github.com/slack-go/slack/conversation.go",
      "status": "modified",
      "additions": 57,
      "deletions": 4,
      "changes": 61,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fconversation.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fconversation.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fconversation.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "810c476b77ce695e94165eecb993d82ea27ad2c2",
      "filename": "backend/vendor/github.com/slack-go/slack/files.go",
      "status": "modified",
      "additions": 100,
      "deletions": 50,
      "changes": 150,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Ffiles.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Ffiles.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Ffiles.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "a026ab494705a78313dd363b4371b721fb4bf523",
      "filename": "backend/vendor/github.com/slack-go/slack/info.go",
      "status": "modified",
      "additions": 6,
      "deletions": 3,
      "changes": 9,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Finfo.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Finfo.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Finfo.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "f658a72c2ade875cd44f39d37dca6cdcf65bece7",
      "filename": "backend/vendor/github.com/slack-go/slack/interactions.go",
      "status": "modified",
      "additions": 43,
      "deletions": 32,
      "changes": 75,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Finteractions.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Finteractions.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Finteractions.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "c53809fdaa95ffff24eaf43ab8d4b72abd44fab0",
      "filename": "backend/vendor/github.com/slack-go/slack/messages.go",
      "status": "modified",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fmessages.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fmessages.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fmessages.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "fa6690ee3f9b713d13189313bbdc45f92b9ac64f",
      "filename": "backend/vendor/github.com/slack-go/slack/migration.go",
      "status": "added",
      "additions": 40,
      "deletions": 0,
      "changes": 40,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fmigration.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fmigration.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fmigration.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "48411ee1905684217b577320a069900385240b5e",
      "filename": "backend/vendor/github.com/slack-go/slack/misc.go",
      "status": "modified",
      "additions": 106,
      "deletions": 30,
      "changes": 136,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fmisc.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fmisc.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fmisc.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "9b30eb31552da66d84c4f2cc6be26716dff885da",
      "filename": "backend/vendor/github.com/slack-go/slack/rtm.go",
      "status": "modified",
      "additions": 6,
      "deletions": 0,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Frtm.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Frtm.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Frtm.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "9df433501b957c14a4532699940a85c78fbc6f85",
      "filename": "backend/vendor/github.com/slack-go/slack/search.go",
      "status": "modified",
      "additions": 5,
      "deletions": 5,
      "changes": 10,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fsearch.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fsearch.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fsearch.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "ea9ea3b7a01ee8c27da730826e330567fd43cc43",
      "filename": "backend/vendor/github.com/slack-go/slack/socket_mode.go",
      "status": "modified",
      "additions": 9,
      "deletions": 0,
      "changes": 9,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fsocket_mode.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fsocket_mode.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fsocket_mode.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "35b699271609755c235c823ce8897b23b64cc978",
      "filename": "backend/vendor/github.com/slack-go/slack/team.go",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fteam.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fteam.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fteam.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "616bbf1580e2a52edf1c2776fe43de879f0970bf",
      "filename": "backend/vendor/github.com/slack-go/slack/usergroups.go",
      "status": "modified",
      "additions": 258,
      "deletions": 19,
      "changes": 277,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fusergroups.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fusergroups.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fusergroups.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "91a9a4c7be0670abe23f2051f5907b70b9af1a73",
      "filename": "backend/vendor/github.com/slack-go/slack/users.go",
      "status": "modified",
      "additions": 1,
      "deletions": 0,
      "changes": 1,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fusers.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fusers.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fusers.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "5f55b537a283d0e03dff8a8cfb209a522ecc84eb",
      "filename": "backend/vendor/github.com/slack-go/slack/views.go",
      "status": "modified",
      "additions": 9,
      "deletions": 16,
      "changes": 25,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fviews.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fviews.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fviews.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "747a5dc003419b4f045db70fcb399c40d34210c5",
      "filename": "backend/vendor/github.com/slack-go/slack/workflow_step.go",
      "status": "removed",
      "additions": 0,
      "deletions": 101,
      "changes": 101,
      "blob_url": "https://github.com/umputun/remark42/blob/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fworkflow_step.go",
      "raw_url": "https://github.com/umputun/remark42/raw/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fworkflow_step.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fworkflow_step.go?ref=b4511427905c02e55730d7cb27036f5796decaa9"
    },
    {
      "sha": "32db9ba0bdc57f7f2a9034c8ab3ceb9685beae2a",
      "filename": "backend/vendor/github.com/slack-go/slack/workflow_step_execute.go",
      "status": "removed",
      "additions": 0,
      "deletions": 85,
      "changes": 85,
      "blob_url": "https://github.com/umputun/remark42/blob/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fworkflow_step_execute.go",
      "raw_url": "https://github.com/umputun/remark42/raw/b4511427905c02e55730d7cb27036f5796decaa9/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fworkflow_step_execute.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fworkflow_step_execute.go?ref=b4511427905c02e55730d7cb27036f5796decaa9"
    },
    {
      "sha": "f9a7cd90c86ad3bef5006d3a0db146620227cf80",
      "filename": "backend/vendor/github.com/slack-go/slack/workflows_triggers.go",
      "status": "added",
      "additions": 177,
      "deletions": 0,
      "changes": 177,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fworkflows_triggers.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fworkflows_triggers.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fslack-go%2Fslack%2Fworkflows_triggers.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "90b2037e100d9b812388c261b17bb94ce4ba7c3a",
      "filename": "backend/vendor/github.com/xdg-go/scram/.gitignore",
      "status": "modified",
      "additions": 34,
      "deletions": 0,
      "changes": 34,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2F.gitignore",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2F.gitignore",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2F.gitignore?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "fc4a1e83d2de61530f8ae70db03edaa32759cffa",
      "filename": "backend/vendor/github.com/xdg-go/scram/CHANGELOG.md",
      "status": "modified",
      "additions": 31,
      "deletions": 0,
      "changes": 31,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2FCHANGELOG.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2FCHANGELOG.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2FCHANGELOG.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "a19b513054ffd8484420790c208220fb32463504",
      "filename": "backend/vendor/github.com/xdg-go/scram/CLAUDE.md",
      "status": "added",
      "additions": 75,
      "deletions": 0,
      "changes": 75,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2FCLAUDE.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2FCLAUDE.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2FCLAUDE.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "8f94e33db770a93fad54606cbdc2c2edaa4d31ce",
      "filename": "backend/vendor/github.com/xdg-go/scram/README.md",
      "status": "modified",
      "additions": 45,
      "deletions": 3,
      "changes": 48,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2FREADME.md",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2FREADME.md",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2FREADME.md?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "52b1b0c3a972d6535054799fc2c3f8e9c2b493b2",
      "filename": "backend/vendor/github.com/xdg-go/scram/channel_binding.go",
      "status": "added",
      "additions": 144,
      "deletions": 0,
      "changes": 144,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fchannel_binding.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fchannel_binding.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fchannel_binding.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "906fe6e590040b806523ac596112bbe668774129",
      "filename": "backend/vendor/github.com/xdg-go/scram/client.go",
      "status": "modified",
      "additions": 80,
      "deletions": 12,
      "changes": 92,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fclient.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fclient.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fclient.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "4da6c2415643b4e1d4df0a05c872e3a43ab825bc",
      "filename": "backend/vendor/github.com/xdg-go/scram/client_conv.go",
      "status": "modified",
      "additions": 48,
      "deletions": 16,
      "changes": 64,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fclient_conv.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fclient_conv.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fclient_conv.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "fa0a81b9558f651936f2328fb698daae6d475f8d",
      "filename": "backend/vendor/github.com/xdg-go/scram/common.go",
      "status": "modified",
      "additions": 47,
      "deletions": 9,
      "changes": 56,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fcommon.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fcommon.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fcommon.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "3314cc1a225ff1f6d559adcc7fd174be3ff33935",
      "filename": "backend/vendor/github.com/xdg-go/scram/doc.go",
      "status": "modified",
      "additions": 42,
      "deletions": 9,
      "changes": 51,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fdoc.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fdoc.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fdoc.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "3585c628d0aaaa762f21cdd9f889f662f39c5d66",
      "filename": "backend/vendor/github.com/xdg-go/scram/parse.go",
      "status": "modified",
      "additions": 29,
      "deletions": 18,
      "changes": 47,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fparse.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fparse.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fparse.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "6708ec4d31cc3c9e8964a7fae268b8a52021bcf9",
      "filename": "backend/vendor/github.com/xdg-go/scram/pbkdf2_go124.go",
      "status": "added",
      "additions": 18,
      "deletions": 0,
      "changes": 18,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fpbkdf2_go124.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fpbkdf2_go124.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fpbkdf2_go124.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "a81ab41796f5709c076f772609724995753541b2",
      "filename": "backend/vendor/github.com/xdg-go/scram/pbkdf2_legacy.go",
      "status": "added",
      "additions": 19,
      "deletions": 0,
      "changes": 19,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fpbkdf2_legacy.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fpbkdf2_legacy.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fpbkdf2_legacy.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "e116623eff2b260fc402908b5b9b4d419b6abaab",
      "filename": "backend/vendor/github.com/xdg-go/scram/server.go",
      "status": "modified",
      "additions": 59,
      "deletions": 0,
      "changes": 59,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fserver.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fserver.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fserver.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "2874888e8abafbd22840e30a30b07b4b9e9b84a5",
      "filename": "backend/vendor/github.com/xdg-go/scram/server_conv.go",
      "status": "modified",
      "additions": 104,
      "deletions": 22,
      "changes": 126,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fserver_conv.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fserver_conv.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgithub.com%2Fxdg-go%2Fscram%2Fserver_conv.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "86a293570f3b35478d2a86ea5faa95093802b718",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/bson/bsonrw/extjson_writer.go",
      "status": "modified",
      "additions": 4,
      "deletions": 3,
      "changes": 7,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fbson%2Fbsonrw%2Fextjson_writer.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fbson%2Fbsonrw%2Fextjson_writer.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fbson%2Fbsonrw%2Fextjson_writer.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "0148756fc3e1b632cabac2f3bbeb8677e5dd2756",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/mongo/mongointernal.go",
      "status": "added",
      "additions": 41,
      "deletions": 0,
      "changes": 41,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fmongo%2Fmongointernal.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fmongo%2Fmongointernal.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fmongo%2Fmongointernal.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "ace11008c180995b66553a19e26bf77508cf48e8",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/version/version.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fversion%2Fversion.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fversion%2Fversion.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fversion%2Fversion.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "bad00e7450c1b8d455ae798f7eed52196a470a52",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/connection.go",
      "status": "modified",
      "additions": 0,
      "deletions": 43,
      "changes": 43,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fx%2Fmongo%2Fdriver%2Ftopology%2Fconnection.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fx%2Fmongo%2Fdriver%2Ftopology%2Fconnection.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fx%2Fmongo%2Fdriver%2Ftopology%2Fconnection.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "45b0b7d9b529c910def99302e16676ed5f37d63d",
      "filename": "backend/vendor/go.mongodb.org/mongo-driver/x/mongo/driver/topology/pool.go",
      "status": "modified",
      "additions": 2,
      "deletions": 4,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fx%2Fmongo%2Fdriver%2Ftopology%2Fpool.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fx%2Fmongo%2Fdriver%2Ftopology%2Fpool.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgo.mongodb.org%2Fmongo-driver%2Fx%2Fmongo%2Fdriver%2Ftopology%2Fpool.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "46d1396f1f7dfa7438b4b6ca0c0e83484e13b264",
      "filename": "backend/vendor/golang.org/x/oauth2/authhandler/authhandler.go",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fauthhandler%2Fauthhandler.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fauthhandler%2Fauthhandler.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fauthhandler%2Fauthhandler.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "e783a94374822a9c2c313148020e272a4eacc08a",
      "filename": "backend/vendor/golang.org/x/oauth2/deviceauth.go",
      "status": "modified",
      "additions": 30,
      "deletions": 1,
      "changes": 31,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fdeviceauth.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fdeviceauth.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fdeviceauth.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "874b128885aecca074e6a32bc430fd6155bea2bd",
      "filename": "backend/vendor/golang.org/x/oauth2/endpoints/endpoints.go",
      "status": "modified",
      "additions": 209,
      "deletions": 16,
      "changes": 225,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fendpoints%2Fendpoints.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fendpoints%2Fendpoints.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fendpoints%2Fendpoints.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "f62ec99a5f0182817adbdb95679cb3608c883f1c",
      "filename": "backend/vendor/golang.org/x/oauth2/google/externalaccount/aws.go",
      "status": "modified",
      "additions": 8,
      "deletions": 10,
      "changes": 18,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Faws.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Faws.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Faws.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "6f7662170e50ad30628a2700b783f66a845d1295",
      "filename": "backend/vendor/golang.org/x/oauth2/google/externalaccount/basecredentials.go",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Fbasecredentials.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Fbasecredentials.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Fbasecredentials.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "b173c61f06e561e5d996acd77a0c60fd8d2d2431",
      "filename": "backend/vendor/golang.org/x/oauth2/google/externalaccount/executablecredsource.go",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Fexecutablecredsource.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Fexecutablecredsource.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Fexecutablecredsource.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "46ebc18361ecab2fbe77d39d7307f9d078bd34d8",
      "filename": "backend/vendor/golang.org/x/oauth2/google/externalaccount/filecredsource.go",
      "status": "modified",
      "additions": 2,
      "deletions": 3,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Ffilecredsource.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Ffilecredsource.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Ffilecredsource.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "65bfd2046cf8dbc1db2646072ae2666dd82aa2a6",
      "filename": "backend/vendor/golang.org/x/oauth2/google/externalaccount/urlcredsource.go",
      "status": "modified",
      "additions": 2,
      "deletions": 3,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Furlcredsource.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Furlcredsource.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fexternalaccount%2Furlcredsource.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "7d1fdd31d343c003ac3e74087e05c604539cf3e1",
      "filename": "backend/vendor/golang.org/x/oauth2/google/google.go",
      "status": "modified",
      "additions": 5,
      "deletions": 9,
      "changes": 14,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fgoogle.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fgoogle.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Fgoogle.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "eaa8b5c71f3e8aa681baad7cbf60ce31681bbd57",
      "filename": "backend/vendor/golang.org/x/oauth2/google/internal/impersonate/impersonate.go",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Finternal%2Fimpersonate%2Fimpersonate.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Finternal%2Fimpersonate%2Fimpersonate.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Finternal%2Fimpersonate%2Fimpersonate.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "edf700e21501bfb870d9cb9377234d71682c7249",
      "filename": "backend/vendor/golang.org/x/oauth2/google/internal/stsexchange/sts_exchange.go",
      "status": "modified",
      "additions": 2,
      "deletions": 3,
      "changes": 5,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Finternal%2Fstsexchange%2Fsts_exchange.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Finternal%2Fstsexchange%2Fsts_exchange.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fgoogle%2Finternal%2Fstsexchange%2Fsts_exchange.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "8c7c475f2db63b9246018b1bfaa74ee95c1fd4d0",
      "filename": "backend/vendor/golang.org/x/oauth2/internal/doc.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Fdoc.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Fdoc.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Fdoc.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "71ea6ad1f5ee320d1301637d10d0f972e5ef7bb6",
      "filename": "backend/vendor/golang.org/x/oauth2/internal/oauth2.go",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Foauth2.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Foauth2.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Foauth2.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "8389f2462949176657ad7aff7a9a732d2fc9940c",
      "filename": "backend/vendor/golang.org/x/oauth2/internal/token.go",
      "status": "modified",
      "additions": 27,
      "deletions": 23,
      "changes": 50,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Ftoken.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Ftoken.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Ftoken.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "afc0aeb274eb0f6bb7b1bf8137e3626d9fde51e1",
      "filename": "backend/vendor/golang.org/x/oauth2/internal/transport.go",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Ftransport.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Ftransport.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Finternal%2Ftransport.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "9bc484406eee8a230238a2b673fa5d2f1fdb8d56",
      "filename": "backend/vendor/golang.org/x/oauth2/jws/jws.go",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fjws%2Fjws.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fjws%2Fjws.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fjws%2Fjws.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "38a92daca8a6ae459e61a5884c31233142c14710",
      "filename": "backend/vendor/golang.org/x/oauth2/jwt/jwt.go",
      "status": "modified",
      "additions": 5,
      "deletions": 8,
      "changes": 13,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fjwt%2Fjwt.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fjwt%2Fjwt.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fjwt%2Fjwt.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "5c527d31fd38a6bdf5f9c7c8c8206a3ec73a0d8a",
      "filename": "backend/vendor/golang.org/x/oauth2/oauth2.go",
      "status": "modified",
      "additions": 29,
      "deletions": 31,
      "changes": 60,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Foauth2.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Foauth2.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Foauth2.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "f99384f0f5c731b908dbaff0e358b296d6b8abe9",
      "filename": "backend/vendor/golang.org/x/oauth2/pkce.go",
      "status": "modified",
      "additions": 9,
      "deletions": 8,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fpkce.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fpkce.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Fpkce.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "e995eebb5e176424ca0de53890f0b18bbb8047bf",
      "filename": "backend/vendor/golang.org/x/oauth2/token.go",
      "status": "modified",
      "additions": 9,
      "deletions": 8,
      "changes": 17,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Ftoken.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Ftoken.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Ftoken.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "9922ec331648d460a2fe2fa0b8be19246cfad98e",
      "filename": "backend/vendor/golang.org/x/oauth2/transport.go",
      "status": "modified",
      "additions": 6,
      "deletions": 20,
      "changes": 26,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Ftransport.go",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Ftransport.go",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fgolang.org%2Fx%2Foauth2%2Ftransport.go?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    },
    {
      "sha": "36c5ac3fcdf0746b9ac51b68e286219e84c5d6c6",
      "filename": "backend/vendor/modules.txt",
      "status": "modified",
      "additions": 35,
      "deletions": 29,
      "changes": 64,
      "blob_url": "https://github.com/umputun/remark42/blob/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fmodules.txt",
      "raw_url": "https://github.com/umputun/remark42/raw/564e8ff3166689bd8dfd1c372e9144ecc61fdba2/backend%2Fvendor%2Fmodules.txt",
      "contents_url": "https://api.github.com/repos/umputun/remark42/contents/backend%2Fvendor%2Fmodules.txt?ref=564e8ff3166689bd8dfd1c372e9144ecc61fdba2"
    }
  ]
}
